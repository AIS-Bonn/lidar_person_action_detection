{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1175071d-2eb1-4b45-a1bb-424b3eeb047e",
   "metadata": {},
   "source": [
    "# Person Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e09d99c-2810-48c5-a79b-82197a2e230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 233/233 [938.9ms elapsed, 0s remaining, 248.2 samples/s]      \n",
      " 100% |███████████████████| 51/51 [238.2ms elapsed, 0s remaining, 214.1 samples/s]     \n",
      " 100% |███████████████████| 50/50 [269.7ms elapsed, 0s remaining, 186.8 samples/s]      \n",
      "Found train dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      "Found test dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      "Found valid dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      " 100% |███████████████████| 74/74 [285.7ms elapsed, 0s remaining, 260.7 samples/s]      \n",
      " 100% |███████████████████| 17/17 [116.4ms elapsed, 0s remaining, 146.0 samples/s]    \n",
      " 100% |███████████████████| 17/17 [121.3ms elapsed, 0s remaining, 140.1 samples/s]    \n",
      "Found difficult_train dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      "Found difficult_test dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      "Found difficult_valid dataset labels:\n",
      "['sitting', 'walking', 'waving']\n",
      "Label dictionary for complete dataset\n",
      "{'person': 0}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'person_detection'\n",
    "\n",
    "import os\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from helpers.init import initialization\n",
    "\n",
    "#cfg, classes, labels_dict, \\\n",
    "#train_dataset_combined, test_dataset_combined, valid_dataset_combined = initialization(merge_to_one_class = True)\n",
    "cfg, classes, labels_dict, \\\n",
    "train_dataset_combined, test_dataset_combined, valid_dataset_combined = initialization(merge_to_one_class = True, swin = False, encoding = True)\n",
    "\n",
    "### Training of the neural network ###\n",
    "\n",
    "#cfg.MODEL.WEIGHTS = \"include/maskdino_swinl_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask52.3ap_box59.0ap.pth\" \n",
    "cfg.MODEL.WEIGHTS = \"include/maskdino_r50_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask46.3ap_box51.7ap.pth\"\n",
    "\n",
    "cfg.SOLVER.BACKBONE_MULTIPLIER = 1e-5\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2 # batch size\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.DATALOADER.PREFETCH_FACTOR = 2\n",
    "\n",
    "\n",
    "cfg.SOLVER.BASE_LR = 1e-4\n",
    "cfg.SOLVER.MAX_ITER = 4000 \n",
    "cfg.SOLVER.STEPS = ([2000]) # when to decay learning rate\n",
    "\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.TEST.EVAL_PERIOD = 100\n",
    "\n",
    "dir_list = os.listdir('output')\n",
    "if '.ipynb_checkpoints' in dir_list:\n",
    "        dir_list.remove('.ipynb_checkpoints')\n",
    "\n",
    "cfg.OUTPUT_DIR = os.path.join('output', f'{len(dir_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0624a-a774-481f-8396-c44c29c806a9",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892d3786-1a8c-40bc-b291-f2eb42f944d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "[03/14 17:01:40 d2.engine.defaults]: Model:\n",
      "MaskDINO(\n",
      "  (backbone): ResNet(\n",
      "    (stem): BasicStem(\n",
      "      (conv1): Conv2d(\n",
      "        7, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (res2): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res3): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res4): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (4): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (5): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res5): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sem_seg_head): MaskDINOHead(\n",
      "    (pixel_decoder): MaskDINOEncoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MaskDINODecoder(\n",
      "      (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential()\n",
      "        (1): Sequential()\n",
      "        (2): Sequential()\n",
      "        (3): Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (label_enc): Embedding(1, 256)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ref_point_head): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (bbox_embed): ModuleList(\n",
      "          (0): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (6): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (7): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (8): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_bbox_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (2): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (3): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (4): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (5): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (6): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (7): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (8): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 4.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks', 'boxes']\n",
      "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "      num_classes: 1\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "[03/14 17:01:40 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from include/maskdino_r50_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask46.3ap_box51.7ap.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'backbone.stem.conv1.weight' to the model due to incompatible shapes: (64, 3, 7, 7) in the checkpoint but (64, 7, 7, 7) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (1, 256) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'sem_seg_head.predictor.label_enc.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (1, 256) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "backbone.stem.conv1.weight\n",
      "criterion.empty_weight\n",
      "sem_seg_head.predictor.class_embed.{bias, weight}\n",
      "sem_seg_head.predictor.label_enc.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskDINO(\n",
      "  (backbone): ResNet(\n",
      "    (stem): BasicStem(\n",
      "      (conv1): Conv2d(\n",
      "        7, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (res2): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res3): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res4): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (4): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (5): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res5): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sem_seg_head): MaskDINOHead(\n",
      "    (pixel_decoder): MaskDINOEncoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MaskDINODecoder(\n",
      "      (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential()\n",
      "        (1): Sequential()\n",
      "        (2): Sequential()\n",
      "        (3): Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (label_enc): Embedding(1, 256)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ref_point_head): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (bbox_embed): ModuleList(\n",
      "          (0): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (6): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (7): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (8): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_bbox_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (2): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (3): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (4): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (5): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (6): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (7): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (8): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 4.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks', 'boxes']\n",
      "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "      num_classes: 1\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "[03/14 17:01:42 d2.data.build]: Distribution of instances among all 1 categories:\n",
      "|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   person   | 427          |\n",
      "|            |              |\n",
      "[03/14 17:01:42 d2.data.build]: Using training sampler TrainingSampler\n",
      "[03/14 17:01:42 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:01:42 d2.data.common]: Serializing 307 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:01:42 d2.data.common]: Serialized dataset takes 0.20 MiB\n",
      "[03/14 17:01:42 d2.engine.train_loop]: Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/seliuninas0/anaconda3/envs/maskdino/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/user/seliuninas0/anaconda3/envs/maskdino/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/14 17:01:57 d2.utils.events]:  eta: 0:33:53  iter: 19  total_loss: 244.2  loss_ce: 2.937  loss_mask: 1.317  loss_dice: 4.255  loss_bbox: 1.995  loss_giou: 1.797  loss_ce_dn: 0.7189  loss_mask_dn: 1.74  loss_dice_dn: 4.386  loss_bbox_dn: 1.081  loss_giou_dn: 0.8736  loss_ce_0: 4.396  loss_mask_0: 1.226  loss_dice_0: 4.437  loss_bbox_0: 1.917  loss_giou_0: 2.014  loss_ce_dn_0: 0.1542  loss_mask_dn_0: 0.7284  loss_dice_dn_0: 4.926  loss_bbox_dn_0: 0.8007  loss_giou_dn_0: 0.8521  loss_ce_1: 2.557  loss_mask_1: 1.461  loss_dice_1: 4.412  loss_bbox_1: 2.131  loss_giou_1: 2.061  loss_ce_dn_1: 0.4995  loss_mask_dn_1: 1.387  loss_dice_dn_1: 4.428  loss_bbox_dn_1: 0.8949  loss_giou_dn_1: 0.877  loss_ce_2: 2.58  loss_mask_2: 1.465  loss_dice_2: 4.56  loss_bbox_2: 1.964  loss_giou_2: 1.846  loss_ce_dn_2: 0.8504  loss_mask_dn_2: 1.905  loss_dice_dn_2: 4.392  loss_bbox_dn_2: 1.04  loss_giou_dn_2: 0.8848  loss_ce_3: 2.567  loss_mask_3: 1.39  loss_dice_3: 4.474  loss_bbox_3: 2.01  loss_giou_3: 1.864  loss_ce_dn_3: 0.6126  loss_mask_dn_3: 1.89  loss_dice_dn_3: 4.478  loss_bbox_dn_3: 1.032  loss_giou_dn_3: 0.8924  loss_ce_4: 2.565  loss_mask_4: 1.278  loss_dice_4: 4.431  loss_bbox_4: 2.275  loss_giou_4: 1.655  loss_ce_dn_4: 0.4894  loss_mask_dn_4: 1.852  loss_dice_dn_4: 4.385  loss_bbox_dn_4: 1.094  loss_giou_dn_4: 0.8849  loss_ce_5: 2.77  loss_mask_5: 1.231  loss_dice_5: 4.251  loss_bbox_5: 2.074  loss_giou_5: 1.914  loss_ce_dn_5: 0.5447  loss_mask_dn_5: 1.862  loss_dice_dn_5: 4.383  loss_bbox_dn_5: 1.042  loss_giou_dn_5: 0.8744  loss_ce_6: 2.816  loss_mask_6: 1.253  loss_dice_6: 4.423  loss_bbox_6: 2.076  loss_giou_6: 1.789  loss_ce_dn_6: 0.6187  loss_mask_dn_6: 1.885  loss_dice_dn_6: 4.439  loss_bbox_dn_6: 1.063  loss_giou_dn_6: 0.866  loss_ce_7: 2.767  loss_mask_7: 1.161  loss_dice_7: 4.325  loss_bbox_7: 1.983  loss_giou_7: 1.818  loss_ce_dn_7: 0.6359  loss_mask_dn_7: 1.821  loss_dice_dn_7: 4.358  loss_bbox_dn_7: 1.082  loss_giou_dn_7: 0.8673  loss_ce_8: 2.614  loss_mask_8: 1.259  loss_dice_8: 4.384  loss_bbox_8: 2.026  loss_giou_8: 1.809  loss_ce_dn_8: 0.6743  loss_mask_dn_8: 1.808  loss_dice_dn_8: 4.336  loss_bbox_dn_8: 1.085  loss_giou_dn_8: 0.8698  loss_ce_interm: 4.517  loss_mask_interm: 1.613  loss_dice_interm: 4.477  loss_bbox_interm: 2.799  loss_giou_interm: 2.101    time: 0.6274  last_time: 0.4798  data_time: 0.0172  last_data_time: 0.0038   lr: 0.0001  max_mem: 2501M\n",
      "[03/14 17:02:07 d2.utils.events]:  eta: 0:33:44  iter: 39  total_loss: 208.2  loss_ce: 1.938  loss_mask: 1.022  loss_dice: 3.539  loss_bbox: 2.833  loss_giou: 1.852  loss_ce_dn: 0.1743  loss_mask_dn: 1.251  loss_dice_dn: 3.678  loss_bbox_dn: 0.8242  loss_giou_dn: 0.8031  loss_ce_0: 2.842  loss_mask_0: 1.235  loss_dice_0: 3.424  loss_bbox_0: 2.793  loss_giou_0: 2.27  loss_ce_dn_0: 0.1608  loss_mask_dn_0: 0.7807  loss_dice_dn_0: 4.918  loss_bbox_dn_0: 0.9644  loss_giou_dn_0: 0.8542  loss_ce_1: 2.344  loss_mask_1: 1.053  loss_dice_1: 3.574  loss_bbox_1: 2.499  loss_giou_1: 2.163  loss_ce_dn_1: 0.09766  loss_mask_dn_1: 1.206  loss_dice_dn_1: 3.775  loss_bbox_dn_1: 0.9067  loss_giou_dn_1: 0.8331  loss_ce_2: 2.153  loss_mask_2: 1.171  loss_dice_2: 3.568  loss_bbox_2: 2.27  loss_giou_2: 2.143  loss_ce_dn_2: 0.1028  loss_mask_dn_2: 1.434  loss_dice_dn_2: 3.728  loss_bbox_dn_2: 0.8741  loss_giou_dn_2: 0.8281  loss_ce_3: 1.987  loss_mask_3: 1.054  loss_dice_3: 3.604  loss_bbox_3: 2.615  loss_giou_3: 2.052  loss_ce_dn_3: 0.08668  loss_mask_dn_3: 1.327  loss_dice_dn_3: 3.733  loss_bbox_dn_3: 0.8699  loss_giou_dn_3: 0.8119  loss_ce_4: 1.918  loss_mask_4: 1.011  loss_dice_4: 3.609  loss_bbox_4: 2.187  loss_giou_4: 1.961  loss_ce_dn_4: 0.1009  loss_mask_dn_4: 1.305  loss_dice_dn_4: 3.605  loss_bbox_dn_4: 0.8475  loss_giou_dn_4: 0.8126  loss_ce_5: 1.979  loss_mask_5: 1.039  loss_dice_5: 3.597  loss_bbox_5: 2.217  loss_giou_5: 1.836  loss_ce_dn_5: 0.1225  loss_mask_dn_5: 1.264  loss_dice_dn_5: 3.637  loss_bbox_dn_5: 0.8433  loss_giou_dn_5: 0.8174  loss_ce_6: 1.98  loss_mask_6: 1.113  loss_dice_6: 3.51  loss_bbox_6: 2.212  loss_giou_6: 1.913  loss_ce_dn_6: 0.1235  loss_mask_dn_6: 1.334  loss_dice_dn_6: 3.668  loss_bbox_dn_6: 0.8323  loss_giou_dn_6: 0.8168  loss_ce_7: 1.937  loss_mask_7: 1.031  loss_dice_7: 3.562  loss_bbox_7: 2.661  loss_giou_7: 1.876  loss_ce_dn_7: 0.1417  loss_mask_dn_7: 1.24  loss_dice_dn_7: 3.644  loss_bbox_dn_7: 0.8307  loss_giou_dn_7: 0.8173  loss_ce_8: 1.907  loss_mask_8: 1.021  loss_dice_8: 3.505  loss_bbox_8: 2.874  loss_giou_8: 1.832  loss_ce_dn_8: 0.1567  loss_mask_dn_8: 1.249  loss_dice_dn_8: 3.637  loss_bbox_dn_8: 0.8283  loss_giou_dn_8: 0.8081  loss_ce_interm: 3.143  loss_mask_interm: 1.208  loss_dice_interm: 3.447  loss_bbox_interm: 2.656  loss_giou_interm: 1.852    time: 0.5639  last_time: 0.4763  data_time: 0.0036  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:02:18 d2.utils.events]:  eta: 0:33:35  iter: 59  total_loss: 164.4  loss_ce: 2.21  loss_mask: 0.9611  loss_dice: 3.016  loss_bbox: 1.056  loss_giou: 1.202  loss_ce_dn: 0.03961  loss_mask_dn: 0.9298  loss_dice_dn: 3.148  loss_bbox_dn: 0.7541  loss_giou_dn: 0.779  loss_ce_0: 2.521  loss_mask_0: 0.9996  loss_dice_0: 3.141  loss_bbox_0: 1.415  loss_giou_0: 1.497  loss_ce_dn_0: 0.1962  loss_mask_dn_0: 0.4782  loss_dice_dn_0: 4.908  loss_bbox_dn_0: 0.8662  loss_giou_dn_0: 0.8584  loss_ce_1: 2.332  loss_mask_1: 0.883  loss_dice_1: 3.01  loss_bbox_1: 1.158  loss_giou_1: 1.211  loss_ce_dn_1: 0.09538  loss_mask_dn_1: 1.108  loss_dice_dn_1: 3.417  loss_bbox_dn_1: 0.8287  loss_giou_dn_1: 0.8219  loss_ce_2: 2.243  loss_mask_2: 0.8698  loss_dice_2: 2.964  loss_bbox_2: 1.256  loss_giou_2: 1.222  loss_ce_dn_2: 0.05095  loss_mask_dn_2: 1.204  loss_dice_dn_2: 3.275  loss_bbox_dn_2: 0.8345  loss_giou_dn_2: 0.8086  loss_ce_3: 2.215  loss_mask_3: 0.8023  loss_dice_3: 3.116  loss_bbox_3: 1.193  loss_giou_3: 1.197  loss_ce_dn_3: 0.05151  loss_mask_dn_3: 1.017  loss_dice_dn_3: 3.267  loss_bbox_dn_3: 0.8172  loss_giou_dn_3: 0.8043  loss_ce_4: 2.206  loss_mask_4: 0.8127  loss_dice_4: 3.137  loss_bbox_4: 1.141  loss_giou_4: 1.2  loss_ce_dn_4: 0.04217  loss_mask_dn_4: 0.9981  loss_dice_dn_4: 3.129  loss_bbox_dn_4: 0.816  loss_giou_dn_4: 0.797  loss_ce_5: 2.03  loss_mask_5: 0.9642  loss_dice_5: 3.093  loss_bbox_5: 1.172  loss_giou_5: 1.276  loss_ce_dn_5: 0.02459  loss_mask_dn_5: 1.06  loss_dice_dn_5: 3.132  loss_bbox_dn_5: 0.7814  loss_giou_dn_5: 0.7812  loss_ce_6: 1.98  loss_mask_6: 0.947  loss_dice_6: 3.141  loss_bbox_6: 1.213  loss_giou_6: 1.241  loss_ce_dn_6: 0.02575  loss_mask_dn_6: 0.985  loss_dice_dn_6: 3.133  loss_bbox_dn_6: 0.7657  loss_giou_dn_6: 0.7781  loss_ce_7: 1.944  loss_mask_7: 0.9143  loss_dice_7: 3.012  loss_bbox_7: 1.167  loss_giou_7: 1.258  loss_ce_dn_7: 0.02823  loss_mask_dn_7: 0.9212  loss_dice_dn_7: 3.123  loss_bbox_dn_7: 0.7663  loss_giou_dn_7: 0.7799  loss_ce_8: 2.221  loss_mask_8: 0.9158  loss_dice_8: 2.982  loss_bbox_8: 1.141  loss_giou_8: 1.225  loss_ce_dn_8: 0.03245  loss_mask_dn_8: 0.9546  loss_dice_dn_8: 3.135  loss_bbox_dn_8: 0.7522  loss_giou_dn_8: 0.7781  loss_ce_interm: 2.617  loss_mask_interm: 1.198  loss_dice_interm: 2.859  loss_bbox_interm: 1.662  loss_giou_interm: 1.526    time: 0.5448  last_time: 0.5108  data_time: 0.0037  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:02:29 d2.utils.events]:  eta: 0:33:54  iter: 79  total_loss: 137.2  loss_ce: 2.065  loss_mask: 0.9135  loss_dice: 2.493  loss_bbox: 0.7343  loss_giou: 0.9299  loss_ce_dn: 0.0211  loss_mask_dn: 0.9218  loss_dice_dn: 2.706  loss_bbox_dn: 0.6921  loss_giou_dn: 0.7713  loss_ce_0: 2.377  loss_mask_0: 0.9227  loss_dice_0: 2.829  loss_bbox_0: 0.9805  loss_giou_0: 1.209  loss_ce_dn_0: 0.1629  loss_mask_dn_0: 0.6538  loss_dice_dn_0: 4.896  loss_bbox_dn_0: 0.7901  loss_giou_dn_0: 0.8472  loss_ce_1: 2.289  loss_mask_1: 0.7533  loss_dice_1: 2.764  loss_bbox_1: 0.8669  loss_giou_1: 1.098  loss_ce_dn_1: 0.0479  loss_mask_dn_1: 1.03  loss_dice_dn_1: 3.22  loss_bbox_dn_1: 0.7186  loss_giou_dn_1: 0.7908  loss_ce_2: 2.155  loss_mask_2: 0.9839  loss_dice_2: 2.661  loss_bbox_2: 0.7475  loss_giou_2: 1.042  loss_ce_dn_2: 0.02709  loss_mask_dn_2: 1.059  loss_dice_dn_2: 2.968  loss_bbox_dn_2: 0.7039  loss_giou_dn_2: 0.7695  loss_ce_3: 2.288  loss_mask_3: 0.9909  loss_dice_3: 2.663  loss_bbox_3: 0.713  loss_giou_3: 0.9661  loss_ce_dn_3: 0.02202  loss_mask_dn_3: 0.9473  loss_dice_dn_3: 2.846  loss_bbox_dn_3: 0.6577  loss_giou_dn_3: 0.7652  loss_ce_4: 2.238  loss_mask_4: 0.9326  loss_dice_4: 2.641  loss_bbox_4: 0.7084  loss_giou_4: 0.9611  loss_ce_dn_4: 0.02186  loss_mask_dn_4: 1.029  loss_dice_dn_4: 2.738  loss_bbox_dn_4: 0.6595  loss_giou_dn_4: 0.7658  loss_ce_5: 2.153  loss_mask_5: 0.9432  loss_dice_5: 2.553  loss_bbox_5: 0.7478  loss_giou_5: 0.9358  loss_ce_dn_5: 0.01243  loss_mask_dn_5: 0.9426  loss_dice_dn_5: 2.799  loss_bbox_dn_5: 0.66  loss_giou_dn_5: 0.7711  loss_ce_6: 2.05  loss_mask_6: 0.8249  loss_dice_6: 2.469  loss_bbox_6: 0.7082  loss_giou_6: 0.9574  loss_ce_dn_6: 0.01416  loss_mask_dn_6: 0.9159  loss_dice_dn_6: 2.766  loss_bbox_dn_6: 0.6672  loss_giou_dn_6: 0.7711  loss_ce_7: 1.993  loss_mask_7: 0.8612  loss_dice_7: 2.541  loss_bbox_7: 0.7676  loss_giou_7: 0.9576  loss_ce_dn_7: 0.01267  loss_mask_dn_7: 0.9577  loss_dice_dn_7: 2.714  loss_bbox_dn_7: 0.6847  loss_giou_dn_7: 0.7765  loss_ce_8: 2.039  loss_mask_8: 0.8604  loss_dice_8: 2.553  loss_bbox_8: 0.695  loss_giou_8: 0.9552  loss_ce_dn_8: 0.01569  loss_mask_dn_8: 0.9347  loss_dice_dn_8: 2.69  loss_bbox_dn_8: 0.69  loss_giou_dn_8: 0.7706  loss_ce_interm: 2.269  loss_mask_interm: 1.041  loss_dice_interm: 2.766  loss_bbox_interm: 0.7545  loss_giou_interm: 1.028    time: 0.5440  last_time: 0.5455  data_time: 0.0039  last_data_time: 0.0043   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:02:40 d2.data.build]: Distribution of instances among all 1 categories:\n",
      "|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   person   | 98           |\n",
      "|            |              |\n",
      "[03/14 17:02:40 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:02:40 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:02:40 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:02:40 d2.evaluation.coco_evaluation]: Trying to convert 'fiftyone_person_valid' to COCO format ...\n",
      "[03/14 17:02:40 d2.data.datasets.coco]: Converting annotations of dataset 'fiftyone_person_valid' to COCO format ...)\n",
      "[03/14 17:02:40 d2.data.datasets.coco]: Converting dataset dicts into COCO format\n",
      "[03/14 17:02:40 d2.data.datasets.coco]: Conversion finished, #images: 67, #annotations: 98\n",
      "[03/14 17:02:40 d2.data.datasets.coco]: Caching COCO format annotations at 'output/1/inference/fiftyone_person_valid_coco_format.json' ...\n",
      "[03/14 17:02:40 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:02:41 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0011 s/iter. Inference: 0.0819 s/iter. Eval: 0.0092 s/iter. Total: 0.0923 s/iter. ETA=0:00:05\n",
      "[03/14 17:02:46 d2.evaluation.evaluator]: Total inference time: 0:00:05.502875 (0.088756 s / iter per device, on 1 devices)\n",
      "[03/14 17:02:46 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076680 s / iter per device, on 1 devices)\n",
      "[03/14 17:02:46 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:02:46 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:02:46 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:02:46 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.099\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.375\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:02:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.251 | 9.947  | 1.273  | 7.108 | 1.097 |  nan  |\n",
      "[03/14 17:02:47 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:02:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.071\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.126\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.171\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:02:47 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.833 | 5.142  | 0.012  | 0.592 | 3.053 |  nan  |\n",
      "[03/14 17:02:47 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:02:47 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: 3.2506,9.9473,1.2731,7.1082,1.0968,nan\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:02:47 d2.evaluation.testing]: copypaste: 0.8335,5.1416,0.0119,0.5917,3.0529,nan\n",
      "[03/14 17:02:47 d2.utils.events]:  eta: 0:34:04  iter: 99  total_loss: 133.5  loss_ce: 1.737  loss_mask: 1.057  loss_dice: 2.418  loss_bbox: 0.6978  loss_giou: 0.9393  loss_ce_dn: 0.01708  loss_mask_dn: 1.231  loss_dice_dn: 2.592  loss_bbox_dn: 0.6557  loss_giou_dn: 0.7325  loss_ce_0: 2.396  loss_mask_0: 1.183  loss_dice_0: 2.277  loss_bbox_0: 0.8362  loss_giou_0: 1.113  loss_ce_dn_0: 0.1624  loss_mask_dn_0: 0.8654  loss_dice_dn_0: 4.796  loss_bbox_dn_0: 0.9052  loss_giou_dn_0: 0.8504  loss_ce_1: 2.364  loss_mask_1: 1.119  loss_dice_1: 2.393  loss_bbox_1: 0.6676  loss_giou_1: 0.9602  loss_ce_dn_1: 0.03744  loss_mask_dn_1: 1.286  loss_dice_dn_1: 2.856  loss_bbox_dn_1: 0.7509  loss_giou_dn_1: 0.7907  loss_ce_2: 2.202  loss_mask_2: 1.074  loss_dice_2: 2.491  loss_bbox_2: 0.67  loss_giou_2: 0.9521  loss_ce_dn_2: 0.01502  loss_mask_dn_2: 1.183  loss_dice_dn_2: 2.495  loss_bbox_dn_2: 0.708  loss_giou_dn_2: 0.7605  loss_ce_3: 2.032  loss_mask_3: 1.002  loss_dice_3: 2.416  loss_bbox_3: 0.8259  loss_giou_3: 0.9663  loss_ce_dn_3: 0.008836  loss_mask_dn_3: 1.137  loss_dice_dn_3: 2.529  loss_bbox_dn_3: 0.6794  loss_giou_dn_3: 0.7399  loss_ce_4: 2.075  loss_mask_4: 1.039  loss_dice_4: 2.34  loss_bbox_4: 0.6358  loss_giou_4: 0.9502  loss_ce_dn_4: 0.01032  loss_mask_dn_4: 1.297  loss_dice_dn_4: 2.484  loss_bbox_dn_4: 0.6752  loss_giou_dn_4: 0.7458  loss_ce_5: 1.97  loss_mask_5: 1.061  loss_dice_5: 2.358  loss_bbox_5: 0.6185  loss_giou_5: 0.9349  loss_ce_dn_5: 0.006212  loss_mask_dn_5: 1.226  loss_dice_dn_5: 2.491  loss_bbox_dn_5: 0.6684  loss_giou_dn_5: 0.7389  loss_ce_6: 1.991  loss_mask_6: 1.085  loss_dice_6: 2.285  loss_bbox_6: 0.593  loss_giou_6: 0.8929  loss_ce_dn_6: 0.00956  loss_mask_dn_6: 1.266  loss_dice_dn_6: 2.47  loss_bbox_dn_6: 0.6654  loss_giou_dn_6: 0.7322  loss_ce_7: 1.796  loss_mask_7: 1.148  loss_dice_7: 2.388  loss_bbox_7: 0.6476  loss_giou_7: 0.9626  loss_ce_dn_7: 0.009573  loss_mask_dn_7: 1.269  loss_dice_dn_7: 2.518  loss_bbox_dn_7: 0.6662  loss_giou_dn_7: 0.7306  loss_ce_8: 1.779  loss_mask_8: 1.102  loss_dice_8: 2.39  loss_bbox_8: 0.69  loss_giou_8: 0.9423  loss_ce_dn_8: 0.01187  loss_mask_dn_8: 1.29  loss_dice_dn_8: 2.54  loss_bbox_dn_8: 0.6526  loss_giou_dn_8: 0.7312  loss_ce_interm: 2.399  loss_mask_interm: 1.231  loss_dice_interm: 2.536  loss_bbox_interm: 0.8498  loss_giou_interm: 0.9914    time: 0.5409  last_time: 0.5291  data_time: 0.0037  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:02:57 d2.utils.events]:  eta: 0:33:40  iter: 119  total_loss: 125  loss_ce: 1.528  loss_mask: 0.6506  loss_dice: 2.651  loss_bbox: 0.7214  loss_giou: 1.122  loss_ce_dn: 0.006837  loss_mask_dn: 0.6407  loss_dice_dn: 2.61  loss_bbox_dn: 0.526  loss_giou_dn: 0.7489  loss_ce_0: 2.191  loss_mask_0: 0.6536  loss_dice_0: 2.474  loss_bbox_0: 0.9143  loss_giou_0: 1.301  loss_ce_dn_0: 0.2582  loss_mask_dn_0: 0.906  loss_dice_dn_0: 4.549  loss_bbox_dn_0: 0.6814  loss_giou_dn_0: 0.8441  loss_ce_1: 2.056  loss_mask_1: 0.6187  loss_dice_1: 2.5  loss_bbox_1: 0.7156  loss_giou_1: 1.061  loss_ce_dn_1: 0.03429  loss_mask_dn_1: 0.7224  loss_dice_dn_1: 2.923  loss_bbox_dn_1: 0.5899  loss_giou_dn_1: 0.7748  loss_ce_2: 1.844  loss_mask_2: 0.6005  loss_dice_2: 2.407  loss_bbox_2: 0.7383  loss_giou_2: 1.075  loss_ce_dn_2: 0.009117  loss_mask_dn_2: 0.6842  loss_dice_dn_2: 2.667  loss_bbox_dn_2: 0.5661  loss_giou_dn_2: 0.7487  loss_ce_3: 1.856  loss_mask_3: 0.5789  loss_dice_3: 2.46  loss_bbox_3: 0.7955  loss_giou_3: 1.087  loss_ce_dn_3: 0.008067  loss_mask_dn_3: 0.7066  loss_dice_dn_3: 2.571  loss_bbox_dn_3: 0.5462  loss_giou_dn_3: 0.7482  loss_ce_4: 1.765  loss_mask_4: 0.5914  loss_dice_4: 2.455  loss_bbox_4: 0.6514  loss_giou_4: 1.052  loss_ce_dn_4: 0.009302  loss_mask_dn_4: 0.6565  loss_dice_dn_4: 2.647  loss_bbox_dn_4: 0.5377  loss_giou_dn_4: 0.7481  loss_ce_5: 1.664  loss_mask_5: 0.6705  loss_dice_5: 2.516  loss_bbox_5: 0.6596  loss_giou_5: 1.114  loss_ce_dn_5: 0.005206  loss_mask_dn_5: 0.6387  loss_dice_dn_5: 2.63  loss_bbox_dn_5: 0.528  loss_giou_dn_5: 0.7481  loss_ce_6: 1.595  loss_mask_6: 0.6762  loss_dice_6: 2.488  loss_bbox_6: 0.681  loss_giou_6: 1.113  loss_ce_dn_6: 0.005899  loss_mask_dn_6: 0.639  loss_dice_dn_6: 2.588  loss_bbox_dn_6: 0.5238  loss_giou_dn_6: 0.7468  loss_ce_7: 1.458  loss_mask_7: 0.6658  loss_dice_7: 2.48  loss_bbox_7: 0.7243  loss_giou_7: 1.125  loss_ce_dn_7: 0.004588  loss_mask_dn_7: 0.6582  loss_dice_dn_7: 2.624  loss_bbox_dn_7: 0.5245  loss_giou_dn_7: 0.7511  loss_ce_8: 1.507  loss_mask_8: 0.7115  loss_dice_8: 2.58  loss_bbox_8: 0.7383  loss_giou_8: 1.119  loss_ce_dn_8: 0.005326  loss_mask_dn_8: 0.6481  loss_dice_dn_8: 2.594  loss_bbox_dn_8: 0.5284  loss_giou_dn_8: 0.7494  loss_ce_interm: 2.17  loss_mask_interm: 0.6899  loss_dice_interm: 2.589  loss_bbox_interm: 0.722  loss_giou_interm: 1.138    time: 0.5334  last_time: 0.5859  data_time: 0.0035  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:03:07 d2.utils.events]:  eta: 0:33:26  iter: 139  total_loss: 125.2  loss_ce: 1.371  loss_mask: 0.7017  loss_dice: 2.262  loss_bbox: 0.8315  loss_giou: 1.015  loss_ce_dn: 0.005241  loss_mask_dn: 0.6976  loss_dice_dn: 2.303  loss_bbox_dn: 0.5206  loss_giou_dn: 0.718  loss_ce_0: 2.224  loss_mask_0: 0.7487  loss_dice_0: 2.227  loss_bbox_0: 0.9211  loss_giou_0: 1.073  loss_ce_dn_0: 0.1578  loss_mask_dn_0: 1.158  loss_dice_dn_0: 4.391  loss_bbox_dn_0: 0.7843  loss_giou_dn_0: 0.8494  loss_ce_1: 2.277  loss_mask_1: 0.6588  loss_dice_1: 2.246  loss_bbox_1: 0.5886  loss_giou_1: 0.9988  loss_ce_dn_1: 0.02107  loss_mask_dn_1: 0.7622  loss_dice_dn_1: 2.412  loss_bbox_dn_1: 0.6287  loss_giou_dn_1: 0.7622  loss_ce_2: 1.904  loss_mask_2: 0.7926  loss_dice_2: 2.123  loss_bbox_2: 0.8175  loss_giou_2: 1.006  loss_ce_dn_2: 0.006823  loss_mask_dn_2: 0.7764  loss_dice_dn_2: 2.35  loss_bbox_dn_2: 0.5972  loss_giou_dn_2: 0.7383  loss_ce_3: 1.934  loss_mask_3: 0.7162  loss_dice_3: 2.134  loss_bbox_3: 0.7265  loss_giou_3: 0.9775  loss_ce_dn_3: 0.003932  loss_mask_dn_3: 0.7611  loss_dice_dn_3: 2.282  loss_bbox_dn_3: 0.5591  loss_giou_dn_3: 0.7283  loss_ce_4: 1.745  loss_mask_4: 0.6669  loss_dice_4: 2.287  loss_bbox_4: 0.681  loss_giou_4: 0.9527  loss_ce_dn_4: 0.005824  loss_mask_dn_4: 0.6947  loss_dice_dn_4: 2.342  loss_bbox_dn_4: 0.5325  loss_giou_dn_4: 0.7253  loss_ce_5: 1.659  loss_mask_5: 0.6898  loss_dice_5: 2.261  loss_bbox_5: 0.7318  loss_giou_5: 1.038  loss_ce_dn_5: 0.00431  loss_mask_dn_5: 0.676  loss_dice_dn_5: 2.314  loss_bbox_dn_5: 0.5373  loss_giou_dn_5: 0.7187  loss_ce_6: 1.498  loss_mask_6: 0.7311  loss_dice_6: 2.224  loss_bbox_6: 0.7194  loss_giou_6: 1.012  loss_ce_dn_6: 0.004877  loss_mask_dn_6: 0.7164  loss_dice_dn_6: 2.336  loss_bbox_dn_6: 0.5325  loss_giou_dn_6: 0.7188  loss_ce_7: 1.363  loss_mask_7: 0.7143  loss_dice_7: 2.241  loss_bbox_7: 0.8027  loss_giou_7: 1.071  loss_ce_dn_7: 0.003547  loss_mask_dn_7: 0.692  loss_dice_dn_7: 2.322  loss_bbox_dn_7: 0.5208  loss_giou_dn_7: 0.721  loss_ce_8: 1.366  loss_mask_8: 0.7312  loss_dice_8: 2.291  loss_bbox_8: 0.8172  loss_giou_8: 1.024  loss_ce_dn_8: 0.003962  loss_mask_dn_8: 0.6968  loss_dice_dn_8: 2.317  loss_bbox_dn_8: 0.5245  loss_giou_dn_8: 0.718  loss_ce_interm: 2.151  loss_mask_interm: 0.8927  loss_dice_interm: 2.331  loss_bbox_interm: 0.7516  loss_giou_interm: 0.9199    time: 0.5304  last_time: 0.5032  data_time: 0.0038  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:03:17 d2.utils.events]:  eta: 0:33:12  iter: 159  total_loss: 117.7  loss_ce: 1.409  loss_mask: 0.6774  loss_dice: 2.353  loss_bbox: 0.8043  loss_giou: 1.097  loss_ce_dn: 0.005335  loss_mask_dn: 0.7371  loss_dice_dn: 2.276  loss_bbox_dn: 0.5721  loss_giou_dn: 0.7088  loss_ce_0: 2.026  loss_mask_0: 0.8316  loss_dice_0: 2.334  loss_bbox_0: 0.9696  loss_giou_0: 1.252  loss_ce_dn_0: 0.1542  loss_mask_dn_0: 0.9628  loss_dice_dn_0: 4.208  loss_bbox_dn_0: 0.7882  loss_giou_dn_0: 0.848  loss_ce_1: 2.069  loss_mask_1: 0.676  loss_dice_1: 2.207  loss_bbox_1: 0.8026  loss_giou_1: 1.037  loss_ce_dn_1: 0.01359  loss_mask_dn_1: 0.7538  loss_dice_dn_1: 2.441  loss_bbox_dn_1: 0.6379  loss_giou_dn_1: 0.7644  loss_ce_2: 1.743  loss_mask_2: 0.75  loss_dice_2: 2.027  loss_bbox_2: 0.9087  loss_giou_2: 1.007  loss_ce_dn_2: 0.004163  loss_mask_dn_2: 0.6908  loss_dice_dn_2: 2.274  loss_bbox_dn_2: 0.6055  loss_giou_dn_2: 0.7085  loss_ce_3: 1.608  loss_mask_3: 0.7286  loss_dice_3: 2.149  loss_bbox_3: 0.9206  loss_giou_3: 0.9524  loss_ce_dn_3: 0.003331  loss_mask_dn_3: 0.7166  loss_dice_dn_3: 2.284  loss_bbox_dn_3: 0.5923  loss_giou_dn_3: 0.7126  loss_ce_4: 1.562  loss_mask_4: 0.7557  loss_dice_4: 2.215  loss_bbox_4: 0.8512  loss_giou_4: 0.9832  loss_ce_dn_4: 0.005276  loss_mask_dn_4: 0.6782  loss_dice_dn_4: 2.288  loss_bbox_dn_4: 0.5817  loss_giou_dn_4: 0.7078  loss_ce_5: 1.477  loss_mask_5: 0.6862  loss_dice_5: 2.219  loss_bbox_5: 0.8362  loss_giou_5: 0.9981  loss_ce_dn_5: 0.00337  loss_mask_dn_5: 0.7415  loss_dice_dn_5: 2.235  loss_bbox_dn_5: 0.5861  loss_giou_dn_5: 0.7068  loss_ce_6: 1.464  loss_mask_6: 0.7135  loss_dice_6: 2.154  loss_bbox_6: 0.7448  loss_giou_6: 1.046  loss_ce_dn_6: 0.004537  loss_mask_dn_6: 0.7436  loss_dice_dn_6: 2.228  loss_bbox_dn_6: 0.5763  loss_giou_dn_6: 0.7046  loss_ce_7: 1.376  loss_mask_7: 0.7214  loss_dice_7: 2.238  loss_bbox_7: 0.8187  loss_giou_7: 1.061  loss_ce_dn_7: 0.003618  loss_mask_dn_7: 0.7605  loss_dice_dn_7: 2.274  loss_bbox_dn_7: 0.5702  loss_giou_dn_7: 0.7125  loss_ce_8: 1.446  loss_mask_8: 0.6885  loss_dice_8: 2.292  loss_bbox_8: 0.8179  loss_giou_8: 1.08  loss_ce_dn_8: 0.004049  loss_mask_dn_8: 0.7299  loss_dice_dn_8: 2.289  loss_bbox_dn_8: 0.5745  loss_giou_dn_8: 0.7086  loss_ce_interm: 2.014  loss_mask_interm: 0.8272  loss_dice_interm: 2.355  loss_bbox_interm: 0.7932  loss_giou_interm: 1.028    time: 0.5263  last_time: 0.5277  data_time: 0.0038  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:03:27 d2.utils.events]:  eta: 0:32:41  iter: 179  total_loss: 121.5  loss_ce: 1.134  loss_mask: 0.847  loss_dice: 2.364  loss_bbox: 0.7459  loss_giou: 0.9861  loss_ce_dn: 0.006358  loss_mask_dn: 0.9928  loss_dice_dn: 2.393  loss_bbox_dn: 0.5509  loss_giou_dn: 0.6518  loss_ce_0: 1.965  loss_mask_0: 1.038  loss_dice_0: 2.371  loss_bbox_0: 0.886  loss_giou_0: 1.198  loss_ce_dn_0: 0.2421  loss_mask_dn_0: 1.312  loss_dice_dn_0: 3.996  loss_bbox_dn_0: 0.9383  loss_giou_dn_0: 0.8473  loss_ce_1: 2.053  loss_mask_1: 1.036  loss_dice_1: 2.346  loss_bbox_1: 0.5984  loss_giou_1: 0.896  loss_ce_dn_1: 0.01746  loss_mask_dn_1: 0.998  loss_dice_dn_1: 2.385  loss_bbox_dn_1: 0.7111  loss_giou_dn_1: 0.7403  loss_ce_2: 1.615  loss_mask_2: 0.8756  loss_dice_2: 2.289  loss_bbox_2: 0.6655  loss_giou_2: 1.007  loss_ce_dn_2: 0.005395  loss_mask_dn_2: 0.9718  loss_dice_dn_2: 2.353  loss_bbox_dn_2: 0.5913  loss_giou_dn_2: 0.704  loss_ce_3: 1.445  loss_mask_3: 0.8519  loss_dice_3: 2.31  loss_bbox_3: 0.6433  loss_giou_3: 0.9677  loss_ce_dn_3: 0.003804  loss_mask_dn_3: 1.035  loss_dice_dn_3: 2.354  loss_bbox_dn_3: 0.5427  loss_giou_dn_3: 0.6667  loss_ce_4: 1.401  loss_mask_4: 0.8874  loss_dice_4: 2.376  loss_bbox_4: 0.6616  loss_giou_4: 1.002  loss_ce_dn_4: 0.004313  loss_mask_dn_4: 1.041  loss_dice_dn_4: 2.416  loss_bbox_dn_4: 0.5464  loss_giou_dn_4: 0.6593  loss_ce_5: 1.304  loss_mask_5: 0.8205  loss_dice_5: 2.29  loss_bbox_5: 0.7214  loss_giou_5: 1.014  loss_ce_dn_5: 0.003496  loss_mask_dn_5: 1.039  loss_dice_dn_5: 2.289  loss_bbox_dn_5: 0.553  loss_giou_dn_5: 0.6575  loss_ce_6: 1.173  loss_mask_6: 0.8691  loss_dice_6: 2.344  loss_bbox_6: 0.7195  loss_giou_6: 1.014  loss_ce_dn_6: 0.003908  loss_mask_dn_6: 1.016  loss_dice_dn_6: 2.325  loss_bbox_dn_6: 0.5591  loss_giou_dn_6: 0.6546  loss_ce_7: 1.158  loss_mask_7: 0.8435  loss_dice_7: 2.393  loss_bbox_7: 0.7486  loss_giou_7: 1.01  loss_ce_dn_7: 0.004349  loss_mask_dn_7: 1.017  loss_dice_dn_7: 2.327  loss_bbox_dn_7: 0.5546  loss_giou_dn_7: 0.6529  loss_ce_8: 1.129  loss_mask_8: 0.8291  loss_dice_8: 2.34  loss_bbox_8: 0.7444  loss_giou_8: 1.016  loss_ce_dn_8: 0.004921  loss_mask_dn_8: 0.9953  loss_dice_dn_8: 2.363  loss_bbox_dn_8: 0.5511  loss_giou_dn_8: 0.6503  loss_ce_interm: 1.908  loss_mask_interm: 0.9228  loss_dice_interm: 2.482  loss_bbox_interm: 0.6537  loss_giou_interm: 0.8784    time: 0.5232  last_time: 0.5049  data_time: 0.0037  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:03:38 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:03:38 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:03:38 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:03:38 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:03:39 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0007 s/iter. Inference: 0.0821 s/iter. Eval: 0.0086 s/iter. Total: 0.0915 s/iter. ETA=0:00:05\n",
      "[03/14 17:03:44 d2.evaluation.evaluator]: Total inference time: 0:00:05.416288 (0.087359 s / iter per device, on 1 devices)\n",
      "[03/14 17:03:44 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075752 s / iter per device, on 1 devices)\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.091\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.068\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.113\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.335\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.476\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 9.143 | 21.521 | 6.824  | 11.319 | 4.830 |  nan  |\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:03:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.090\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.126\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.199 | 11.935 | 0.029  | 1.643 | 9.656 |  nan  |\n",
      "[03/14 17:03:44 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:03:44 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: 9.1433,21.5210,6.8239,11.3187,4.8300,nan\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:03:44 d2.evaluation.testing]: copypaste: 2.1994,11.9348,0.0295,1.6427,9.6561,nan\n",
      "[03/14 17:03:44 d2.utils.events]:  eta: 0:32:27  iter: 199  total_loss: 109  loss_ce: 1.369  loss_mask: 0.7336  loss_dice: 1.778  loss_bbox: 0.7317  loss_giou: 0.9121  loss_ce_dn: 0.009269  loss_mask_dn: 0.7125  loss_dice_dn: 1.85  loss_bbox_dn: 0.513  loss_giou_dn: 0.6443  loss_ce_0: 2.115  loss_mask_0: 0.7592  loss_dice_0: 1.857  loss_bbox_0: 0.7459  loss_giou_0: 0.9948  loss_ce_dn_0: 0.1478  loss_mask_dn_0: 1.244  loss_dice_dn_0: 3.925  loss_bbox_dn_0: 0.9172  loss_giou_dn_0: 0.8543  loss_ce_1: 1.982  loss_mask_1: 0.7185  loss_dice_1: 1.757  loss_bbox_1: 0.6123  loss_giou_1: 0.7277  loss_ce_dn_1: 0.008141  loss_mask_dn_1: 0.8434  loss_dice_dn_1: 1.895  loss_bbox_dn_1: 0.6264  loss_giou_dn_1: 0.7034  loss_ce_2: 1.876  loss_mask_2: 0.7032  loss_dice_2: 1.78  loss_bbox_2: 0.6118  loss_giou_2: 0.8024  loss_ce_dn_2: 0.004223  loss_mask_dn_2: 0.7986  loss_dice_dn_2: 1.83  loss_bbox_dn_2: 0.5431  loss_giou_dn_2: 0.6687  loss_ce_3: 1.559  loss_mask_3: 0.7116  loss_dice_3: 1.768  loss_bbox_3: 0.5688  loss_giou_3: 0.8026  loss_ce_dn_3: 0.002959  loss_mask_dn_3: 0.7549  loss_dice_dn_3: 1.82  loss_bbox_dn_3: 0.5543  loss_giou_dn_3: 0.6557  loss_ce_4: 1.672  loss_mask_4: 0.7127  loss_dice_4: 1.768  loss_bbox_4: 0.71  loss_giou_4: 0.8605  loss_ce_dn_4: 0.004667  loss_mask_dn_4: 0.741  loss_dice_dn_4: 1.815  loss_bbox_dn_4: 0.5391  loss_giou_dn_4: 0.6445  loss_ce_5: 1.597  loss_mask_5: 0.7528  loss_dice_5: 1.784  loss_bbox_5: 0.6941  loss_giou_5: 0.8517  loss_ce_dn_5: 0.004011  loss_mask_dn_5: 0.7311  loss_dice_dn_5: 1.819  loss_bbox_dn_5: 0.5285  loss_giou_dn_5: 0.6398  loss_ce_6: 1.506  loss_mask_6: 0.7567  loss_dice_6: 1.816  loss_bbox_6: 0.7338  loss_giou_6: 0.9147  loss_ce_dn_6: 0.00643  loss_mask_dn_6: 0.7133  loss_dice_dn_6: 1.799  loss_bbox_dn_6: 0.5196  loss_giou_dn_6: 0.6368  loss_ce_7: 1.421  loss_mask_7: 0.7419  loss_dice_7: 1.847  loss_bbox_7: 0.7369  loss_giou_7: 0.9226  loss_ce_dn_7: 0.00569  loss_mask_dn_7: 0.7245  loss_dice_dn_7: 1.82  loss_bbox_dn_7: 0.5207  loss_giou_dn_7: 0.642  loss_ce_8: 1.253  loss_mask_8: 0.7417  loss_dice_8: 1.825  loss_bbox_8: 0.7479  loss_giou_8: 0.916  loss_ce_dn_8: 0.007177  loss_mask_dn_8: 0.7067  loss_dice_dn_8: 1.83  loss_bbox_dn_8: 0.5123  loss_giou_dn_8: 0.6439  loss_ce_interm: 2.193  loss_mask_interm: 0.7972  loss_dice_interm: 1.747  loss_bbox_interm: 1.003  loss_giou_interm: 0.9783    time: 0.5204  last_time: 0.4531  data_time: 0.0036  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:03:54 d2.utils.events]:  eta: 0:32:11  iter: 219  total_loss: 111.8  loss_ce: 1.305  loss_mask: 0.7994  loss_dice: 2.371  loss_bbox: 0.6221  loss_giou: 0.9588  loss_ce_dn: 0.006725  loss_mask_dn: 0.7618  loss_dice_dn: 2.358  loss_bbox_dn: 0.5246  loss_giou_dn: 0.6759  loss_ce_0: 1.97  loss_mask_0: 0.7629  loss_dice_0: 2.231  loss_bbox_0: 0.7445  loss_giou_0: 1.038  loss_ce_dn_0: 0.1873  loss_mask_dn_0: 1.149  loss_dice_dn_0: 4.296  loss_bbox_dn_0: 0.7989  loss_giou_dn_0: 0.851  loss_ce_1: 1.94  loss_mask_1: 0.6937  loss_dice_1: 2.223  loss_bbox_1: 0.4923  loss_giou_1: 0.87  loss_ce_dn_1: 0.01031  loss_mask_dn_1: 0.7687  loss_dice_dn_1: 2.392  loss_bbox_dn_1: 0.5867  loss_giou_dn_1: 0.6955  loss_ce_2: 1.693  loss_mask_2: 0.6308  loss_dice_2: 2.221  loss_bbox_2: 0.5645  loss_giou_2: 0.8581  loss_ce_dn_2: 0.004889  loss_mask_dn_2: 0.7583  loss_dice_dn_2: 2.357  loss_bbox_dn_2: 0.5527  loss_giou_dn_2: 0.6845  loss_ce_3: 1.336  loss_mask_3: 0.6485  loss_dice_3: 2.243  loss_bbox_3: 0.6392  loss_giou_3: 0.9179  loss_ce_dn_3: 0.004064  loss_mask_dn_3: 0.7273  loss_dice_dn_3: 2.375  loss_bbox_dn_3: 0.5404  loss_giou_dn_3: 0.6785  loss_ce_4: 1.393  loss_mask_4: 0.6817  loss_dice_4: 2.287  loss_bbox_4: 0.5617  loss_giou_4: 0.8597  loss_ce_dn_4: 0.004608  loss_mask_dn_4: 0.7034  loss_dice_dn_4: 2.372  loss_bbox_dn_4: 0.5519  loss_giou_dn_4: 0.6853  loss_ce_5: 1.402  loss_mask_5: 0.6324  loss_dice_5: 2.26  loss_bbox_5: 0.5296  loss_giou_5: 0.9561  loss_ce_dn_5: 0.004923  loss_mask_dn_5: 0.7389  loss_dice_dn_5: 2.384  loss_bbox_dn_5: 0.5485  loss_giou_dn_5: 0.6782  loss_ce_6: 1.391  loss_mask_6: 0.6565  loss_dice_6: 2.358  loss_bbox_6: 0.5716  loss_giou_6: 0.959  loss_ce_dn_6: 0.006378  loss_mask_dn_6: 0.7503  loss_dice_dn_6: 2.371  loss_bbox_dn_6: 0.5482  loss_giou_dn_6: 0.6766  loss_ce_7: 1.235  loss_mask_7: 0.8193  loss_dice_7: 2.367  loss_bbox_7: 0.5634  loss_giou_7: 0.9447  loss_ce_dn_7: 0.004555  loss_mask_dn_7: 0.7505  loss_dice_dn_7: 2.385  loss_bbox_dn_7: 0.5279  loss_giou_dn_7: 0.6711  loss_ce_8: 1.256  loss_mask_8: 0.8075  loss_dice_8: 2.357  loss_bbox_8: 0.575  loss_giou_8: 0.9614  loss_ce_dn_8: 0.005304  loss_mask_dn_8: 0.7544  loss_dice_dn_8: 2.367  loss_bbox_dn_8: 0.5296  loss_giou_dn_8: 0.6773  loss_ce_interm: 2.068  loss_mask_interm: 0.8552  loss_dice_interm: 2.295  loss_bbox_interm: 0.692  loss_giou_interm: 1.046    time: 0.5194  last_time: 0.5075  data_time: 0.0035  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:04:04 d2.utils.events]:  eta: 0:31:53  iter: 239  total_loss: 108.7  loss_ce: 1.355  loss_mask: 0.7078  loss_dice: 1.827  loss_bbox: 0.6214  loss_giou: 0.8097  loss_ce_dn: 0.005387  loss_mask_dn: 0.7194  loss_dice_dn: 2.009  loss_bbox_dn: 0.5757  loss_giou_dn: 0.6481  loss_ce_0: 2.085  loss_mask_0: 0.8834  loss_dice_0: 2.042  loss_bbox_0: 0.9785  loss_giou_0: 1.064  loss_ce_dn_0: 0.1834  loss_mask_dn_0: 1.125  loss_dice_dn_0: 3.736  loss_bbox_dn_0: 0.9791  loss_giou_dn_0: 0.8496  loss_ce_1: 2.098  loss_mask_1: 0.6773  loss_dice_1: 1.874  loss_bbox_1: 0.7444  loss_giou_1: 0.7817  loss_ce_dn_1: 0.01146  loss_mask_dn_1: 0.7739  loss_dice_dn_1: 2.053  loss_bbox_dn_1: 0.6849  loss_giou_dn_1: 0.707  loss_ce_2: 1.469  loss_mask_2: 0.6678  loss_dice_2: 1.763  loss_bbox_2: 0.8323  loss_giou_2: 0.837  loss_ce_dn_2: 0.003335  loss_mask_dn_2: 0.708  loss_dice_dn_2: 1.982  loss_bbox_dn_2: 0.6347  loss_giou_dn_2: 0.6668  loss_ce_3: 1.53  loss_mask_3: 0.7596  loss_dice_3: 2.06  loss_bbox_3: 0.6253  loss_giou_3: 0.8235  loss_ce_dn_3: 0.002082  loss_mask_dn_3: 0.77  loss_dice_dn_3: 1.987  loss_bbox_dn_3: 0.6059  loss_giou_dn_3: 0.6502  loss_ce_4: 1.627  loss_mask_4: 0.6521  loss_dice_4: 1.804  loss_bbox_4: 0.7662  loss_giou_4: 0.8273  loss_ce_dn_4: 0.003514  loss_mask_dn_4: 0.7972  loss_dice_dn_4: 1.982  loss_bbox_dn_4: 0.6049  loss_giou_dn_4: 0.651  loss_ce_5: 1.652  loss_mask_5: 0.6636  loss_dice_5: 1.773  loss_bbox_5: 0.6833  loss_giou_5: 0.7785  loss_ce_dn_5: 0.002732  loss_mask_dn_5: 0.7832  loss_dice_dn_5: 1.983  loss_bbox_dn_5: 0.5946  loss_giou_dn_5: 0.6457  loss_ce_6: 1.414  loss_mask_6: 0.7075  loss_dice_6: 1.746  loss_bbox_6: 0.6131  loss_giou_6: 0.843  loss_ce_dn_6: 0.003912  loss_mask_dn_6: 0.7555  loss_dice_dn_6: 1.962  loss_bbox_dn_6: 0.5851  loss_giou_dn_6: 0.648  loss_ce_7: 1.314  loss_mask_7: 0.8287  loss_dice_7: 1.833  loss_bbox_7: 0.66  loss_giou_7: 0.8128  loss_ce_dn_7: 0.003801  loss_mask_dn_7: 0.7323  loss_dice_dn_7: 1.982  loss_bbox_dn_7: 0.5784  loss_giou_dn_7: 0.6457  loss_ce_8: 1.303  loss_mask_8: 0.7773  loss_dice_8: 2.028  loss_bbox_8: 0.5929  loss_giou_8: 0.8025  loss_ce_dn_8: 0.004069  loss_mask_dn_8: 0.7337  loss_dice_dn_8: 1.98  loss_bbox_dn_8: 0.575  loss_giou_dn_8: 0.6473  loss_ce_interm: 1.962  loss_mask_interm: 0.8335  loss_dice_interm: 1.941  loss_bbox_interm: 0.8359  loss_giou_interm: 0.8713    time: 0.5163  last_time: 0.4424  data_time: 0.0035  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:04:14 d2.utils.events]:  eta: 0:31:38  iter: 259  total_loss: 105.3  loss_ce: 1.348  loss_mask: 0.6071  loss_dice: 1.942  loss_bbox: 0.5763  loss_giou: 0.7711  loss_ce_dn: 0.004645  loss_mask_dn: 0.7083  loss_dice_dn: 1.848  loss_bbox_dn: 0.5268  loss_giou_dn: 0.637  loss_ce_0: 1.773  loss_mask_0: 0.724  loss_dice_0: 1.85  loss_bbox_0: 0.8266  loss_giou_0: 0.9153  loss_ce_dn_0: 0.1368  loss_mask_dn_0: 1.097  loss_dice_dn_0: 4.122  loss_bbox_dn_0: 0.8232  loss_giou_dn_0: 0.8577  loss_ce_1: 1.904  loss_mask_1: 0.6711  loss_dice_1: 1.821  loss_bbox_1: 0.5152  loss_giou_1: 0.776  loss_ce_dn_1: 0.01249  loss_mask_dn_1: 0.6618  loss_dice_dn_1: 2  loss_bbox_dn_1: 0.6516  loss_giou_dn_1: 0.715  loss_ce_2: 1.646  loss_mask_2: 0.6599  loss_dice_2: 1.912  loss_bbox_2: 0.6281  loss_giou_2: 0.8161  loss_ce_dn_2: 0.003929  loss_mask_dn_2: 0.6933  loss_dice_dn_2: 1.921  loss_bbox_dn_2: 0.5512  loss_giou_dn_2: 0.668  loss_ce_3: 1.562  loss_mask_3: 0.6958  loss_dice_3: 1.934  loss_bbox_3: 0.5498  loss_giou_3: 0.7567  loss_ce_dn_3: 0.002246  loss_mask_dn_3: 0.6553  loss_dice_dn_3: 1.854  loss_bbox_dn_3: 0.5358  loss_giou_dn_3: 0.653  loss_ce_4: 1.588  loss_mask_4: 0.6143  loss_dice_4: 1.942  loss_bbox_4: 0.534  loss_giou_4: 0.7254  loss_ce_dn_4: 0.002987  loss_mask_dn_4: 0.6681  loss_dice_dn_4: 1.853  loss_bbox_dn_4: 0.5222  loss_giou_dn_4: 0.6495  loss_ce_5: 1.496  loss_mask_5: 0.5724  loss_dice_5: 1.867  loss_bbox_5: 0.5144  loss_giou_5: 0.7319  loss_ce_dn_5: 0.002489  loss_mask_dn_5: 0.6611  loss_dice_dn_5: 1.856  loss_bbox_dn_5: 0.5271  loss_giou_dn_5: 0.6393  loss_ce_6: 1.346  loss_mask_6: 0.6584  loss_dice_6: 1.899  loss_bbox_6: 0.573  loss_giou_6: 0.7531  loss_ce_dn_6: 0.003224  loss_mask_dn_6: 0.695  loss_dice_dn_6: 1.85  loss_bbox_dn_6: 0.5324  loss_giou_dn_6: 0.6376  loss_ce_7: 1.268  loss_mask_7: 0.6164  loss_dice_7: 1.918  loss_bbox_7: 0.5637  loss_giou_7: 0.7277  loss_ce_dn_7: 0.003118  loss_mask_dn_7: 0.6919  loss_dice_dn_7: 1.852  loss_bbox_dn_7: 0.5286  loss_giou_dn_7: 0.6323  loss_ce_8: 1.336  loss_mask_8: 0.6288  loss_dice_8: 1.9  loss_bbox_8: 0.5767  loss_giou_8: 0.7638  loss_ce_dn_8: 0.003306  loss_mask_dn_8: 0.7189  loss_dice_dn_8: 1.849  loss_bbox_dn_8: 0.5176  loss_giou_dn_8: 0.6334  loss_ce_interm: 1.81  loss_mask_interm: 0.8056  loss_dice_interm: 1.957  loss_bbox_interm: 0.7592  loss_giou_interm: 0.9575    time: 0.5144  last_time: 0.4938  data_time: 0.0035  last_data_time: 0.0030   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:04:24 d2.utils.events]:  eta: 0:31:26  iter: 279  total_loss: 112  loss_ce: 1.308  loss_mask: 0.5726  loss_dice: 2.138  loss_bbox: 0.6696  loss_giou: 0.998  loss_ce_dn: 0.003638  loss_mask_dn: 0.5966  loss_dice_dn: 1.96  loss_bbox_dn: 0.526  loss_giou_dn: 0.6838  loss_ce_0: 2.099  loss_mask_0: 0.587  loss_dice_0: 1.963  loss_bbox_0: 0.8117  loss_giou_0: 0.9268  loss_ce_dn_0: 0.1332  loss_mask_dn_0: 1.047  loss_dice_dn_0: 4.085  loss_bbox_dn_0: 0.8282  loss_giou_dn_0: 0.8567  loss_ce_1: 2.073  loss_mask_1: 0.6164  loss_dice_1: 1.975  loss_bbox_1: 0.5755  loss_giou_1: 0.8923  loss_ce_dn_1: 0.005554  loss_mask_dn_1: 0.6421  loss_dice_dn_1: 2.031  loss_bbox_dn_1: 0.5742  loss_giou_dn_1: 0.7049  loss_ce_2: 1.5  loss_mask_2: 0.6166  loss_dice_2: 1.992  loss_bbox_2: 0.646  loss_giou_2: 0.8752  loss_ce_dn_2: 0.00166  loss_mask_dn_2: 0.6485  loss_dice_dn_2: 2.001  loss_bbox_dn_2: 0.5286  loss_giou_dn_2: 0.691  loss_ce_3: 1.529  loss_mask_3: 0.6264  loss_dice_3: 2.006  loss_bbox_3: 0.6315  loss_giou_3: 0.905  loss_ce_dn_3: 0.00134  loss_mask_dn_3: 0.6606  loss_dice_dn_3: 1.974  loss_bbox_dn_3: 0.541  loss_giou_dn_3: 0.6852  loss_ce_4: 1.468  loss_mask_4: 0.651  loss_dice_4: 2.183  loss_bbox_4: 0.6137  loss_giou_4: 0.9467  loss_ce_dn_4: 0.002018  loss_mask_dn_4: 0.6399  loss_dice_dn_4: 1.928  loss_bbox_dn_4: 0.5362  loss_giou_dn_4: 0.6784  loss_ce_5: 1.285  loss_mask_5: 0.6065  loss_dice_5: 2.249  loss_bbox_5: 0.6344  loss_giou_5: 0.9297  loss_ce_dn_5: 0.001473  loss_mask_dn_5: 0.6192  loss_dice_dn_5: 1.953  loss_bbox_dn_5: 0.5199  loss_giou_dn_5: 0.6918  loss_ce_6: 1.225  loss_mask_6: 0.6636  loss_dice_6: 2.149  loss_bbox_6: 0.6766  loss_giou_6: 0.9698  loss_ce_dn_6: 0.002962  loss_mask_dn_6: 0.6181  loss_dice_dn_6: 1.94  loss_bbox_dn_6: 0.5266  loss_giou_dn_6: 0.6864  loss_ce_7: 1.309  loss_mask_7: 0.6237  loss_dice_7: 2.161  loss_bbox_7: 0.6818  loss_giou_7: 0.9168  loss_ce_dn_7: 0.002361  loss_mask_dn_7: 0.6113  loss_dice_dn_7: 1.949  loss_bbox_dn_7: 0.5269  loss_giou_dn_7: 0.6825  loss_ce_8: 1.307  loss_mask_8: 0.6198  loss_dice_8: 2.113  loss_bbox_8: 0.6812  loss_giou_8: 1.006  loss_ce_dn_8: 0.002578  loss_mask_dn_8: 0.6011  loss_dice_dn_8: 1.946  loss_bbox_dn_8: 0.5232  loss_giou_dn_8: 0.6842  loss_ce_interm: 1.88  loss_mask_interm: 0.6287  loss_dice_interm: 1.984  loss_bbox_interm: 0.7026  loss_giou_interm: 0.9015    time: 0.5137  last_time: 0.5308  data_time: 0.0037  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:04:34 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:04:34 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:04:34 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:04:34 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:04:36 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0030 s/iter. Inference: 0.0779 s/iter. Eval: 0.0101 s/iter. Total: 0.0909 s/iter. ETA=0:00:05\n",
      "[03/14 17:04:41 d2.evaluation.evaluator]: Total inference time: 0:00:05.488600 (0.088526 s / iter per device, on 1 devices)\n",
      "[03/14 17:04:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076752 s / iter per device, on 1 devices)\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.343\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.107\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.173\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.148\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 13.664 | 34.345 | 10.669 | 17.293 | 14.824 |  nan  |\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:04:41 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.061\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.144\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.197\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.252\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 4.866 | 30.336 | 0.064  | 4.116 | 8.371 |  nan  |\n",
      "[03/14 17:04:41 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:04:41 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: 13.6641,34.3449,10.6691,17.2929,14.8245,nan\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:04:41 d2.evaluation.testing]: copypaste: 4.8655,30.3365,0.0642,4.1163,8.3706,nan\n",
      "[03/14 17:04:41 d2.utils.events]:  eta: 0:31:14  iter: 299  total_loss: 113  loss_ce: 1.075  loss_mask: 0.4533  loss_dice: 2.584  loss_bbox: 0.842  loss_giou: 1.192  loss_ce_dn: 0.004551  loss_mask_dn: 0.4187  loss_dice_dn: 2.292  loss_bbox_dn: 0.4168  loss_giou_dn: 0.7131  loss_ce_0: 1.698  loss_mask_0: 0.539  loss_dice_0: 2.461  loss_bbox_0: 1.046  loss_giou_0: 1.47  loss_ce_dn_0: 0.1268  loss_mask_dn_0: 0.7063  loss_dice_dn_0: 4.009  loss_bbox_dn_0: 0.6107  loss_giou_dn_0: 0.8534  loss_ce_1: 1.69  loss_mask_1: 0.5642  loss_dice_1: 2.41  loss_bbox_1: 0.6872  loss_giou_1: 1.115  loss_ce_dn_1: 0.004057  loss_mask_dn_1: 0.4641  loss_dice_dn_1: 2.364  loss_bbox_dn_1: 0.4579  loss_giou_dn_1: 0.7375  loss_ce_2: 1.309  loss_mask_2: 0.4929  loss_dice_2: 2.497  loss_bbox_2: 0.8129  loss_giou_2: 1.205  loss_ce_dn_2: 0.002093  loss_mask_dn_2: 0.4562  loss_dice_dn_2: 2.305  loss_bbox_dn_2: 0.4122  loss_giou_dn_2: 0.7291  loss_ce_3: 1.215  loss_mask_3: 0.4979  loss_dice_3: 2.54  loss_bbox_3: 0.9453  loss_giou_3: 1.264  loss_ce_dn_3: 0.001477  loss_mask_dn_3: 0.4135  loss_dice_dn_3: 2.252  loss_bbox_dn_3: 0.4005  loss_giou_dn_3: 0.7134  loss_ce_4: 1.18  loss_mask_4: 0.4653  loss_dice_4: 2.429  loss_bbox_4: 0.869  loss_giou_4: 1.275  loss_ce_dn_4: 0.002277  loss_mask_dn_4: 0.4214  loss_dice_dn_4: 2.21  loss_bbox_dn_4: 0.3954  loss_giou_dn_4: 0.704  loss_ce_5: 1.15  loss_mask_5: 0.4502  loss_dice_5: 2.589  loss_bbox_5: 0.7933  loss_giou_5: 1.184  loss_ce_dn_5: 0.002269  loss_mask_dn_5: 0.4261  loss_dice_dn_5: 2.247  loss_bbox_dn_5: 0.4083  loss_giou_dn_5: 0.7135  loss_ce_6: 1.084  loss_mask_6: 0.444  loss_dice_6: 2.495  loss_bbox_6: 0.7699  loss_giou_6: 1.212  loss_ce_dn_6: 0.00421  loss_mask_dn_6: 0.425  loss_dice_dn_6: 2.27  loss_bbox_dn_6: 0.4123  loss_giou_dn_6: 0.7158  loss_ce_7: 1  loss_mask_7: 0.4393  loss_dice_7: 2.547  loss_bbox_7: 0.8549  loss_giou_7: 1.195  loss_ce_dn_7: 0.003458  loss_mask_dn_7: 0.4233  loss_dice_dn_7: 2.29  loss_bbox_dn_7: 0.4106  loss_giou_dn_7: 0.7174  loss_ce_8: 1.008  loss_mask_8: 0.4419  loss_dice_8: 2.585  loss_bbox_8: 0.8495  loss_giou_8: 1.214  loss_ce_dn_8: 0.003823  loss_mask_dn_8: 0.4205  loss_dice_dn_8: 2.266  loss_bbox_dn_8: 0.4123  loss_giou_dn_8: 0.7139  loss_ce_interm: 1.514  loss_mask_interm: 0.577  loss_dice_interm: 2.66  loss_bbox_interm: 0.6721  loss_giou_interm: 1.067    time: 0.5127  last_time: 0.4851  data_time: 0.0035  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:04:52 d2.utils.events]:  eta: 0:31:04  iter: 319  total_loss: 103.9  loss_ce: 1.128  loss_mask: 0.7245  loss_dice: 1.815  loss_bbox: 0.6278  loss_giou: 0.7907  loss_ce_dn: 0.004578  loss_mask_dn: 0.6273  loss_dice_dn: 1.821  loss_bbox_dn: 0.5221  loss_giou_dn: 0.5965  loss_ce_0: 1.927  loss_mask_0: 0.7135  loss_dice_0: 1.762  loss_bbox_0: 0.9118  loss_giou_0: 1.015  loss_ce_dn_0: 0.2031  loss_mask_dn_0: 1.042  loss_dice_dn_0: 3.506  loss_bbox_dn_0: 0.8917  loss_giou_dn_0: 0.8522  loss_ce_1: 1.872  loss_mask_1: 0.6488  loss_dice_1: 1.765  loss_bbox_1: 0.5771  loss_giou_1: 0.7149  loss_ce_dn_1: 0.01023  loss_mask_dn_1: 0.6563  loss_dice_dn_1: 1.994  loss_bbox_dn_1: 0.6361  loss_giou_dn_1: 0.6885  loss_ce_2: 1.43  loss_mask_2: 0.6999  loss_dice_2: 1.785  loss_bbox_2: 0.6303  loss_giou_2: 0.7484  loss_ce_dn_2: 0.002786  loss_mask_dn_2: 0.6783  loss_dice_dn_2: 1.871  loss_bbox_dn_2: 0.5461  loss_giou_dn_2: 0.6179  loss_ce_3: 1.252  loss_mask_3: 0.7692  loss_dice_3: 1.782  loss_bbox_3: 0.6745  loss_giou_3: 0.7784  loss_ce_dn_3: 0.001712  loss_mask_dn_3: 0.668  loss_dice_dn_3: 1.854  loss_bbox_dn_3: 0.5384  loss_giou_dn_3: 0.6144  loss_ce_4: 1.191  loss_mask_4: 0.7714  loss_dice_4: 1.763  loss_bbox_4: 0.6257  loss_giou_4: 0.7844  loss_ce_dn_4: 0.002768  loss_mask_dn_4: 0.6668  loss_dice_dn_4: 1.787  loss_bbox_dn_4: 0.534  loss_giou_dn_4: 0.6089  loss_ce_5: 1.106  loss_mask_5: 0.7064  loss_dice_5: 1.838  loss_bbox_5: 0.6259  loss_giou_5: 0.791  loss_ce_dn_5: 0.002635  loss_mask_dn_5: 0.6434  loss_dice_dn_5: 1.821  loss_bbox_dn_5: 0.5344  loss_giou_dn_5: 0.5991  loss_ce_6: 1.071  loss_mask_6: 0.7459  loss_dice_6: 1.83  loss_bbox_6: 0.6395  loss_giou_6: 0.7912  loss_ce_dn_6: 0.004279  loss_mask_dn_6: 0.6354  loss_dice_dn_6: 1.82  loss_bbox_dn_6: 0.5237  loss_giou_dn_6: 0.6064  loss_ce_7: 1.074  loss_mask_7: 0.7156  loss_dice_7: 1.775  loss_bbox_7: 0.6476  loss_giou_7: 0.7919  loss_ce_dn_7: 0.003328  loss_mask_dn_7: 0.6307  loss_dice_dn_7: 1.841  loss_bbox_dn_7: 0.5178  loss_giou_dn_7: 0.5959  loss_ce_8: 1.123  loss_mask_8: 0.7414  loss_dice_8: 1.763  loss_bbox_8: 0.6413  loss_giou_8: 0.7926  loss_ce_dn_8: 0.003502  loss_mask_dn_8: 0.6323  loss_dice_dn_8: 1.82  loss_bbox_dn_8: 0.5189  loss_giou_dn_8: 0.5978  loss_ce_interm: 1.828  loss_mask_interm: 0.8394  loss_dice_interm: 1.682  loss_bbox_interm: 0.6788  loss_giou_interm: 0.881    time: 0.5126  last_time: 0.5234  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:02 d2.utils.events]:  eta: 0:30:57  iter: 339  total_loss: 101.4  loss_ce: 0.8903  loss_mask: 0.7415  loss_dice: 1.658  loss_bbox: 0.5387  loss_giou: 0.8136  loss_ce_dn: 0.00177  loss_mask_dn: 0.7336  loss_dice_dn: 1.569  loss_bbox_dn: 0.4785  loss_giou_dn: 0.5671  loss_ce_0: 1.586  loss_mask_0: 0.834  loss_dice_0: 1.511  loss_bbox_0: 0.8571  loss_giou_0: 1.181  loss_ce_dn_0: 0.1183  loss_mask_dn_0: 1.082  loss_dice_dn_0: 3.347  loss_bbox_dn_0: 0.9093  loss_giou_dn_0: 0.8505  loss_ce_1: 1.68  loss_mask_1: 0.8342  loss_dice_1: 1.506  loss_bbox_1: 0.6315  loss_giou_1: 0.8372  loss_ce_dn_1: 0.004763  loss_mask_dn_1: 0.7539  loss_dice_dn_1: 1.716  loss_bbox_dn_1: 0.5249  loss_giou_dn_1: 0.6574  loss_ce_2: 1.119  loss_mask_2: 0.7919  loss_dice_2: 1.46  loss_bbox_2: 0.5718  loss_giou_2: 0.7927  loss_ce_dn_2: 0.001613  loss_mask_dn_2: 0.723  loss_dice_dn_2: 1.59  loss_bbox_dn_2: 0.4819  loss_giou_dn_2: 0.6059  loss_ce_3: 1.091  loss_mask_3: 0.7809  loss_dice_3: 1.622  loss_bbox_3: 0.4935  loss_giou_3: 0.796  loss_ce_dn_3: 0.001923  loss_mask_dn_3: 0.7577  loss_dice_dn_3: 1.564  loss_bbox_dn_3: 0.4917  loss_giou_dn_3: 0.6034  loss_ce_4: 0.9787  loss_mask_4: 0.799  loss_dice_4: 1.632  loss_bbox_4: 0.5278  loss_giou_4: 0.7519  loss_ce_dn_4: 0.002166  loss_mask_dn_4: 0.7377  loss_dice_dn_4: 1.59  loss_bbox_dn_4: 0.4813  loss_giou_dn_4: 0.5903  loss_ce_5: 0.9622  loss_mask_5: 0.7627  loss_dice_5: 1.619  loss_bbox_5: 0.4748  loss_giou_5: 0.7767  loss_ce_dn_5: 0.001974  loss_mask_dn_5: 0.7248  loss_dice_dn_5: 1.625  loss_bbox_dn_5: 0.4821  loss_giou_dn_5: 0.5777  loss_ce_6: 0.9122  loss_mask_6: 0.7451  loss_dice_6: 1.678  loss_bbox_6: 0.5239  loss_giou_6: 0.8274  loss_ce_dn_6: 0.002556  loss_mask_dn_6: 0.7042  loss_dice_dn_6: 1.611  loss_bbox_dn_6: 0.481  loss_giou_dn_6: 0.5732  loss_ce_7: 0.9777  loss_mask_7: 0.7481  loss_dice_7: 1.564  loss_bbox_7: 0.503  loss_giou_7: 0.8326  loss_ce_dn_7: 0.001619  loss_mask_dn_7: 0.7213  loss_dice_dn_7: 1.584  loss_bbox_dn_7: 0.4782  loss_giou_dn_7: 0.5686  loss_ce_8: 0.8955  loss_mask_8: 0.7423  loss_dice_8: 1.62  loss_bbox_8: 0.5647  loss_giou_8: 0.8211  loss_ce_dn_8: 0.001514  loss_mask_dn_8: 0.7322  loss_dice_dn_8: 1.574  loss_bbox_dn_8: 0.4788  loss_giou_dn_8: 0.5647  loss_ce_interm: 1.775  loss_mask_interm: 0.8431  loss_dice_interm: 1.55  loss_bbox_interm: 0.5887  loss_giou_interm: 0.8604    time: 0.5127  last_time: 0.5174  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:12 d2.utils.events]:  eta: 0:30:40  iter: 359  total_loss: 94.93  loss_ce: 1.036  loss_mask: 0.5799  loss_dice: 1.764  loss_bbox: 0.5189  loss_giou: 0.8166  loss_ce_dn: 0.001845  loss_mask_dn: 0.567  loss_dice_dn: 1.656  loss_bbox_dn: 0.3776  loss_giou_dn: 0.6131  loss_ce_0: 1.565  loss_mask_0: 0.5475  loss_dice_0: 1.902  loss_bbox_0: 0.8071  loss_giou_0: 1.004  loss_ce_dn_0: 0.1622  loss_mask_dn_0: 0.987  loss_dice_dn_0: 3.565  loss_bbox_dn_0: 0.7827  loss_giou_dn_0: 0.8507  loss_ce_1: 1.68  loss_mask_1: 0.5367  loss_dice_1: 1.673  loss_bbox_1: 0.4393  loss_giou_1: 0.8187  loss_ce_dn_1: 0.009457  loss_mask_dn_1: 0.6166  loss_dice_dn_1: 1.753  loss_bbox_dn_1: 0.5041  loss_giou_dn_1: 0.6797  loss_ce_2: 1.351  loss_mask_2: 0.5981  loss_dice_2: 1.816  loss_bbox_2: 0.4553  loss_giou_2: 0.8102  loss_ce_dn_2: 0.0028  loss_mask_dn_2: 0.6234  loss_dice_dn_2: 1.691  loss_bbox_dn_2: 0.4421  loss_giou_dn_2: 0.6352  loss_ce_3: 1.375  loss_mask_3: 0.546  loss_dice_3: 1.62  loss_bbox_3: 0.4651  loss_giou_3: 0.813  loss_ce_dn_3: 0.0009828  loss_mask_dn_3: 0.6155  loss_dice_dn_3: 1.651  loss_bbox_dn_3: 0.4141  loss_giou_dn_3: 0.6323  loss_ce_4: 1.074  loss_mask_4: 0.5728  loss_dice_4: 1.745  loss_bbox_4: 0.5635  loss_giou_4: 0.833  loss_ce_dn_4: 0.001499  loss_mask_dn_4: 0.5967  loss_dice_dn_4: 1.596  loss_bbox_dn_4: 0.4041  loss_giou_dn_4: 0.6113  loss_ce_5: 1.024  loss_mask_5: 0.6002  loss_dice_5: 1.827  loss_bbox_5: 0.5852  loss_giou_5: 0.8569  loss_ce_dn_5: 0.001201  loss_mask_dn_5: 0.5742  loss_dice_dn_5: 1.601  loss_bbox_dn_5: 0.3848  loss_giou_dn_5: 0.6081  loss_ce_6: 1.091  loss_mask_6: 0.5422  loss_dice_6: 1.791  loss_bbox_6: 0.5855  loss_giou_6: 0.8072  loss_ce_dn_6: 0.001445  loss_mask_dn_6: 0.5682  loss_dice_dn_6: 1.628  loss_bbox_dn_6: 0.3707  loss_giou_dn_6: 0.6063  loss_ce_7: 1.159  loss_mask_7: 0.5919  loss_dice_7: 1.794  loss_bbox_7: 0.5586  loss_giou_7: 0.8249  loss_ce_dn_7: 0.001313  loss_mask_dn_7: 0.5752  loss_dice_dn_7: 1.642  loss_bbox_dn_7: 0.3752  loss_giou_dn_7: 0.6119  loss_ce_8: 1.031  loss_mask_8: 0.6037  loss_dice_8: 1.81  loss_bbox_8: 0.5497  loss_giou_8: 0.814  loss_ce_dn_8: 0.001527  loss_mask_dn_8: 0.572  loss_dice_dn_8: 1.636  loss_bbox_dn_8: 0.3788  loss_giou_dn_8: 0.6121  loss_ce_interm: 1.552  loss_mask_interm: 0.6306  loss_dice_interm: 1.825  loss_bbox_interm: 0.6377  loss_giou_interm: 0.9858    time: 0.5110  last_time: 0.5033  data_time: 0.0036  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:22 d2.utils.events]:  eta: 0:30:29  iter: 379  total_loss: 91.62  loss_ce: 0.9712  loss_mask: 0.4832  loss_dice: 1.628  loss_bbox: 0.5309  loss_giou: 0.8227  loss_ce_dn: 0.003583  loss_mask_dn: 0.4747  loss_dice_dn: 1.591  loss_bbox_dn: 0.3831  loss_giou_dn: 0.6035  loss_ce_0: 1.659  loss_mask_0: 0.4807  loss_dice_0: 1.587  loss_bbox_0: 0.7854  loss_giou_0: 1.031  loss_ce_dn_0: 0.1109  loss_mask_dn_0: 0.9454  loss_dice_dn_0: 3.577  loss_bbox_dn_0: 0.8131  loss_giou_dn_0: 0.8513  loss_ce_1: 1.452  loss_mask_1: 0.5216  loss_dice_1: 1.644  loss_bbox_1: 0.4612  loss_giou_1: 0.8338  loss_ce_dn_1: 0.005529  loss_mask_dn_1: 0.4841  loss_dice_dn_1: 1.664  loss_bbox_dn_1: 0.4462  loss_giou_dn_1: 0.6506  loss_ce_2: 1.298  loss_mask_2: 0.4857  loss_dice_2: 1.609  loss_bbox_2: 0.4424  loss_giou_2: 0.7355  loss_ce_dn_2: 0.004146  loss_mask_dn_2: 0.5045  loss_dice_dn_2: 1.668  loss_bbox_dn_2: 0.435  loss_giou_dn_2: 0.6275  loss_ce_3: 1.163  loss_mask_3: 0.4917  loss_dice_3: 1.625  loss_bbox_3: 0.5113  loss_giou_3: 0.7291  loss_ce_dn_3: 0.001835  loss_mask_dn_3: 0.4603  loss_dice_dn_3: 1.65  loss_bbox_dn_3: 0.4095  loss_giou_dn_3: 0.613  loss_ce_4: 1.159  loss_mask_4: 0.5006  loss_dice_4: 1.669  loss_bbox_4: 0.5358  loss_giou_4: 0.7712  loss_ce_dn_4: 0.003104  loss_mask_dn_4: 0.4572  loss_dice_dn_4: 1.596  loss_bbox_dn_4: 0.3935  loss_giou_dn_4: 0.6071  loss_ce_5: 1.133  loss_mask_5: 0.4683  loss_dice_5: 1.608  loss_bbox_5: 0.5481  loss_giou_5: 0.8416  loss_ce_dn_5: 0.002101  loss_mask_dn_5: 0.4658  loss_dice_dn_5: 1.638  loss_bbox_dn_5: 0.4018  loss_giou_dn_5: 0.5927  loss_ce_6: 1.011  loss_mask_6: 0.4519  loss_dice_6: 1.645  loss_bbox_6: 0.5716  loss_giou_6: 0.8356  loss_ce_dn_6: 0.002979  loss_mask_dn_6: 0.4748  loss_dice_dn_6: 1.593  loss_bbox_dn_6: 0.387  loss_giou_dn_6: 0.6038  loss_ce_7: 1.098  loss_mask_7: 0.4435  loss_dice_7: 1.615  loss_bbox_7: 0.4756  loss_giou_7: 0.8399  loss_ce_dn_7: 0.002918  loss_mask_dn_7: 0.4801  loss_dice_dn_7: 1.6  loss_bbox_dn_7: 0.376  loss_giou_dn_7: 0.6073  loss_ce_8: 0.9617  loss_mask_8: 0.4682  loss_dice_8: 1.568  loss_bbox_8: 0.505  loss_giou_8: 0.8246  loss_ce_dn_8: 0.00301  loss_mask_dn_8: 0.4806  loss_dice_dn_8: 1.582  loss_bbox_dn_8: 0.3797  loss_giou_dn_8: 0.6026  loss_ce_interm: 1.41  loss_mask_interm: 0.4937  loss_dice_interm: 1.749  loss_bbox_interm: 0.5766  loss_giou_interm: 0.9487    time: 0.5103  last_time: 0.5033  data_time: 0.0036  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:32 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:05:32 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:05:32 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:05:32 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:05:34 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0010 s/iter. Inference: 0.0835 s/iter. Eval: 0.0103 s/iter. Total: 0.0949 s/iter. ETA=0:00:05\n",
      "[03/14 17:05:38 d2.evaluation.evaluator]: Total inference time: 0:00:05.444737 (0.087818 s / iter per device, on 1 devices)\n",
      "[03/14 17:05:38 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075968 s / iter per device, on 1 devices)\n",
      "[03/14 17:05:38 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:05:38 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:05:39 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.381\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.093\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.183\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.141\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.552\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:05:39 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 13.709 | 38.141 | 9.262  | 19.123 | 18.277 |  nan  |\n",
      "[03/14 17:05:39 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:05:39 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.180\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.061\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.152\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.252\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:05:39 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.524 | 18.010 | 0.048  | 4.097 | 4.749 |  nan  |\n",
      "[03/14 17:05:39 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:05:39 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: 13.7093,38.1414,9.2615,19.1230,18.2774,nan\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:05:39 d2.evaluation.testing]: copypaste: 3.5243,18.0101,0.0480,4.0972,4.7493,nan\n",
      "[03/14 17:05:39 d2.utils.events]:  eta: 0:30:20  iter: 399  total_loss: 103.3  loss_ce: 0.9205  loss_mask: 0.5088  loss_dice: 2.285  loss_bbox: 0.6863  loss_giou: 0.9154  loss_ce_dn: 0.002745  loss_mask_dn: 0.4956  loss_dice_dn: 2.106  loss_bbox_dn: 0.3671  loss_giou_dn: 0.6261  loss_ce_0: 1.533  loss_mask_0: 0.55  loss_dice_0: 2.204  loss_bbox_0: 0.8117  loss_giou_0: 1.256  loss_ce_dn_0: 0.1075  loss_mask_dn_0: 0.8817  loss_dice_dn_0: 3.804  loss_bbox_dn_0: 0.7281  loss_giou_dn_0: 0.8504  loss_ce_1: 1.453  loss_mask_1: 0.5344  loss_dice_1: 2.273  loss_bbox_1: 0.6209  loss_giou_1: 0.9916  loss_ce_dn_1: 0.003813  loss_mask_dn_1: 0.5064  loss_dice_dn_1: 2.057  loss_bbox_dn_1: 0.4082  loss_giou_dn_1: 0.6662  loss_ce_2: 1.207  loss_mask_2: 0.4428  loss_dice_2: 2.16  loss_bbox_2: 0.6457  loss_giou_2: 0.9162  loss_ce_dn_2: 0.001845  loss_mask_dn_2: 0.4765  loss_dice_dn_2: 2.063  loss_bbox_dn_2: 0.3806  loss_giou_dn_2: 0.6568  loss_ce_3: 1.264  loss_mask_3: 0.5134  loss_dice_3: 2.17  loss_bbox_3: 0.6554  loss_giou_3: 0.9689  loss_ce_dn_3: 0.001167  loss_mask_dn_3: 0.4849  loss_dice_dn_3: 2.009  loss_bbox_dn_3: 0.3664  loss_giou_dn_3: 0.6423  loss_ce_4: 1.02  loss_mask_4: 0.5604  loss_dice_4: 2.124  loss_bbox_4: 0.6792  loss_giou_4: 0.9298  loss_ce_dn_4: 0.001482  loss_mask_dn_4: 0.5084  loss_dice_dn_4: 2.008  loss_bbox_dn_4: 0.3577  loss_giou_dn_4: 0.6424  loss_ce_5: 1.077  loss_mask_5: 0.509  loss_dice_5: 2.133  loss_bbox_5: 0.7098  loss_giou_5: 0.9388  loss_ce_dn_5: 0.001398  loss_mask_dn_5: 0.4869  loss_dice_dn_5: 2  loss_bbox_dn_5: 0.3536  loss_giou_dn_5: 0.6335  loss_ce_6: 0.9265  loss_mask_6: 0.529  loss_dice_6: 2.166  loss_bbox_6: 0.6458  loss_giou_6: 0.9378  loss_ce_dn_6: 0.002301  loss_mask_dn_6: 0.4998  loss_dice_dn_6: 2.048  loss_bbox_dn_6: 0.3526  loss_giou_dn_6: 0.6366  loss_ce_7: 0.9081  loss_mask_7: 0.4996  loss_dice_7: 2.217  loss_bbox_7: 0.7083  loss_giou_7: 0.9454  loss_ce_dn_7: 0.002214  loss_mask_dn_7: 0.4948  loss_dice_dn_7: 2.041  loss_bbox_dn_7: 0.3591  loss_giou_dn_7: 0.6307  loss_ce_8: 0.912  loss_mask_8: 0.5053  loss_dice_8: 2.215  loss_bbox_8: 0.6974  loss_giou_8: 0.9459  loss_ce_dn_8: 0.002363  loss_mask_dn_8: 0.4996  loss_dice_dn_8: 2.063  loss_bbox_dn_8: 0.3655  loss_giou_dn_8: 0.6288  loss_ce_interm: 1.679  loss_mask_interm: 0.5884  loss_dice_interm: 2.131  loss_bbox_interm: 0.6848  loss_giou_interm: 1.065    time: 0.5107  last_time: 0.5255  data_time: 0.0037  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:49 d2.utils.events]:  eta: 0:30:13  iter: 419  total_loss: 96.23  loss_ce: 1.047  loss_mask: 0.6596  loss_dice: 2.101  loss_bbox: 0.5107  loss_giou: 0.7134  loss_ce_dn: 0.001771  loss_mask_dn: 0.5605  loss_dice_dn: 1.822  loss_bbox_dn: 0.4283  loss_giou_dn: 0.5696  loss_ce_0: 1.665  loss_mask_0: 0.6811  loss_dice_0: 2.013  loss_bbox_0: 0.6801  loss_giou_0: 0.9644  loss_ce_dn_0: 0.1025  loss_mask_dn_0: 0.8941  loss_dice_dn_0: 3.744  loss_bbox_dn_0: 0.773  loss_giou_dn_0: 0.8465  loss_ce_1: 1.325  loss_mask_1: 0.6533  loss_dice_1: 2.016  loss_bbox_1: 0.5692  loss_giou_1: 0.807  loss_ce_dn_1: 0.004235  loss_mask_dn_1: 0.61  loss_dice_dn_1: 1.978  loss_bbox_dn_1: 0.5055  loss_giou_dn_1: 0.6501  loss_ce_2: 1.225  loss_mask_2: 0.6851  loss_dice_2: 2.013  loss_bbox_2: 0.6084  loss_giou_2: 0.8479  loss_ce_dn_2: 0.002195  loss_mask_dn_2: 0.5818  loss_dice_dn_2: 1.927  loss_bbox_dn_2: 0.4631  loss_giou_dn_2: 0.6034  loss_ce_3: 1.087  loss_mask_3: 0.6997  loss_dice_3: 2.017  loss_bbox_3: 0.5161  loss_giou_3: 0.7939  loss_ce_dn_3: 0.0009504  loss_mask_dn_3: 0.5647  loss_dice_dn_3: 1.828  loss_bbox_dn_3: 0.4429  loss_giou_dn_3: 0.5915  loss_ce_4: 1.142  loss_mask_4: 0.709  loss_dice_4: 2.032  loss_bbox_4: 0.4665  loss_giou_4: 0.7598  loss_ce_dn_4: 0.001409  loss_mask_dn_4: 0.5528  loss_dice_dn_4: 1.836  loss_bbox_dn_4: 0.4358  loss_giou_dn_4: 0.5678  loss_ce_5: 1.033  loss_mask_5: 0.6858  loss_dice_5: 2.03  loss_bbox_5: 0.5084  loss_giou_5: 0.7078  loss_ce_dn_5: 0.001393  loss_mask_dn_5: 0.5523  loss_dice_dn_5: 1.829  loss_bbox_dn_5: 0.4361  loss_giou_dn_5: 0.5671  loss_ce_6: 1.067  loss_mask_6: 0.6665  loss_dice_6: 1.96  loss_bbox_6: 0.4976  loss_giou_6: 0.7459  loss_ce_dn_6: 0.001597  loss_mask_dn_6: 0.5469  loss_dice_dn_6: 1.812  loss_bbox_dn_6: 0.428  loss_giou_dn_6: 0.5667  loss_ce_7: 1.035  loss_mask_7: 0.6297  loss_dice_7: 2.019  loss_bbox_7: 0.5114  loss_giou_7: 0.7428  loss_ce_dn_7: 0.001398  loss_mask_dn_7: 0.5621  loss_dice_dn_7: 1.801  loss_bbox_dn_7: 0.4284  loss_giou_dn_7: 0.5663  loss_ce_8: 1.028  loss_mask_8: 0.6773  loss_dice_8: 2.085  loss_bbox_8: 0.5174  loss_giou_8: 0.7251  loss_ce_dn_8: 0.00146  loss_mask_dn_8: 0.5641  loss_dice_dn_8: 1.809  loss_bbox_dn_8: 0.4288  loss_giou_dn_8: 0.569  loss_ce_interm: 1.686  loss_mask_interm: 0.7365  loss_dice_interm: 2.029  loss_bbox_interm: 0.6617  loss_giou_interm: 0.9487    time: 0.5108  last_time: 0.5600  data_time: 0.0037  last_data_time: 0.0043   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:05:59 d2.utils.events]:  eta: 0:30:04  iter: 439  total_loss: 95.72  loss_ce: 1.13  loss_mask: 0.5612  loss_dice: 1.845  loss_bbox: 0.6592  loss_giou: 0.6932  loss_ce_dn: 0.001719  loss_mask_dn: 0.5051  loss_dice_dn: 1.805  loss_bbox_dn: 0.3648  loss_giou_dn: 0.5791  loss_ce_0: 1.67  loss_mask_0: 0.6718  loss_dice_0: 1.761  loss_bbox_0: 0.6287  loss_giou_0: 1.106  loss_ce_dn_0: 0.09934  loss_mask_dn_0: 1.172  loss_dice_dn_0: 3.695  loss_bbox_dn_0: 0.6533  loss_giou_dn_0: 0.8534  loss_ce_1: 1.499  loss_mask_1: 0.4799  loss_dice_1: 1.728  loss_bbox_1: 0.5216  loss_giou_1: 0.7343  loss_ce_dn_1: 0.002971  loss_mask_dn_1: 0.5303  loss_dice_dn_1: 1.811  loss_bbox_dn_1: 0.481  loss_giou_dn_1: 0.6506  loss_ce_2: 1.242  loss_mask_2: 0.5283  loss_dice_2: 1.937  loss_bbox_2: 0.559  loss_giou_2: 0.7723  loss_ce_dn_2: 0.001919  loss_mask_dn_2: 0.5349  loss_dice_dn_2: 1.833  loss_bbox_dn_2: 0.4163  loss_giou_dn_2: 0.6037  loss_ce_3: 1.279  loss_mask_3: 0.4683  loss_dice_3: 1.838  loss_bbox_3: 0.6061  loss_giou_3: 0.7916  loss_ce_dn_3: 0.001663  loss_mask_dn_3: 0.5106  loss_dice_dn_3: 1.814  loss_bbox_dn_3: 0.3836  loss_giou_dn_3: 0.5952  loss_ce_4: 1.182  loss_mask_4: 0.4638  loss_dice_4: 1.826  loss_bbox_4: 0.6058  loss_giou_4: 0.7901  loss_ce_dn_4: 0.001551  loss_mask_dn_4: 0.4971  loss_dice_dn_4: 1.827  loss_bbox_dn_4: 0.3682  loss_giou_dn_4: 0.5813  loss_ce_5: 1.063  loss_mask_5: 0.5077  loss_dice_5: 1.845  loss_bbox_5: 0.6142  loss_giou_5: 0.7229  loss_ce_dn_5: 0.001035  loss_mask_dn_5: 0.5069  loss_dice_dn_5: 1.818  loss_bbox_dn_5: 0.3561  loss_giou_dn_5: 0.5718  loss_ce_6: 1.03  loss_mask_6: 0.4904  loss_dice_6: 1.848  loss_bbox_6: 0.6322  loss_giou_6: 0.7096  loss_ce_dn_6: 0.001386  loss_mask_dn_6: 0.4908  loss_dice_dn_6: 1.803  loss_bbox_dn_6: 0.3549  loss_giou_dn_6: 0.5814  loss_ce_7: 1.062  loss_mask_7: 0.5307  loss_dice_7: 1.863  loss_bbox_7: 0.6528  loss_giou_7: 0.7122  loss_ce_dn_7: 0.001265  loss_mask_dn_7: 0.4912  loss_dice_dn_7: 1.784  loss_bbox_dn_7: 0.3543  loss_giou_dn_7: 0.5775  loss_ce_8: 1.013  loss_mask_8: 0.5562  loss_dice_8: 1.859  loss_bbox_8: 0.6588  loss_giou_8: 0.6943  loss_ce_dn_8: 0.001448  loss_mask_dn_8: 0.5008  loss_dice_dn_8: 1.789  loss_bbox_dn_8: 0.3628  loss_giou_dn_8: 0.5784  loss_ce_interm: 1.599  loss_mask_interm: 0.5594  loss_dice_interm: 1.875  loss_bbox_interm: 0.6208  loss_giou_interm: 0.9714    time: 0.5105  last_time: 0.5303  data_time: 0.0040  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:06:09 d2.utils.events]:  eta: 0:29:52  iter: 459  total_loss: 95.86  loss_ce: 0.8646  loss_mask: 0.5524  loss_dice: 1.842  loss_bbox: 0.4841  loss_giou: 0.7849  loss_ce_dn: 0.001942  loss_mask_dn: 0.6239  loss_dice_dn: 1.786  loss_bbox_dn: 0.4177  loss_giou_dn: 0.5778  loss_ce_0: 1.526  loss_mask_0: 0.6365  loss_dice_0: 1.854  loss_bbox_0: 0.7622  loss_giou_0: 1.173  loss_ce_dn_0: 0.1703  loss_mask_dn_0: 0.9001  loss_dice_dn_0: 3.801  loss_bbox_dn_0: 0.8872  loss_giou_dn_0: 0.8391  loss_ce_1: 1.162  loss_mask_1: 0.5555  loss_dice_1: 1.962  loss_bbox_1: 0.5444  loss_giou_1: 0.741  loss_ce_dn_1: 0.008922  loss_mask_dn_1: 0.6149  loss_dice_dn_1: 1.836  loss_bbox_dn_1: 0.5007  loss_giou_dn_1: 0.6487  loss_ce_2: 1.104  loss_mask_2: 0.5516  loss_dice_2: 1.825  loss_bbox_2: 0.5181  loss_giou_2: 0.728  loss_ce_dn_2: 0.002325  loss_mask_dn_2: 0.6142  loss_dice_dn_2: 1.787  loss_bbox_dn_2: 0.4578  loss_giou_dn_2: 0.5914  loss_ce_3: 0.9329  loss_mask_3: 0.5827  loss_dice_3: 1.894  loss_bbox_3: 0.5233  loss_giou_3: 0.76  loss_ce_dn_3: 0.001236  loss_mask_dn_3: 0.6371  loss_dice_dn_3: 1.79  loss_bbox_dn_3: 0.4149  loss_giou_dn_3: 0.5753  loss_ce_4: 0.8424  loss_mask_4: 0.5776  loss_dice_4: 1.893  loss_bbox_4: 0.5062  loss_giou_4: 0.7553  loss_ce_dn_4: 0.001518  loss_mask_dn_4: 0.6471  loss_dice_dn_4: 1.778  loss_bbox_dn_4: 0.4034  loss_giou_dn_4: 0.5694  loss_ce_5: 0.8499  loss_mask_5: 0.5922  loss_dice_5: 1.868  loss_bbox_5: 0.5291  loss_giou_5: 0.7742  loss_ce_dn_5: 0.001579  loss_mask_dn_5: 0.6242  loss_dice_dn_5: 1.791  loss_bbox_dn_5: 0.4017  loss_giou_dn_5: 0.569  loss_ce_6: 0.8639  loss_mask_6: 0.5616  loss_dice_6: 1.897  loss_bbox_6: 0.5203  loss_giou_6: 0.7814  loss_ce_dn_6: 0.002318  loss_mask_dn_6: 0.6213  loss_dice_dn_6: 1.792  loss_bbox_dn_6: 0.4068  loss_giou_dn_6: 0.5716  loss_ce_7: 0.8495  loss_mask_7: 0.5768  loss_dice_7: 1.922  loss_bbox_7: 0.5096  loss_giou_7: 0.7544  loss_ce_dn_7: 0.00179  loss_mask_dn_7: 0.6122  loss_dice_dn_7: 1.786  loss_bbox_dn_7: 0.4082  loss_giou_dn_7: 0.5681  loss_ce_8: 0.8946  loss_mask_8: 0.5749  loss_dice_8: 1.873  loss_bbox_8: 0.4581  loss_giou_8: 0.7385  loss_ce_dn_8: 0.001681  loss_mask_dn_8: 0.62  loss_dice_dn_8: 1.786  loss_bbox_dn_8: 0.4143  loss_giou_dn_8: 0.5746  loss_ce_interm: 1.513  loss_mask_interm: 0.6303  loss_dice_interm: 1.788  loss_bbox_interm: 0.5443  loss_giou_interm: 0.7654    time: 0.5101  last_time: 0.5147  data_time: 0.0036  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:06:19 d2.utils.events]:  eta: 0:29:39  iter: 479  total_loss: 93.64  loss_ce: 0.9143  loss_mask: 0.4616  loss_dice: 1.888  loss_bbox: 0.4934  loss_giou: 0.8538  loss_ce_dn: 0.001331  loss_mask_dn: 0.4444  loss_dice_dn: 1.971  loss_bbox_dn: 0.3637  loss_giou_dn: 0.5985  loss_ce_0: 1.376  loss_mask_0: 0.4787  loss_dice_0: 2.042  loss_bbox_0: 0.7613  loss_giou_0: 1.114  loss_ce_dn_0: 0.09392  loss_mask_dn_0: 0.8268  loss_dice_dn_0: 3.663  loss_bbox_dn_0: 0.7044  loss_giou_dn_0: 0.85  loss_ce_1: 1.363  loss_mask_1: 0.5052  loss_dice_1: 2.103  loss_bbox_1: 0.5051  loss_giou_1: 0.9014  loss_ce_dn_1: 0.003173  loss_mask_dn_1: 0.4349  loss_dice_dn_1: 2.073  loss_bbox_dn_1: 0.4853  loss_giou_dn_1: 0.6556  loss_ce_2: 1.267  loss_mask_2: 0.4679  loss_dice_2: 2.051  loss_bbox_2: 0.5553  loss_giou_2: 0.9042  loss_ce_dn_2: 0.00175  loss_mask_dn_2: 0.4265  loss_dice_dn_2: 1.976  loss_bbox_dn_2: 0.4132  loss_giou_dn_2: 0.6173  loss_ce_3: 1.301  loss_mask_3: 0.4996  loss_dice_3: 1.984  loss_bbox_3: 0.5058  loss_giou_3: 0.8656  loss_ce_dn_3: 0.001248  loss_mask_dn_3: 0.4133  loss_dice_dn_3: 1.938  loss_bbox_dn_3: 0.393  loss_giou_dn_3: 0.5886  loss_ce_4: 1.018  loss_mask_4: 0.4775  loss_dice_4: 1.974  loss_bbox_4: 0.5078  loss_giou_4: 0.89  loss_ce_dn_4: 0.001637  loss_mask_dn_4: 0.4242  loss_dice_dn_4: 1.902  loss_bbox_dn_4: 0.3869  loss_giou_dn_4: 0.5963  loss_ce_5: 1.015  loss_mask_5: 0.4771  loss_dice_5: 1.866  loss_bbox_5: 0.4673  loss_giou_5: 0.9168  loss_ce_dn_5: 0.001013  loss_mask_dn_5: 0.424  loss_dice_dn_5: 1.929  loss_bbox_dn_5: 0.3705  loss_giou_dn_5: 0.5965  loss_ce_6: 0.9726  loss_mask_6: 0.4767  loss_dice_6: 1.937  loss_bbox_6: 0.4773  loss_giou_6: 0.9083  loss_ce_dn_6: 0.001308  loss_mask_dn_6: 0.4317  loss_dice_dn_6: 1.92  loss_bbox_dn_6: 0.3644  loss_giou_dn_6: 0.5922  loss_ce_7: 0.9181  loss_mask_7: 0.4784  loss_dice_7: 2.032  loss_bbox_7: 0.4994  loss_giou_7: 0.861  loss_ce_dn_7: 0.001145  loss_mask_dn_7: 0.4339  loss_dice_dn_7: 1.962  loss_bbox_dn_7: 0.3678  loss_giou_dn_7: 0.5928  loss_ce_8: 0.8856  loss_mask_8: 0.4854  loss_dice_8: 2.013  loss_bbox_8: 0.4881  loss_giou_8: 0.8628  loss_ce_dn_8: 0.001139  loss_mask_dn_8: 0.441  loss_dice_dn_8: 1.953  loss_bbox_dn_8: 0.3647  loss_giou_dn_8: 0.598  loss_ce_interm: 1.491  loss_mask_interm: 0.496  loss_dice_interm: 2.122  loss_bbox_interm: 0.5227  loss_giou_interm: 0.97    time: 0.5090  last_time: 0.4835  data_time: 0.0034  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:06:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:06:30 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:06:30 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:06:30 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:06:31 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0016 s/iter. Inference: 0.0749 s/iter. Eval: 0.0105 s/iter. Total: 0.0870 s/iter. ETA=0:00:04\n",
      "[03/14 17:06:36 d2.evaluation.evaluator]: Total inference time: 0:00:05.415188 (0.087342 s / iter per device, on 1 devices)\n",
      "[03/14 17:06:36 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075711 s / iter per device, on 1 devices)\n",
      "[03/14 17:06:36 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:06:36 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:06:36 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:06:36 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:06:36 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:06:36 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:06:36 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.471\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.168\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.330\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.166\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.535\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.505\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:06:36 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 19.945 | 47.112 | 16.831 | 33.002 | 16.569 |  nan  |\n",
      "[03/14 17:06:36 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:06:36 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:06:37 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:06:37 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:06:37 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.270\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.181\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.248\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:06:37 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.548 | 27.034 | 0.064  | 7.316 | 3.062 |  nan  |\n",
      "[03/14 17:06:37 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:06:37 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: 19.9447,47.1119,16.8313,33.0015,16.5687,nan\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:06:37 d2.evaluation.testing]: copypaste: 5.5478,27.0342,0.0635,7.3163,3.0618,nan\n",
      "[03/14 17:06:37 d2.utils.events]:  eta: 0:29:26  iter: 499  total_loss: 91.76  loss_ce: 0.8473  loss_mask: 0.6917  loss_dice: 1.553  loss_bbox: 0.6149  loss_giou: 0.8071  loss_ce_dn: 0.001628  loss_mask_dn: 0.6754  loss_dice_dn: 1.581  loss_bbox_dn: 0.4944  loss_giou_dn: 0.6009  loss_ce_0: 1.538  loss_mask_0: 0.7317  loss_dice_0: 1.508  loss_bbox_0: 0.7437  loss_giou_0: 0.9476  loss_ce_dn_0: 0.09105  loss_mask_dn_0: 1.224  loss_dice_dn_0: 3.555  loss_bbox_dn_0: 0.9123  loss_giou_dn_0: 0.8525  loss_ce_1: 1.358  loss_mask_1: 0.6567  loss_dice_1: 1.573  loss_bbox_1: 0.5955  loss_giou_1: 0.7931  loss_ce_dn_1: 0.002846  loss_mask_dn_1: 0.7293  loss_dice_dn_1: 1.616  loss_bbox_dn_1: 0.5587  loss_giou_dn_1: 0.6542  loss_ce_2: 0.8481  loss_mask_2: 0.7321  loss_dice_2: 1.574  loss_bbox_2: 0.597  loss_giou_2: 0.7865  loss_ce_dn_2: 0.002649  loss_mask_dn_2: 0.7312  loss_dice_dn_2: 1.544  loss_bbox_dn_2: 0.493  loss_giou_dn_2: 0.6039  loss_ce_3: 0.7814  loss_mask_3: 0.685  loss_dice_3: 1.564  loss_bbox_3: 0.5565  loss_giou_3: 0.7853  loss_ce_dn_3: 0.001274  loss_mask_dn_3: 0.7347  loss_dice_dn_3: 1.594  loss_bbox_dn_3: 0.4963  loss_giou_dn_3: 0.6094  loss_ce_4: 0.7981  loss_mask_4: 0.6689  loss_dice_4: 1.515  loss_bbox_4: 0.597  loss_giou_4: 0.8159  loss_ce_dn_4: 0.001245  loss_mask_dn_4: 0.7075  loss_dice_dn_4: 1.589  loss_bbox_dn_4: 0.5134  loss_giou_dn_4: 0.6139  loss_ce_5: 0.8033  loss_mask_5: 0.6691  loss_dice_5: 1.574  loss_bbox_5: 0.6266  loss_giou_5: 0.7305  loss_ce_dn_5: 0.001108  loss_mask_dn_5: 0.689  loss_dice_dn_5: 1.559  loss_bbox_dn_5: 0.5032  loss_giou_dn_5: 0.6195  loss_ce_6: 0.8304  loss_mask_6: 0.6666  loss_dice_6: 1.55  loss_bbox_6: 0.6389  loss_giou_6: 0.8228  loss_ce_dn_6: 0.001465  loss_mask_dn_6: 0.6844  loss_dice_dn_6: 1.577  loss_bbox_dn_6: 0.4984  loss_giou_dn_6: 0.6128  loss_ce_7: 0.805  loss_mask_7: 0.6893  loss_dice_7: 1.52  loss_bbox_7: 0.633  loss_giou_7: 0.811  loss_ce_dn_7: 0.001191  loss_mask_dn_7: 0.6928  loss_dice_dn_7: 1.598  loss_bbox_dn_7: 0.4993  loss_giou_dn_7: 0.605  loss_ce_8: 0.8262  loss_mask_8: 0.6998  loss_dice_8: 1.524  loss_bbox_8: 0.6153  loss_giou_8: 0.8118  loss_ce_dn_8: 0.001309  loss_mask_dn_8: 0.6793  loss_dice_dn_8: 1.594  loss_bbox_dn_8: 0.4939  loss_giou_dn_8: 0.6036  loss_ce_interm: 1.604  loss_mask_interm: 0.7385  loss_dice_interm: 1.558  loss_bbox_interm: 0.6786  loss_giou_interm: 0.8281    time: 0.5079  last_time: 0.4744  data_time: 0.0035  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:06:46 d2.utils.events]:  eta: 0:29:11  iter: 519  total_loss: 95.21  loss_ce: 1.11  loss_mask: 0.4731  loss_dice: 1.655  loss_bbox: 0.4636  loss_giou: 0.7792  loss_ce_dn: 0.002796  loss_mask_dn: 0.4817  loss_dice_dn: 1.88  loss_bbox_dn: 0.3492  loss_giou_dn: 0.5981  loss_ce_0: 1.651  loss_mask_0: 0.4924  loss_dice_0: 1.651  loss_bbox_0: 0.6971  loss_giou_0: 0.9596  loss_ce_dn_0: 0.08928  loss_mask_dn_0: 1.027  loss_dice_dn_0: 3.584  loss_bbox_dn_0: 0.7628  loss_giou_dn_0: 0.8522  loss_ce_1: 1.312  loss_mask_1: 0.4483  loss_dice_1: 1.668  loss_bbox_1: 0.4179  loss_giou_1: 0.7209  loss_ce_dn_1: 0.003092  loss_mask_dn_1: 0.5149  loss_dice_dn_1: 1.818  loss_bbox_dn_1: 0.409  loss_giou_dn_1: 0.6395  loss_ce_2: 1.079  loss_mask_2: 0.4775  loss_dice_2: 1.944  loss_bbox_2: 0.4593  loss_giou_2: 0.803  loss_ce_dn_2: 0.001659  loss_mask_dn_2: 0.5098  loss_dice_dn_2: 1.822  loss_bbox_dn_2: 0.3811  loss_giou_dn_2: 0.5931  loss_ce_3: 1.017  loss_mask_3: 0.4408  loss_dice_3: 1.893  loss_bbox_3: 0.5072  loss_giou_3: 0.837  loss_ce_dn_3: 0.001095  loss_mask_dn_3: 0.4926  loss_dice_dn_3: 1.907  loss_bbox_dn_3: 0.3586  loss_giou_dn_3: 0.5975  loss_ce_4: 1.009  loss_mask_4: 0.4328  loss_dice_4: 1.806  loss_bbox_4: 0.4925  loss_giou_4: 0.8247  loss_ce_dn_4: 0.001637  loss_mask_dn_4: 0.4987  loss_dice_dn_4: 1.872  loss_bbox_dn_4: 0.3648  loss_giou_dn_4: 0.5997  loss_ce_5: 1.288  loss_mask_5: 0.4529  loss_dice_5: 1.633  loss_bbox_5: 0.4226  loss_giou_5: 0.7716  loss_ce_dn_5: 0.001686  loss_mask_dn_5: 0.4694  loss_dice_dn_5: 1.86  loss_bbox_dn_5: 0.358  loss_giou_dn_5: 0.5883  loss_ce_6: 1.231  loss_mask_6: 0.4301  loss_dice_6: 1.552  loss_bbox_6: 0.4749  loss_giou_6: 0.806  loss_ce_dn_6: 0.002881  loss_mask_dn_6: 0.4809  loss_dice_dn_6: 1.841  loss_bbox_dn_6: 0.3641  loss_giou_dn_6: 0.5971  loss_ce_7: 1.142  loss_mask_7: 0.4456  loss_dice_7: 1.666  loss_bbox_7: 0.4697  loss_giou_7: 0.7874  loss_ce_dn_7: 0.002225  loss_mask_dn_7: 0.4773  loss_dice_dn_7: 1.853  loss_bbox_dn_7: 0.3538  loss_giou_dn_7: 0.5872  loss_ce_8: 1.043  loss_mask_8: 0.4503  loss_dice_8: 1.653  loss_bbox_8: 0.4552  loss_giou_8: 0.7867  loss_ce_dn_8: 0.002287  loss_mask_dn_8: 0.4822  loss_dice_dn_8: 1.873  loss_bbox_dn_8: 0.3568  loss_giou_dn_8: 0.5973  loss_ce_interm: 1.459  loss_mask_interm: 0.4955  loss_dice_interm: 1.851  loss_bbox_interm: 0.581  loss_giou_interm: 0.9101    time: 0.5072  last_time: 0.5210  data_time: 0.0041  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:06:57 d2.utils.events]:  eta: 0:29:02  iter: 539  total_loss: 96.78  loss_ce: 0.8197  loss_mask: 0.5826  loss_dice: 1.654  loss_bbox: 0.5841  loss_giou: 0.8736  loss_ce_dn: 0.002155  loss_mask_dn: 0.6024  loss_dice_dn: 1.756  loss_bbox_dn: 0.4466  loss_giou_dn: 0.6155  loss_ce_0: 1.491  loss_mask_0: 0.6757  loss_dice_0: 1.693  loss_bbox_0: 0.9956  loss_giou_0: 1.164  loss_ce_dn_0: 0.08575  loss_mask_dn_0: 0.9441  loss_dice_dn_0: 3.501  loss_bbox_dn_0: 0.8434  loss_giou_dn_0: 0.8495  loss_ce_1: 1.38  loss_mask_1: 0.6499  loss_dice_1: 1.675  loss_bbox_1: 0.6576  loss_giou_1: 0.8582  loss_ce_dn_1: 0.004531  loss_mask_dn_1: 0.6154  loss_dice_dn_1: 1.841  loss_bbox_dn_1: 0.5133  loss_giou_dn_1: 0.6634  loss_ce_2: 1.029  loss_mask_2: 0.6027  loss_dice_2: 1.677  loss_bbox_2: 0.6861  loss_giou_2: 0.8684  loss_ce_dn_2: 0.002359  loss_mask_dn_2: 0.6042  loss_dice_dn_2: 1.752  loss_bbox_dn_2: 0.4596  loss_giou_dn_2: 0.6433  loss_ce_3: 0.9318  loss_mask_3: 0.5716  loss_dice_3: 1.663  loss_bbox_3: 0.6725  loss_giou_3: 0.9008  loss_ce_dn_3: 0.001978  loss_mask_dn_3: 0.5826  loss_dice_dn_3: 1.734  loss_bbox_dn_3: 0.458  loss_giou_dn_3: 0.6416  loss_ce_4: 0.8574  loss_mask_4: 0.5835  loss_dice_4: 1.619  loss_bbox_4: 0.6219  loss_giou_4: 0.8895  loss_ce_dn_4: 0.002193  loss_mask_dn_4: 0.59  loss_dice_dn_4: 1.746  loss_bbox_dn_4: 0.4488  loss_giou_dn_4: 0.6369  loss_ce_5: 0.8592  loss_mask_5: 0.5629  loss_dice_5: 1.612  loss_bbox_5: 0.581  loss_giou_5: 0.8674  loss_ce_dn_5: 0.001905  loss_mask_dn_5: 0.579  loss_dice_dn_5: 1.763  loss_bbox_dn_5: 0.449  loss_giou_dn_5: 0.6269  loss_ce_6: 0.8836  loss_mask_6: 0.5419  loss_dice_6: 1.649  loss_bbox_6: 0.5819  loss_giou_6: 0.8725  loss_ce_dn_6: 0.00257  loss_mask_dn_6: 0.5812  loss_dice_dn_6: 1.743  loss_bbox_dn_6: 0.4419  loss_giou_dn_6: 0.6199  loss_ce_7: 0.8414  loss_mask_7: 0.55  loss_dice_7: 1.673  loss_bbox_7: 0.5748  loss_giou_7: 0.8928  loss_ce_dn_7: 0.001967  loss_mask_dn_7: 0.6018  loss_dice_dn_7: 1.735  loss_bbox_dn_7: 0.4433  loss_giou_dn_7: 0.6189  loss_ce_8: 0.8189  loss_mask_8: 0.5849  loss_dice_8: 1.695  loss_bbox_8: 0.5798  loss_giou_8: 0.8808  loss_ce_dn_8: 0.001962  loss_mask_dn_8: 0.6065  loss_dice_dn_8: 1.752  loss_bbox_dn_8: 0.4452  loss_giou_dn_8: 0.6174  loss_ce_interm: 1.57  loss_mask_interm: 0.6763  loss_dice_interm: 1.673  loss_bbox_interm: 0.6354  loss_giou_interm: 0.9379    time: 0.5071  last_time: 0.4965  data_time: 0.0037  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:07:06 d2.utils.events]:  eta: 0:28:49  iter: 559  total_loss: 100.8  loss_ce: 0.9754  loss_mask: 0.5352  loss_dice: 1.781  loss_bbox: 0.5318  loss_giou: 0.7496  loss_ce_dn: 0.002481  loss_mask_dn: 0.4994  loss_dice_dn: 1.817  loss_bbox_dn: 0.4034  loss_giou_dn: 0.6064  loss_ce_0: 1.63  loss_mask_0: 0.5178  loss_dice_0: 1.838  loss_bbox_0: 0.6623  loss_giou_0: 0.9144  loss_ce_dn_0: 0.1163  loss_mask_dn_0: 0.8444  loss_dice_dn_0: 3.651  loss_bbox_dn_0: 0.8187  loss_giou_dn_0: 0.8538  loss_ce_1: 1.328  loss_mask_1: 0.526  loss_dice_1: 1.733  loss_bbox_1: 0.5475  loss_giou_1: 0.758  loss_ce_dn_1: 0.003748  loss_mask_dn_1: 0.5268  loss_dice_dn_1: 1.85  loss_bbox_dn_1: 0.4113  loss_giou_dn_1: 0.6566  loss_ce_2: 1.36  loss_mask_2: 0.5067  loss_dice_2: 1.7  loss_bbox_2: 0.5333  loss_giou_2: 0.7954  loss_ce_dn_2: 0.002045  loss_mask_dn_2: 0.5216  loss_dice_dn_2: 1.846  loss_bbox_dn_2: 0.3962  loss_giou_dn_2: 0.6221  loss_ce_3: 1.182  loss_mask_3: 0.5102  loss_dice_3: 1.694  loss_bbox_3: 0.4972  loss_giou_3: 0.769  loss_ce_dn_3: 0.001163  loss_mask_dn_3: 0.4985  loss_dice_dn_3: 1.809  loss_bbox_dn_3: 0.3865  loss_giou_dn_3: 0.6108  loss_ce_4: 1.053  loss_mask_4: 0.5297  loss_dice_4: 1.731  loss_bbox_4: 0.5572  loss_giou_4: 0.7528  loss_ce_dn_4: 0.001783  loss_mask_dn_4: 0.5213  loss_dice_dn_4: 1.84  loss_bbox_dn_4: 0.3839  loss_giou_dn_4: 0.5988  loss_ce_5: 1.006  loss_mask_5: 0.5476  loss_dice_5: 1.696  loss_bbox_5: 0.5334  loss_giou_5: 0.7735  loss_ce_dn_5: 0.001443  loss_mask_dn_5: 0.503  loss_dice_dn_5: 1.828  loss_bbox_dn_5: 0.3945  loss_giou_dn_5: 0.6002  loss_ce_6: 1.022  loss_mask_6: 0.5377  loss_dice_6: 1.806  loss_bbox_6: 0.5857  loss_giou_6: 0.7676  loss_ce_dn_6: 0.002134  loss_mask_dn_6: 0.5003  loss_dice_dn_6: 1.8  loss_bbox_dn_6: 0.4001  loss_giou_dn_6: 0.6012  loss_ce_7: 0.9893  loss_mask_7: 0.5446  loss_dice_7: 1.797  loss_bbox_7: 0.5847  loss_giou_7: 0.7444  loss_ce_dn_7: 0.001716  loss_mask_dn_7: 0.5023  loss_dice_dn_7: 1.811  loss_bbox_dn_7: 0.4049  loss_giou_dn_7: 0.6028  loss_ce_8: 0.9194  loss_mask_8: 0.5481  loss_dice_8: 1.752  loss_bbox_8: 0.5828  loss_giou_8: 0.7451  loss_ce_dn_8: 0.002095  loss_mask_dn_8: 0.4987  loss_dice_dn_8: 1.806  loss_bbox_dn_8: 0.4035  loss_giou_dn_8: 0.6038  loss_ce_interm: 1.717  loss_mask_interm: 0.4675  loss_dice_interm: 1.674  loss_bbox_interm: 0.6089  loss_giou_interm: 0.9227    time: 0.5059  last_time: 0.4651  data_time: 0.0035  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:07:16 d2.utils.events]:  eta: 0:28:34  iter: 579  total_loss: 86.11  loss_ce: 0.7771  loss_mask: 0.5096  loss_dice: 1.662  loss_bbox: 0.5034  loss_giou: 0.7514  loss_ce_dn: 0.001702  loss_mask_dn: 0.4927  loss_dice_dn: 1.672  loss_bbox_dn: 0.3721  loss_giou_dn: 0.565  loss_ce_0: 1.279  loss_mask_0: 0.4923  loss_dice_0: 1.568  loss_bbox_0: 0.8115  loss_giou_0: 1.08  loss_ce_dn_0: 0.1136  loss_mask_dn_0: 0.6772  loss_dice_dn_0: 3.204  loss_bbox_dn_0: 0.8064  loss_giou_dn_0: 0.8526  loss_ce_1: 1.142  loss_mask_1: 0.4498  loss_dice_1: 1.516  loss_bbox_1: 0.4885  loss_giou_1: 0.8008  loss_ce_dn_1: 0.002764  loss_mask_dn_1: 0.4732  loss_dice_dn_1: 1.642  loss_bbox_dn_1: 0.4152  loss_giou_dn_1: 0.6668  loss_ce_2: 0.9016  loss_mask_2: 0.4578  loss_dice_2: 1.554  loss_bbox_2: 0.5077  loss_giou_2: 0.7572  loss_ce_dn_2: 0.001135  loss_mask_dn_2: 0.466  loss_dice_dn_2: 1.626  loss_bbox_dn_2: 0.3966  loss_giou_dn_2: 0.6177  loss_ce_3: 0.8683  loss_mask_3: 0.4596  loss_dice_3: 1.602  loss_bbox_3: 0.4678  loss_giou_3: 0.7093  loss_ce_dn_3: 0.0006699  loss_mask_dn_3: 0.4785  loss_dice_dn_3: 1.602  loss_bbox_dn_3: 0.3889  loss_giou_dn_3: 0.5823  loss_ce_4: 0.6974  loss_mask_4: 0.4734  loss_dice_4: 1.636  loss_bbox_4: 0.4762  loss_giou_4: 0.733  loss_ce_dn_4: 0.0008219  loss_mask_dn_4: 0.5055  loss_dice_dn_4: 1.622  loss_bbox_dn_4: 0.3777  loss_giou_dn_4: 0.5683  loss_ce_5: 0.813  loss_mask_5: 0.4499  loss_dice_5: 1.578  loss_bbox_5: 0.4929  loss_giou_5: 0.7908  loss_ce_dn_5: 0.001099  loss_mask_dn_5: 0.4866  loss_dice_dn_5: 1.617  loss_bbox_dn_5: 0.3778  loss_giou_dn_5: 0.5625  loss_ce_6: 0.7331  loss_mask_6: 0.4902  loss_dice_6: 1.663  loss_bbox_6: 0.4851  loss_giou_6: 0.8379  loss_ce_dn_6: 0.001624  loss_mask_dn_6: 0.4776  loss_dice_dn_6: 1.678  loss_bbox_dn_6: 0.3723  loss_giou_dn_6: 0.569  loss_ce_7: 0.797  loss_mask_7: 0.4905  loss_dice_7: 1.646  loss_bbox_7: 0.5257  loss_giou_7: 0.8052  loss_ce_dn_7: 0.001255  loss_mask_dn_7: 0.4807  loss_dice_dn_7: 1.68  loss_bbox_dn_7: 0.37  loss_giou_dn_7: 0.5658  loss_ce_8: 0.7888  loss_mask_8: 0.5115  loss_dice_8: 1.661  loss_bbox_8: 0.4861  loss_giou_8: 0.8011  loss_ce_dn_8: 0.001468  loss_mask_dn_8: 0.4888  loss_dice_dn_8: 1.675  loss_bbox_dn_8: 0.372  loss_giou_dn_8: 0.5651  loss_ce_interm: 1.3  loss_mask_interm: 0.4989  loss_dice_interm: 1.583  loss_bbox_interm: 0.5457  loss_giou_interm: 0.8639    time: 0.5048  last_time: 0.4756  data_time: 0.0034  last_data_time: 0.0038   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:07:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:07:26 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:07:26 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:07:26 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:07:27 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0010 s/iter. Inference: 0.0826 s/iter. Eval: 0.0100 s/iter. Total: 0.0935 s/iter. ETA=0:00:05\n",
      "[03/14 17:07:32 d2.evaluation.evaluator]: Total inference time: 0:00:05.454713 (0.087979 s / iter per device, on 1 devices)\n",
      "[03/14 17:07:32 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075921 s / iter per device, on 1 devices)\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.142\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.273\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.199\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 21.878 | 57.381 | 14.172 | 27.331 | 20.791 |  nan  |\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:07:32 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.444\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.235\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
      "| 11.947 | 44.373 | 0.198  | 9.981 | 19.658 |  nan  |\n",
      "[03/14 17:07:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:07:32 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: 21.8776,57.3806,14.1718,27.3310,20.7914,nan\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:07:32 d2.evaluation.testing]: copypaste: 11.9467,44.3731,0.1983,9.9806,19.6579,nan\n",
      "[03/14 17:07:32 d2.utils.events]:  eta: 0:28:20  iter: 599  total_loss: 85.59  loss_ce: 0.7611  loss_mask: 0.53  loss_dice: 1.62  loss_bbox: 0.4567  loss_giou: 0.7345  loss_ce_dn: 0.001059  loss_mask_dn: 0.5092  loss_dice_dn: 1.576  loss_bbox_dn: 0.3386  loss_giou_dn: 0.5559  loss_ce_0: 1.412  loss_mask_0: 0.4956  loss_dice_0: 1.692  loss_bbox_0: 0.6996  loss_giou_0: 0.8992  loss_ce_dn_0: 0.1455  loss_mask_dn_0: 0.9444  loss_dice_dn_0: 3.537  loss_bbox_dn_0: 0.8197  loss_giou_dn_0: 0.8516  loss_ce_1: 1.474  loss_mask_1: 0.462  loss_dice_1: 1.779  loss_bbox_1: 0.4446  loss_giou_1: 0.7636  loss_ce_dn_1: 0.003615  loss_mask_dn_1: 0.5217  loss_dice_dn_1: 1.658  loss_bbox_dn_1: 0.405  loss_giou_dn_1: 0.6209  loss_ce_2: 1.198  loss_mask_2: 0.4925  loss_dice_2: 1.726  loss_bbox_2: 0.5053  loss_giou_2: 0.8066  loss_ce_dn_2: 0.001313  loss_mask_dn_2: 0.5015  loss_dice_dn_2: 1.625  loss_bbox_dn_2: 0.3801  loss_giou_dn_2: 0.5747  loss_ce_3: 0.9826  loss_mask_3: 0.4869  loss_dice_3: 1.787  loss_bbox_3: 0.5014  loss_giou_3: 0.8068  loss_ce_dn_3: 0.001177  loss_mask_dn_3: 0.508  loss_dice_dn_3: 1.606  loss_bbox_dn_3: 0.3538  loss_giou_dn_3: 0.5691  loss_ce_4: 0.8522  loss_mask_4: 0.5291  loss_dice_4: 1.724  loss_bbox_4: 0.4936  loss_giou_4: 0.7977  loss_ce_dn_4: 0.001137  loss_mask_dn_4: 0.5147  loss_dice_dn_4: 1.572  loss_bbox_dn_4: 0.3394  loss_giou_dn_4: 0.5601  loss_ce_5: 0.895  loss_mask_5: 0.522  loss_dice_5: 1.713  loss_bbox_5: 0.4912  loss_giou_5: 0.7747  loss_ce_dn_5: 0.0009692  loss_mask_dn_5: 0.5021  loss_dice_dn_5: 1.607  loss_bbox_dn_5: 0.3372  loss_giou_dn_5: 0.5543  loss_ce_6: 0.8296  loss_mask_6: 0.5279  loss_dice_6: 1.674  loss_bbox_6: 0.4929  loss_giou_6: 0.7531  loss_ce_dn_6: 0.001147  loss_mask_dn_6: 0.5078  loss_dice_dn_6: 1.603  loss_bbox_dn_6: 0.3417  loss_giou_dn_6: 0.5586  loss_ce_7: 0.7396  loss_mask_7: 0.5449  loss_dice_7: 1.631  loss_bbox_7: 0.4895  loss_giou_7: 0.7619  loss_ce_dn_7: 0.0007524  loss_mask_dn_7: 0.5068  loss_dice_dn_7: 1.589  loss_bbox_dn_7: 0.3432  loss_giou_dn_7: 0.5586  loss_ce_8: 0.7227  loss_mask_8: 0.5455  loss_dice_8: 1.562  loss_bbox_8: 0.463  loss_giou_8: 0.7536  loss_ce_dn_8: 0.0008413  loss_mask_dn_8: 0.5134  loss_dice_dn_8: 1.566  loss_bbox_dn_8: 0.3421  loss_giou_dn_8: 0.5565  loss_ce_interm: 1.503  loss_mask_interm: 0.496  loss_dice_interm: 1.613  loss_bbox_interm: 0.5818  loss_giou_interm: 0.8605    time: 0.5044  last_time: 0.4673  data_time: 0.0035  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:07:42 d2.utils.events]:  eta: 0:28:04  iter: 619  total_loss: 93.51  loss_ce: 0.8871  loss_mask: 0.5482  loss_dice: 1.65  loss_bbox: 0.4834  loss_giou: 0.6811  loss_ce_dn: 0.0008045  loss_mask_dn: 0.6127  loss_dice_dn: 1.689  loss_bbox_dn: 0.405  loss_giou_dn: 0.5802  loss_ce_0: 1.474  loss_mask_0: 0.5415  loss_dice_0: 1.793  loss_bbox_0: 0.8492  loss_giou_0: 0.9658  loss_ce_dn_0: 0.1093  loss_mask_dn_0: 0.8386  loss_dice_dn_0: 3.316  loss_bbox_dn_0: 0.889  loss_giou_dn_0: 0.8494  loss_ce_1: 1.288  loss_mask_1: 0.567  loss_dice_1: 1.736  loss_bbox_1: 0.5448  loss_giou_1: 0.7146  loss_ce_dn_1: 0.003271  loss_mask_dn_1: 0.5889  loss_dice_dn_1: 1.685  loss_bbox_dn_1: 0.5218  loss_giou_dn_1: 0.6119  loss_ce_2: 1.197  loss_mask_2: 0.5359  loss_dice_2: 1.571  loss_bbox_2: 0.5435  loss_giou_2: 0.6697  loss_ce_dn_2: 0.001658  loss_mask_dn_2: 0.6427  loss_dice_dn_2: 1.656  loss_bbox_dn_2: 0.4458  loss_giou_dn_2: 0.5802  loss_ce_3: 1.034  loss_mask_3: 0.5834  loss_dice_3: 1.79  loss_bbox_3: 0.51  loss_giou_3: 0.761  loss_ce_dn_3: 0.001543  loss_mask_dn_3: 0.6691  loss_dice_dn_3: 1.754  loss_bbox_dn_3: 0.4269  loss_giou_dn_3: 0.5687  loss_ce_4: 1.119  loss_mask_4: 0.5538  loss_dice_4: 1.566  loss_bbox_4: 0.5371  loss_giou_4: 0.7238  loss_ce_dn_4: 0.001383  loss_mask_dn_4: 0.6543  loss_dice_dn_4: 1.677  loss_bbox_dn_4: 0.4195  loss_giou_dn_4: 0.5807  loss_ce_5: 1.086  loss_mask_5: 0.5036  loss_dice_5: 1.578  loss_bbox_5: 0.5156  loss_giou_5: 0.7374  loss_ce_dn_5: 0.0009246  loss_mask_dn_5: 0.641  loss_dice_dn_5: 1.708  loss_bbox_dn_5: 0.4253  loss_giou_dn_5: 0.5851  loss_ce_6: 1.039  loss_mask_6: 0.5575  loss_dice_6: 1.617  loss_bbox_6: 0.5068  loss_giou_6: 0.7331  loss_ce_dn_6: 0.001292  loss_mask_dn_6: 0.6284  loss_dice_dn_6: 1.698  loss_bbox_dn_6: 0.4208  loss_giou_dn_6: 0.5801  loss_ce_7: 0.8576  loss_mask_7: 0.5929  loss_dice_7: 1.576  loss_bbox_7: 0.5078  loss_giou_7: 0.715  loss_ce_dn_7: 0.0006611  loss_mask_dn_7: 0.604  loss_dice_dn_7: 1.696  loss_bbox_dn_7: 0.4199  loss_giou_dn_7: 0.5715  loss_ce_8: 0.8848  loss_mask_8: 0.5486  loss_dice_8: 1.604  loss_bbox_8: 0.5079  loss_giou_8: 0.715  loss_ce_dn_8: 0.000684  loss_mask_dn_8: 0.6162  loss_dice_dn_8: 1.677  loss_bbox_dn_8: 0.4117  loss_giou_dn_8: 0.5805  loss_ce_interm: 1.461  loss_mask_interm: 0.5999  loss_dice_interm: 1.788  loss_bbox_interm: 0.5404  loss_giou_interm: 0.7998    time: 0.5031  last_time: 0.4947  data_time: 0.0034  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:07:52 d2.utils.events]:  eta: 0:27:53  iter: 639  total_loss: 85.85  loss_ce: 0.9358  loss_mask: 0.4791  loss_dice: 1.651  loss_bbox: 0.5155  loss_giou: 0.7801  loss_ce_dn: 0.00143  loss_mask_dn: 0.5021  loss_dice_dn: 1.643  loss_bbox_dn: 0.3937  loss_giou_dn: 0.5953  loss_ce_0: 1.228  loss_mask_0: 0.5068  loss_dice_0: 1.671  loss_bbox_0: 0.7706  loss_giou_0: 1.011  loss_ce_dn_0: 0.1144  loss_mask_dn_0: 0.6457  loss_dice_dn_0: 3.133  loss_bbox_dn_0: 0.7476  loss_giou_dn_0: 0.8525  loss_ce_1: 1.02  loss_mask_1: 0.5246  loss_dice_1: 1.724  loss_bbox_1: 0.4371  loss_giou_1: 0.7004  loss_ce_dn_1: 0.003051  loss_mask_dn_1: 0.4529  loss_dice_dn_1: 1.722  loss_bbox_dn_1: 0.4767  loss_giou_dn_1: 0.6291  loss_ce_2: 0.9289  loss_mask_2: 0.5562  loss_dice_2: 1.661  loss_bbox_2: 0.487  loss_giou_2: 0.7992  loss_ce_dn_2: 0.001638  loss_mask_dn_2: 0.4605  loss_dice_dn_2: 1.71  loss_bbox_dn_2: 0.4238  loss_giou_dn_2: 0.6116  loss_ce_3: 0.7225  loss_mask_3: 0.5526  loss_dice_3: 1.7  loss_bbox_3: 0.5934  loss_giou_3: 0.8462  loss_ce_dn_3: 0.0008866  loss_mask_dn_3: 0.4554  loss_dice_dn_3: 1.651  loss_bbox_dn_3: 0.4079  loss_giou_dn_3: 0.6157  loss_ce_4: 0.861  loss_mask_4: 0.5398  loss_dice_4: 1.7  loss_bbox_4: 0.5243  loss_giou_4: 0.7557  loss_ce_dn_4: 0.0009383  loss_mask_dn_4: 0.4777  loss_dice_dn_4: 1.643  loss_bbox_dn_4: 0.4025  loss_giou_dn_4: 0.6142  loss_ce_5: 0.8174  loss_mask_5: 0.5304  loss_dice_5: 1.675  loss_bbox_5: 0.5181  loss_giou_5: 0.7805  loss_ce_dn_5: 0.0009772  loss_mask_dn_5: 0.4718  loss_dice_dn_5: 1.642  loss_bbox_dn_5: 0.3984  loss_giou_dn_5: 0.6206  loss_ce_6: 0.7445  loss_mask_6: 0.5373  loss_dice_6: 1.706  loss_bbox_6: 0.5058  loss_giou_6: 0.7889  loss_ce_dn_6: 0.00141  loss_mask_dn_6: 0.476  loss_dice_dn_6: 1.61  loss_bbox_dn_6: 0.4001  loss_giou_dn_6: 0.6037  loss_ce_7: 0.8108  loss_mask_7: 0.4852  loss_dice_7: 1.656  loss_bbox_7: 0.5048  loss_giou_7: 0.7682  loss_ce_dn_7: 0.001105  loss_mask_dn_7: 0.4825  loss_dice_dn_7: 1.626  loss_bbox_dn_7: 0.3932  loss_giou_dn_7: 0.6008  loss_ce_8: 0.8932  loss_mask_8: 0.4934  loss_dice_8: 1.648  loss_bbox_8: 0.5186  loss_giou_8: 0.7749  loss_ce_dn_8: 0.00129  loss_mask_dn_8: 0.4988  loss_dice_dn_8: 1.64  loss_bbox_dn_8: 0.3861  loss_giou_dn_8: 0.5919  loss_ce_interm: 1.307  loss_mask_interm: 0.4866  loss_dice_interm: 1.72  loss_bbox_interm: 0.5213  loss_giou_interm: 0.8037    time: 0.5027  last_time: 0.5042  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:01 d2.utils.events]:  eta: 0:27:41  iter: 659  total_loss: 89.33  loss_ce: 0.6685  loss_mask: 0.5416  loss_dice: 1.616  loss_bbox: 0.4569  loss_giou: 0.6847  loss_ce_dn: 0.0008597  loss_mask_dn: 0.53  loss_dice_dn: 1.588  loss_bbox_dn: 0.4287  loss_giou_dn: 0.5237  loss_ce_0: 1.245  loss_mask_0: 0.6066  loss_dice_0: 1.537  loss_bbox_0: 0.8645  loss_giou_0: 1.044  loss_ce_dn_0: 0.07321  loss_mask_dn_0: 0.8118  loss_dice_dn_0: 3.041  loss_bbox_dn_0: 0.8657  loss_giou_dn_0: 0.8505  loss_ce_1: 1.048  loss_mask_1: 0.5274  loss_dice_1: 1.674  loss_bbox_1: 0.5129  loss_giou_1: 0.6669  loss_ce_dn_1: 0.00306  loss_mask_dn_1: 0.518  loss_dice_dn_1: 1.623  loss_bbox_dn_1: 0.5047  loss_giou_dn_1: 0.6117  loss_ce_2: 0.8792  loss_mask_2: 0.5277  loss_dice_2: 1.651  loss_bbox_2: 0.4864  loss_giou_2: 0.6436  loss_ce_dn_2: 0.001386  loss_mask_dn_2: 0.5143  loss_dice_dn_2: 1.516  loss_bbox_dn_2: 0.4498  loss_giou_dn_2: 0.5637  loss_ce_3: 0.785  loss_mask_3: 0.5498  loss_dice_3: 1.54  loss_bbox_3: 0.4817  loss_giou_3: 0.6915  loss_ce_dn_3: 0.000841  loss_mask_dn_3: 0.5378  loss_dice_dn_3: 1.513  loss_bbox_dn_3: 0.4489  loss_giou_dn_3: 0.5315  loss_ce_4: 0.7907  loss_mask_4: 0.5145  loss_dice_4: 1.599  loss_bbox_4: 0.4995  loss_giou_4: 0.7037  loss_ce_dn_4: 0.001043  loss_mask_dn_4: 0.5311  loss_dice_dn_4: 1.563  loss_bbox_dn_4: 0.4535  loss_giou_dn_4: 0.5229  loss_ce_5: 0.7018  loss_mask_5: 0.5259  loss_dice_5: 1.601  loss_bbox_5: 0.4719  loss_giou_5: 0.6857  loss_ce_dn_5: 0.0009345  loss_mask_dn_5: 0.5371  loss_dice_dn_5: 1.589  loss_bbox_dn_5: 0.4502  loss_giou_dn_5: 0.509  loss_ce_6: 0.6847  loss_mask_6: 0.5381  loss_dice_6: 1.587  loss_bbox_6: 0.5069  loss_giou_6: 0.7173  loss_ce_dn_6: 0.00101  loss_mask_dn_6: 0.5262  loss_dice_dn_6: 1.58  loss_bbox_dn_6: 0.4412  loss_giou_dn_6: 0.5173  loss_ce_7: 0.673  loss_mask_7: 0.5355  loss_dice_7: 1.626  loss_bbox_7: 0.5026  loss_giou_7: 0.721  loss_ce_dn_7: 0.0007926  loss_mask_dn_7: 0.5363  loss_dice_dn_7: 1.569  loss_bbox_dn_7: 0.4466  loss_giou_dn_7: 0.5245  loss_ce_8: 0.6767  loss_mask_8: 0.5217  loss_dice_8: 1.617  loss_bbox_8: 0.5013  loss_giou_8: 0.6959  loss_ce_dn_8: 0.0007976  loss_mask_dn_8: 0.5313  loss_dice_dn_8: 1.591  loss_bbox_dn_8: 0.4341  loss_giou_dn_8: 0.52  loss_ce_interm: 1.312  loss_mask_interm: 0.5369  loss_dice_interm: 1.544  loss_bbox_interm: 0.6616  loss_giou_interm: 0.8737    time: 0.5020  last_time: 0.5258  data_time: 0.0036  last_data_time: 0.0042   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:11 d2.utils.events]:  eta: 0:27:28  iter: 679  total_loss: 89.32  loss_ce: 0.8668  loss_mask: 0.415  loss_dice: 1.844  loss_bbox: 0.4102  loss_giou: 0.7549  loss_ce_dn: 0.0007004  loss_mask_dn: 0.4342  loss_dice_dn: 1.827  loss_bbox_dn: 0.3599  loss_giou_dn: 0.6304  loss_ce_0: 1.165  loss_mask_0: 0.4549  loss_dice_0: 1.695  loss_bbox_0: 0.6117  loss_giou_0: 1.044  loss_ce_dn_0: 0.07062  loss_mask_dn_0: 0.7454  loss_dice_dn_0: 3.568  loss_bbox_dn_0: 0.736  loss_giou_dn_0: 0.8582  loss_ce_1: 1.191  loss_mask_1: 0.4599  loss_dice_1: 1.969  loss_bbox_1: 0.408  loss_giou_1: 0.7927  loss_ce_dn_1: 0.003151  loss_mask_dn_1: 0.4595  loss_dice_dn_1: 1.872  loss_bbox_dn_1: 0.41  loss_giou_dn_1: 0.6696  loss_ce_2: 0.8576  loss_mask_2: 0.4475  loss_dice_2: 1.835  loss_bbox_2: 0.3864  loss_giou_2: 0.6756  loss_ce_dn_2: 0.001719  loss_mask_dn_2: 0.4261  loss_dice_dn_2: 1.814  loss_bbox_dn_2: 0.3294  loss_giou_dn_2: 0.66  loss_ce_3: 0.7725  loss_mask_3: 0.4633  loss_dice_3: 1.962  loss_bbox_3: 0.4913  loss_giou_3: 0.8362  loss_ce_dn_3: 0.0007965  loss_mask_dn_3: 0.4557  loss_dice_dn_3: 1.872  loss_bbox_dn_3: 0.3482  loss_giou_dn_3: 0.6838  loss_ce_4: 0.9721  loss_mask_4: 0.4047  loss_dice_4: 1.867  loss_bbox_4: 0.4868  loss_giou_4: 0.7759  loss_ce_dn_4: 0.0006503  loss_mask_dn_4: 0.4526  loss_dice_dn_4: 1.848  loss_bbox_dn_4: 0.3462  loss_giou_dn_4: 0.6496  loss_ce_5: 0.9252  loss_mask_5: 0.4229  loss_dice_5: 1.876  loss_bbox_5: 0.4731  loss_giou_5: 0.7605  loss_ce_dn_5: 0.0005738  loss_mask_dn_5: 0.4563  loss_dice_dn_5: 1.808  loss_bbox_dn_5: 0.3391  loss_giou_dn_5: 0.6227  loss_ce_6: 0.8082  loss_mask_6: 0.4495  loss_dice_6: 1.831  loss_bbox_6: 0.4049  loss_giou_6: 0.7396  loss_ce_dn_6: 0.0007191  loss_mask_dn_6: 0.4432  loss_dice_dn_6: 1.858  loss_bbox_dn_6: 0.3412  loss_giou_dn_6: 0.6263  loss_ce_7: 0.8774  loss_mask_7: 0.4309  loss_dice_7: 1.92  loss_bbox_7: 0.4455  loss_giou_7: 0.7584  loss_ce_dn_7: 0.0005175  loss_mask_dn_7: 0.4409  loss_dice_dn_7: 1.824  loss_bbox_dn_7: 0.349  loss_giou_dn_7: 0.6289  loss_ce_8: 0.8311  loss_mask_8: 0.4122  loss_dice_8: 1.891  loss_bbox_8: 0.4485  loss_giou_8: 0.8136  loss_ce_dn_8: 0.0006471  loss_mask_dn_8: 0.4305  loss_dice_dn_8: 1.845  loss_bbox_dn_8: 0.3592  loss_giou_dn_8: 0.6276  loss_ce_interm: 1.275  loss_mask_interm: 0.455  loss_dice_interm: 1.79  loss_bbox_interm: 0.4059  loss_giou_interm: 0.7598    time: 0.5016  last_time: 0.5220  data_time: 0.0036  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:21 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:08:21 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:08:21 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:08:21 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:08:22 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0817 s/iter. Eval: 0.0094 s/iter. Total: 0.0920 s/iter. ETA=0:00:05\n",
      "[03/14 17:08:27 d2.evaluation.evaluator]: Total inference time: 0:00:05.461014 (0.088081 s / iter per device, on 1 devices)\n",
      "[03/14 17:08:27 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076182 s / iter per device, on 1 devices)\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.615\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.170\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.288\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.484\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.576\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 23.879 | 61.451 | 16.988 | 28.817 | 26.341 |  nan  |\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:08:27 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.478\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.099\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.136\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.228\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.253\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.329\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
      "| 11.796 | 47.763 | 0.288  | 9.878 | 20.218 |  nan  |\n",
      "[03/14 17:08:27 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:08:27 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: 23.8787,61.4512,16.9879,28.8170,26.3410,nan\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:08:27 d2.evaluation.testing]: copypaste: 11.7963,47.7632,0.2881,9.8783,20.2176,nan\n",
      "[03/14 17:08:27 d2.utils.events]:  eta: 0:27:16  iter: 699  total_loss: 84.61  loss_ce: 0.8646  loss_mask: 0.4755  loss_dice: 1.577  loss_bbox: 0.3988  loss_giou: 0.7255  loss_ce_dn: 0.001081  loss_mask_dn: 0.462  loss_dice_dn: 1.516  loss_bbox_dn: 0.354  loss_giou_dn: 0.5628  loss_ce_0: 1.194  loss_mask_0: 0.4724  loss_dice_0: 1.476  loss_bbox_0: 0.7644  loss_giou_0: 1.017  loss_ce_dn_0: 0.08758  loss_mask_dn_0: 0.8061  loss_dice_dn_0: 3.268  loss_bbox_dn_0: 0.8354  loss_giou_dn_0: 0.849  loss_ce_1: 1.036  loss_mask_1: 0.5646  loss_dice_1: 1.518  loss_bbox_1: 0.4548  loss_giou_1: 0.7526  loss_ce_dn_1: 0.002547  loss_mask_dn_1: 0.5134  loss_dice_dn_1: 1.662  loss_bbox_dn_1: 0.4296  loss_giou_dn_1: 0.6173  loss_ce_2: 1.056  loss_mask_2: 0.4932  loss_dice_2: 1.525  loss_bbox_2: 0.4689  loss_giou_2: 0.7114  loss_ce_dn_2: 0.001117  loss_mask_dn_2: 0.5039  loss_dice_dn_2: 1.536  loss_bbox_dn_2: 0.3741  loss_giou_dn_2: 0.5866  loss_ce_3: 0.9344  loss_mask_3: 0.505  loss_dice_3: 1.563  loss_bbox_3: 0.426  loss_giou_3: 0.7084  loss_ce_dn_3: 0.0008095  loss_mask_dn_3: 0.4789  loss_dice_dn_3: 1.483  loss_bbox_dn_3: 0.3591  loss_giou_dn_3: 0.5795  loss_ce_4: 0.9138  loss_mask_4: 0.5085  loss_dice_4: 1.568  loss_bbox_4: 0.4088  loss_giou_4: 0.7318  loss_ce_dn_4: 0.0007341  loss_mask_dn_4: 0.4652  loss_dice_dn_4: 1.465  loss_bbox_dn_4: 0.3547  loss_giou_dn_4: 0.5701  loss_ce_5: 0.9055  loss_mask_5: 0.4837  loss_dice_5: 1.581  loss_bbox_5: 0.3946  loss_giou_5: 0.6883  loss_ce_dn_5: 0.0007887  loss_mask_dn_5: 0.4587  loss_dice_dn_5: 1.495  loss_bbox_dn_5: 0.3467  loss_giou_dn_5: 0.5636  loss_ce_6: 0.8591  loss_mask_6: 0.4785  loss_dice_6: 1.59  loss_bbox_6: 0.4057  loss_giou_6: 0.7312  loss_ce_dn_6: 0.001139  loss_mask_dn_6: 0.4555  loss_dice_dn_6: 1.514  loss_bbox_dn_6: 0.3581  loss_giou_dn_6: 0.5697  loss_ce_7: 0.8673  loss_mask_7: 0.4667  loss_dice_7: 1.546  loss_bbox_7: 0.3929  loss_giou_7: 0.7219  loss_ce_dn_7: 0.0009581  loss_mask_dn_7: 0.4644  loss_dice_dn_7: 1.485  loss_bbox_dn_7: 0.3523  loss_giou_dn_7: 0.5629  loss_ce_8: 0.8689  loss_mask_8: 0.4796  loss_dice_8: 1.533  loss_bbox_8: 0.3934  loss_giou_8: 0.7207  loss_ce_dn_8: 0.0009823  loss_mask_dn_8: 0.4625  loss_dice_dn_8: 1.519  loss_bbox_dn_8: 0.3548  loss_giou_dn_8: 0.5626  loss_ce_interm: 1.444  loss_mask_interm: 0.4761  loss_dice_interm: 1.527  loss_bbox_interm: 0.5581  loss_giou_interm: 0.8526    time: 0.5005  last_time: 0.4501  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:37 d2.utils.events]:  eta: 0:27:01  iter: 719  total_loss: 84.2  loss_ce: 1.008  loss_mask: 0.4822  loss_dice: 1.451  loss_bbox: 0.4208  loss_giou: 0.6488  loss_ce_dn: 0.001433  loss_mask_dn: 0.4521  loss_dice_dn: 1.451  loss_bbox_dn: 0.3493  loss_giou_dn: 0.5303  loss_ce_0: 1.366  loss_mask_0: 0.4659  loss_dice_0: 1.479  loss_bbox_0: 0.6682  loss_giou_0: 1.064  loss_ce_dn_0: 0.1296  loss_mask_dn_0: 0.6718  loss_dice_dn_0: 3.142  loss_bbox_dn_0: 0.7713  loss_giou_dn_0: 0.8433  loss_ce_1: 1.207  loss_mask_1: 0.4866  loss_dice_1: 1.432  loss_bbox_1: 0.3878  loss_giou_1: 0.8252  loss_ce_dn_1: 0.002768  loss_mask_dn_1: 0.4739  loss_dice_dn_1: 1.46  loss_bbox_dn_1: 0.4355  loss_giou_dn_1: 0.6  loss_ce_2: 1.19  loss_mask_2: 0.4622  loss_dice_2: 1.448  loss_bbox_2: 0.4341  loss_giou_2: 0.7688  loss_ce_dn_2: 0.002083  loss_mask_dn_2: 0.4596  loss_dice_dn_2: 1.483  loss_bbox_dn_2: 0.3997  loss_giou_dn_2: 0.5591  loss_ce_3: 0.8223  loss_mask_3: 0.4548  loss_dice_3: 1.389  loss_bbox_3: 0.4414  loss_giou_3: 0.7009  loss_ce_dn_3: 0.001048  loss_mask_dn_3: 0.4584  loss_dice_dn_3: 1.439  loss_bbox_dn_3: 0.3838  loss_giou_dn_3: 0.5442  loss_ce_4: 0.7514  loss_mask_4: 0.4768  loss_dice_4: 1.463  loss_bbox_4: 0.4427  loss_giou_4: 0.7262  loss_ce_dn_4: 0.0007823  loss_mask_dn_4: 0.4479  loss_dice_dn_4: 1.452  loss_bbox_dn_4: 0.3721  loss_giou_dn_4: 0.534  loss_ce_5: 0.7565  loss_mask_5: 0.4662  loss_dice_5: 1.391  loss_bbox_5: 0.4204  loss_giou_5: 0.6905  loss_ce_dn_5: 0.0008971  loss_mask_dn_5: 0.4511  loss_dice_dn_5: 1.426  loss_bbox_dn_5: 0.349  loss_giou_dn_5: 0.5254  loss_ce_6: 0.7374  loss_mask_6: 0.4749  loss_dice_6: 1.44  loss_bbox_6: 0.4195  loss_giou_6: 0.7078  loss_ce_dn_6: 0.001145  loss_mask_dn_6: 0.4472  loss_dice_dn_6: 1.448  loss_bbox_dn_6: 0.3411  loss_giou_dn_6: 0.5332  loss_ce_7: 0.9176  loss_mask_7: 0.4949  loss_dice_7: 1.422  loss_bbox_7: 0.4092  loss_giou_7: 0.6426  loss_ce_dn_7: 0.001159  loss_mask_dn_7: 0.4521  loss_dice_dn_7: 1.456  loss_bbox_dn_7: 0.3413  loss_giou_dn_7: 0.5291  loss_ce_8: 0.9838  loss_mask_8: 0.4922  loss_dice_8: 1.435  loss_bbox_8: 0.4077  loss_giou_8: 0.6449  loss_ce_dn_8: 0.001351  loss_mask_dn_8: 0.4505  loss_dice_dn_8: 1.45  loss_bbox_dn_8: 0.3483  loss_giou_dn_8: 0.5288  loss_ce_interm: 1.379  loss_mask_interm: 0.512  loss_dice_interm: 1.423  loss_bbox_interm: 0.5666  loss_giou_interm: 0.8404    time: 0.4995  last_time: 0.4787  data_time: 0.0034  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:46 d2.utils.events]:  eta: 0:26:50  iter: 739  total_loss: 91.05  loss_ce: 1.068  loss_mask: 0.7137  loss_dice: 1.598  loss_bbox: 0.5129  loss_giou: 0.6942  loss_ce_dn: 0.001015  loss_mask_dn: 0.6564  loss_dice_dn: 1.653  loss_bbox_dn: 0.462  loss_giou_dn: 0.5665  loss_ce_0: 1.212  loss_mask_0: 0.684  loss_dice_0: 1.753  loss_bbox_0: 0.8106  loss_giou_0: 1.011  loss_ce_dn_0: 0.06244  loss_mask_dn_0: 1.171  loss_dice_dn_0: 3.71  loss_bbox_dn_0: 0.8723  loss_giou_dn_0: 0.8482  loss_ce_1: 1.224  loss_mask_1: 0.7073  loss_dice_1: 1.711  loss_bbox_1: 0.6525  loss_giou_1: 0.866  loss_ce_dn_1: 0.002515  loss_mask_dn_1: 0.6707  loss_dice_dn_1: 1.691  loss_bbox_dn_1: 0.4946  loss_giou_dn_1: 0.619  loss_ce_2: 1.067  loss_mask_2: 0.6749  loss_dice_2: 1.721  loss_bbox_2: 0.5703  loss_giou_2: 0.8095  loss_ce_dn_2: 0.001832  loss_mask_dn_2: 0.6752  loss_dice_dn_2: 1.628  loss_bbox_dn_2: 0.463  loss_giou_dn_2: 0.5851  loss_ce_3: 1.134  loss_mask_3: 0.7409  loss_dice_3: 1.656  loss_bbox_3: 0.5671  loss_giou_3: 0.7283  loss_ce_dn_3: 0.0008599  loss_mask_dn_3: 0.6675  loss_dice_dn_3: 1.669  loss_bbox_dn_3: 0.4462  loss_giou_dn_3: 0.5877  loss_ce_4: 1.169  loss_mask_4: 0.7197  loss_dice_4: 1.618  loss_bbox_4: 0.5566  loss_giou_4: 0.7143  loss_ce_dn_4: 0.0007072  loss_mask_dn_4: 0.6762  loss_dice_dn_4: 1.688  loss_bbox_dn_4: 0.4641  loss_giou_dn_4: 0.5664  loss_ce_5: 1.129  loss_mask_5: 0.7419  loss_dice_5: 1.576  loss_bbox_5: 0.5152  loss_giou_5: 0.6875  loss_ce_dn_5: 0.0007545  loss_mask_dn_5: 0.6524  loss_dice_dn_5: 1.677  loss_bbox_dn_5: 0.4551  loss_giou_dn_5: 0.5687  loss_ce_6: 1.007  loss_mask_6: 0.7187  loss_dice_6: 1.725  loss_bbox_6: 0.5114  loss_giou_6: 0.7405  loss_ce_dn_6: 0.0008658  loss_mask_dn_6: 0.6532  loss_dice_dn_6: 1.675  loss_bbox_dn_6: 0.4624  loss_giou_dn_6: 0.5726  loss_ce_7: 0.9959  loss_mask_7: 0.7082  loss_dice_7: 1.791  loss_bbox_7: 0.517  loss_giou_7: 0.6852  loss_ce_dn_7: 0.000742  loss_mask_dn_7: 0.6492  loss_dice_dn_7: 1.629  loss_bbox_dn_7: 0.466  loss_giou_dn_7: 0.57  loss_ce_8: 1.039  loss_mask_8: 0.725  loss_dice_8: 1.561  loss_bbox_8: 0.5075  loss_giou_8: 0.7084  loss_ce_dn_8: 0.0008574  loss_mask_dn_8: 0.6546  loss_dice_dn_8: 1.635  loss_bbox_dn_8: 0.4652  loss_giou_dn_8: 0.5687  loss_ce_interm: 1.206  loss_mask_interm: 0.787  loss_dice_interm: 1.817  loss_bbox_interm: 0.6019  loss_giou_interm: 0.9444    time: 0.4990  last_time: 0.4730  data_time: 0.0035  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:08:57 d2.utils.events]:  eta: 0:26:40  iter: 759  total_loss: 90.15  loss_ce: 0.8344  loss_mask: 0.5605  loss_dice: 1.936  loss_bbox: 0.5246  loss_giou: 0.8208  loss_ce_dn: 0.0007579  loss_mask_dn: 0.4757  loss_dice_dn: 1.717  loss_bbox_dn: 0.3519  loss_giou_dn: 0.5488  loss_ce_0: 1.215  loss_mask_0: 0.5733  loss_dice_0: 2.009  loss_bbox_0: 0.7513  loss_giou_0: 1.036  loss_ce_dn_0: 0.06135  loss_mask_dn_0: 0.7902  loss_dice_dn_0: 3.25  loss_bbox_dn_0: 0.8131  loss_giou_dn_0: 0.8449  loss_ce_1: 1.092  loss_mask_1: 0.5861  loss_dice_1: 2.101  loss_bbox_1: 0.5041  loss_giou_1: 0.7558  loss_ce_dn_1: 0.002073  loss_mask_dn_1: 0.5266  loss_dice_dn_1: 1.808  loss_bbox_dn_1: 0.4165  loss_giou_dn_1: 0.6257  loss_ce_2: 0.9417  loss_mask_2: 0.5793  loss_dice_2: 2.11  loss_bbox_2: 0.5703  loss_giou_2: 0.8086  loss_ce_dn_2: 0.001285  loss_mask_dn_2: 0.5057  loss_dice_dn_2: 1.764  loss_bbox_dn_2: 0.3885  loss_giou_dn_2: 0.5827  loss_ce_3: 0.8199  loss_mask_3: 0.5448  loss_dice_3: 1.982  loss_bbox_3: 0.5705  loss_giou_3: 0.7853  loss_ce_dn_3: 0.0007135  loss_mask_dn_3: 0.4716  loss_dice_dn_3: 1.724  loss_bbox_dn_3: 0.3863  loss_giou_dn_3: 0.5691  loss_ce_4: 0.7938  loss_mask_4: 0.5547  loss_dice_4: 1.955  loss_bbox_4: 0.5337  loss_giou_4: 0.8026  loss_ce_dn_4: 0.0007114  loss_mask_dn_4: 0.4901  loss_dice_dn_4: 1.723  loss_bbox_dn_4: 0.3673  loss_giou_dn_4: 0.551  loss_ce_5: 0.7383  loss_mask_5: 0.545  loss_dice_5: 2.021  loss_bbox_5: 0.5329  loss_giou_5: 0.8009  loss_ce_dn_5: 0.000747  loss_mask_dn_5: 0.479  loss_dice_dn_5: 1.699  loss_bbox_dn_5: 0.3721  loss_giou_dn_5: 0.5545  loss_ce_6: 0.8826  loss_mask_6: 0.5487  loss_dice_6: 1.998  loss_bbox_6: 0.5169  loss_giou_6: 0.8164  loss_ce_dn_6: 0.0008971  loss_mask_dn_6: 0.467  loss_dice_dn_6: 1.704  loss_bbox_dn_6: 0.3664  loss_giou_dn_6: 0.5493  loss_ce_7: 0.7187  loss_mask_7: 0.5444  loss_dice_7: 2.028  loss_bbox_7: 0.5155  loss_giou_7: 0.816  loss_ce_dn_7: 0.0007365  loss_mask_dn_7: 0.466  loss_dice_dn_7: 1.7  loss_bbox_dn_7: 0.3583  loss_giou_dn_7: 0.5538  loss_ce_8: 0.835  loss_mask_8: 0.5378  loss_dice_8: 2.042  loss_bbox_8: 0.5293  loss_giou_8: 0.8166  loss_ce_dn_8: 0.0007339  loss_mask_dn_8: 0.4721  loss_dice_dn_8: 1.696  loss_bbox_dn_8: 0.3563  loss_giou_dn_8: 0.5496  loss_ce_interm: 1.391  loss_mask_interm: 0.6052  loss_dice_interm: 2.122  loss_bbox_interm: 0.6228  loss_giou_interm: 0.8873    time: 0.4990  last_time: 0.4727  data_time: 0.0034  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:09:07 d2.utils.events]:  eta: 0:26:31  iter: 779  total_loss: 88.99  loss_ce: 0.876  loss_mask: 0.3154  loss_dice: 1.827  loss_bbox: 0.4446  loss_giou: 0.8193  loss_ce_dn: 0.000659  loss_mask_dn: 0.2987  loss_dice_dn: 1.815  loss_bbox_dn: 0.2818  loss_giou_dn: 0.5771  loss_ce_0: 1.447  loss_mask_0: 0.3514  loss_dice_0: 1.899  loss_bbox_0: 0.6482  loss_giou_0: 1.101  loss_ce_dn_0: 0.09081  loss_mask_dn_0: 0.8021  loss_dice_dn_0: 3.62  loss_bbox_dn_0: 0.7163  loss_giou_dn_0: 0.8495  loss_ce_1: 1.236  loss_mask_1: 0.3543  loss_dice_1: 2.021  loss_bbox_1: 0.4369  loss_giou_1: 0.802  loss_ce_dn_1: 0.001795  loss_mask_dn_1: 0.3715  loss_dice_dn_1: 1.806  loss_bbox_dn_1: 0.3533  loss_giou_dn_1: 0.6278  loss_ce_2: 1.007  loss_mask_2: 0.3469  loss_dice_2: 1.872  loss_bbox_2: 0.5059  loss_giou_2: 0.8568  loss_ce_dn_2: 0.0008573  loss_mask_dn_2: 0.3306  loss_dice_dn_2: 1.766  loss_bbox_dn_2: 0.3155  loss_giou_dn_2: 0.5901  loss_ce_3: 0.9231  loss_mask_3: 0.3222  loss_dice_3: 1.98  loss_bbox_3: 0.4594  loss_giou_3: 0.819  loss_ce_dn_3: 0.0005753  loss_mask_dn_3: 0.3078  loss_dice_dn_3: 1.778  loss_bbox_dn_3: 0.3149  loss_giou_dn_3: 0.5862  loss_ce_4: 0.8291  loss_mask_4: 0.3333  loss_dice_4: 1.891  loss_bbox_4: 0.4858  loss_giou_4: 0.8468  loss_ce_dn_4: 0.0003904  loss_mask_dn_4: 0.2892  loss_dice_dn_4: 1.763  loss_bbox_dn_4: 0.3085  loss_giou_dn_4: 0.5745  loss_ce_5: 0.8579  loss_mask_5: 0.3337  loss_dice_5: 1.841  loss_bbox_5: 0.438  loss_giou_5: 0.8279  loss_ce_dn_5: 0.0004356  loss_mask_dn_5: 0.2873  loss_dice_dn_5: 1.771  loss_bbox_dn_5: 0.2858  loss_giou_dn_5: 0.574  loss_ce_6: 0.9074  loss_mask_6: 0.3565  loss_dice_6: 1.995  loss_bbox_6: 0.4324  loss_giou_6: 0.8324  loss_ce_dn_6: 0.0006202  loss_mask_dn_6: 0.2919  loss_dice_dn_6: 1.812  loss_bbox_dn_6: 0.2857  loss_giou_dn_6: 0.5787  loss_ce_7: 0.8494  loss_mask_7: 0.3155  loss_dice_7: 1.84  loss_bbox_7: 0.4369  loss_giou_7: 0.8394  loss_ce_dn_7: 0.0005349  loss_mask_dn_7: 0.2878  loss_dice_dn_7: 1.799  loss_bbox_dn_7: 0.2805  loss_giou_dn_7: 0.5773  loss_ce_8: 0.879  loss_mask_8: 0.3355  loss_dice_8: 1.867  loss_bbox_8: 0.4272  loss_giou_8: 0.8132  loss_ce_dn_8: 0.0005382  loss_mask_dn_8: 0.2946  loss_dice_dn_8: 1.815  loss_bbox_dn_8: 0.2798  loss_giou_dn_8: 0.5756  loss_ce_interm: 1.368  loss_mask_interm: 0.3447  loss_dice_interm: 1.877  loss_bbox_interm: 0.4544  loss_giou_interm: 0.9133    time: 0.4991  last_time: 0.5297  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:09:17 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:09:17 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:09:17 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:09:17 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:09:18 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0709 s/iter. Eval: 0.0097 s/iter. Total: 0.0815 s/iter. ETA=0:00:04\n",
      "[03/14 17:09:23 d2.evaluation.evaluator]: Total inference time: 0:00:05.418231 (0.087391 s / iter per device, on 1 devices)\n",
      "[03/14 17:09:23 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075863 s / iter per device, on 1 devices)\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.626\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.296\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.337\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.271\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.516\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 29.937 | 62.557 | 29.565 | 33.738 | 34.420 |  nan  |\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:09:23 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.148\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.539\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.246\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.278\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 14.784 | 53.930 | 0.391  | 13.501 | 20.084 |  nan  |\n",
      "[03/14 17:09:23 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:09:23 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: 29.9365,62.5568,29.5654,33.7381,34.4203,nan\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:09:23 d2.evaluation.testing]: copypaste: 14.7839,53.9303,0.3913,13.5005,20.0842,nan\n",
      "[03/14 17:09:23 d2.utils.events]:  eta: 0:26:20  iter: 799  total_loss: 79.69  loss_ce: 0.854  loss_mask: 0.4081  loss_dice: 1.432  loss_bbox: 0.5209  loss_giou: 0.7556  loss_ce_dn: 0.0005903  loss_mask_dn: 0.4799  loss_dice_dn: 1.439  loss_bbox_dn: 0.3666  loss_giou_dn: 0.5838  loss_ce_0: 1.153  loss_mask_0: 0.4769  loss_dice_0: 1.653  loss_bbox_0: 0.8069  loss_giou_0: 1.157  loss_ce_dn_0: 0.08891  loss_mask_dn_0: 0.8687  loss_dice_dn_0: 3.237  loss_bbox_dn_0: 0.8147  loss_giou_dn_0: 0.8532  loss_ce_1: 1.152  loss_mask_1: 0.5492  loss_dice_1: 1.602  loss_bbox_1: 0.5438  loss_giou_1: 0.7702  loss_ce_dn_1: 0.002785  loss_mask_dn_1: 0.5592  loss_dice_dn_1: 1.454  loss_bbox_dn_1: 0.4388  loss_giou_dn_1: 0.625  loss_ce_2: 0.9612  loss_mask_2: 0.4953  loss_dice_2: 1.542  loss_bbox_2: 0.5095  loss_giou_2: 0.7774  loss_ce_dn_2: 0.001388  loss_mask_dn_2: 0.5026  loss_dice_dn_2: 1.458  loss_bbox_dn_2: 0.3938  loss_giou_dn_2: 0.5809  loss_ce_3: 0.8093  loss_mask_3: 0.5315  loss_dice_3: 1.721  loss_bbox_3: 0.5145  loss_giou_3: 0.7708  loss_ce_dn_3: 0.0006599  loss_mask_dn_3: 0.4863  loss_dice_dn_3: 1.434  loss_bbox_dn_3: 0.3861  loss_giou_dn_3: 0.5835  loss_ce_4: 0.9756  loss_mask_4: 0.4406  loss_dice_4: 1.488  loss_bbox_4: 0.4922  loss_giou_4: 0.7106  loss_ce_dn_4: 0.0007064  loss_mask_dn_4: 0.473  loss_dice_dn_4: 1.415  loss_bbox_dn_4: 0.3615  loss_giou_dn_4: 0.5742  loss_ce_5: 0.9167  loss_mask_5: 0.429  loss_dice_5: 1.532  loss_bbox_5: 0.5014  loss_giou_5: 0.742  loss_ce_dn_5: 0.0005916  loss_mask_dn_5: 0.468  loss_dice_dn_5: 1.456  loss_bbox_dn_5: 0.371  loss_giou_dn_5: 0.5759  loss_ce_6: 0.84  loss_mask_6: 0.4551  loss_dice_6: 1.451  loss_bbox_6: 0.5366  loss_giou_6: 0.7753  loss_ce_dn_6: 0.0005799  loss_mask_dn_6: 0.4866  loss_dice_dn_6: 1.435  loss_bbox_dn_6: 0.3727  loss_giou_dn_6: 0.5843  loss_ce_7: 0.8534  loss_mask_7: 0.5242  loss_dice_7: 1.54  loss_bbox_7: 0.5367  loss_giou_7: 0.7781  loss_ce_dn_7: 0.0004556  loss_mask_dn_7: 0.4777  loss_dice_dn_7: 1.468  loss_bbox_dn_7: 0.3784  loss_giou_dn_7: 0.5748  loss_ce_8: 0.7793  loss_mask_8: 0.5046  loss_dice_8: 1.59  loss_bbox_8: 0.5235  loss_giou_8: 0.7497  loss_ce_dn_8: 0.0005139  loss_mask_dn_8: 0.4763  loss_dice_dn_8: 1.445  loss_bbox_dn_8: 0.3631  loss_giou_dn_8: 0.5828  loss_ce_interm: 1.222  loss_mask_interm: 0.5005  loss_dice_interm: 1.589  loss_bbox_interm: 0.583  loss_giou_interm: 0.8838    time: 0.4989  last_time: 0.4339  data_time: 0.0036  last_data_time: 0.0029   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:09:33 d2.utils.events]:  eta: 0:26:07  iter: 819  total_loss: 81.82  loss_ce: 0.6796  loss_mask: 0.4501  loss_dice: 1.661  loss_bbox: 0.4576  loss_giou: 0.7049  loss_ce_dn: 0.0004096  loss_mask_dn: 0.4385  loss_dice_dn: 1.646  loss_bbox_dn: 0.3436  loss_giou_dn: 0.556  loss_ce_0: 1.007  loss_mask_0: 0.4648  loss_dice_0: 1.716  loss_bbox_0: 0.7493  loss_giou_0: 1.139  loss_ce_dn_0: 0.1179  loss_mask_dn_0: 0.7635  loss_dice_dn_0: 3.366  loss_bbox_dn_0: 0.7335  loss_giou_dn_0: 0.8564  loss_ce_1: 1.012  loss_mask_1: 0.4343  loss_dice_1: 1.612  loss_bbox_1: 0.4135  loss_giou_1: 0.8236  loss_ce_dn_1: 0.002994  loss_mask_dn_1: 0.4625  loss_dice_dn_1: 1.724  loss_bbox_dn_1: 0.4171  loss_giou_dn_1: 0.621  loss_ce_2: 0.9394  loss_mask_2: 0.4531  loss_dice_2: 1.647  loss_bbox_2: 0.4438  loss_giou_2: 0.7342  loss_ce_dn_2: 0.001095  loss_mask_dn_2: 0.4415  loss_dice_dn_2: 1.661  loss_bbox_dn_2: 0.3462  loss_giou_dn_2: 0.5731  loss_ce_3: 0.9027  loss_mask_3: 0.4245  loss_dice_3: 1.656  loss_bbox_3: 0.4601  loss_giou_3: 0.7423  loss_ce_dn_3: 0.000474  loss_mask_dn_3: 0.4353  loss_dice_dn_3: 1.65  loss_bbox_dn_3: 0.3445  loss_giou_dn_3: 0.5585  loss_ce_4: 0.7509  loss_mask_4: 0.4795  loss_dice_4: 1.682  loss_bbox_4: 0.4757  loss_giou_4: 0.7415  loss_ce_dn_4: 0.0004974  loss_mask_dn_4: 0.4374  loss_dice_dn_4: 1.652  loss_bbox_dn_4: 0.3578  loss_giou_dn_4: 0.5693  loss_ce_5: 0.7475  loss_mask_5: 0.4217  loss_dice_5: 1.608  loss_bbox_5: 0.4687  loss_giou_5: 0.7117  loss_ce_dn_5: 0.0004601  loss_mask_dn_5: 0.4212  loss_dice_dn_5: 1.643  loss_bbox_dn_5: 0.3467  loss_giou_dn_5: 0.5688  loss_ce_6: 0.761  loss_mask_6: 0.4382  loss_dice_6: 1.612  loss_bbox_6: 0.4936  loss_giou_6: 0.7199  loss_ce_dn_6: 0.0004877  loss_mask_dn_6: 0.4232  loss_dice_dn_6: 1.626  loss_bbox_dn_6: 0.3492  loss_giou_dn_6: 0.5634  loss_ce_7: 0.775  loss_mask_7: 0.43  loss_dice_7: 1.539  loss_bbox_7: 0.4611  loss_giou_7: 0.7392  loss_ce_dn_7: 0.0003757  loss_mask_dn_7: 0.4341  loss_dice_dn_7: 1.617  loss_bbox_dn_7: 0.3445  loss_giou_dn_7: 0.5561  loss_ce_8: 0.7338  loss_mask_8: 0.4422  loss_dice_8: 1.691  loss_bbox_8: 0.4603  loss_giou_8: 0.7032  loss_ce_dn_8: 0.0003708  loss_mask_dn_8: 0.4376  loss_dice_dn_8: 1.619  loss_bbox_dn_8: 0.3435  loss_giou_dn_8: 0.5552  loss_ce_interm: 1.205  loss_mask_interm: 0.4336  loss_dice_interm: 1.657  loss_bbox_interm: 0.5638  loss_giou_interm: 0.8547    time: 0.4983  last_time: 0.4878  data_time: 0.0035  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:09:43 d2.utils.events]:  eta: 0:25:54  iter: 839  total_loss: 79.4  loss_ce: 0.8374  loss_mask: 0.4003  loss_dice: 1.647  loss_bbox: 0.3989  loss_giou: 0.6895  loss_ce_dn: 0.0003635  loss_mask_dn: 0.4157  loss_dice_dn: 1.49  loss_bbox_dn: 0.2899  loss_giou_dn: 0.4916  loss_ce_0: 1.17  loss_mask_0: 0.4571  loss_dice_0: 1.681  loss_bbox_0: 0.7333  loss_giou_0: 0.9089  loss_ce_dn_0: 0.09198  loss_mask_dn_0: 0.6951  loss_dice_dn_0: 3.309  loss_bbox_dn_0: 0.7395  loss_giou_dn_0: 0.8512  loss_ce_1: 1.077  loss_mask_1: 0.4048  loss_dice_1: 1.626  loss_bbox_1: 0.4597  loss_giou_1: 0.734  loss_ce_dn_1: 0.002505  loss_mask_dn_1: 0.406  loss_dice_dn_1: 1.635  loss_bbox_dn_1: 0.3721  loss_giou_dn_1: 0.5734  loss_ce_2: 0.9608  loss_mask_2: 0.4006  loss_dice_2: 1.685  loss_bbox_2: 0.4711  loss_giou_2: 0.6727  loss_ce_dn_2: 0.001512  loss_mask_dn_2: 0.4146  loss_dice_dn_2: 1.579  loss_bbox_dn_2: 0.3274  loss_giou_dn_2: 0.5163  loss_ce_3: 0.9653  loss_mask_3: 0.3757  loss_dice_3: 1.611  loss_bbox_3: 0.4339  loss_giou_3: 0.6881  loss_ce_dn_3: 0.0005652  loss_mask_dn_3: 0.4183  loss_dice_dn_3: 1.484  loss_bbox_dn_3: 0.2988  loss_giou_dn_3: 0.5001  loss_ce_4: 0.8176  loss_mask_4: 0.3912  loss_dice_4: 1.599  loss_bbox_4: 0.3892  loss_giou_4: 0.662  loss_ce_dn_4: 0.0006809  loss_mask_dn_4: 0.4159  loss_dice_dn_4: 1.485  loss_bbox_dn_4: 0.2899  loss_giou_dn_4: 0.4918  loss_ce_5: 0.7544  loss_mask_5: 0.4374  loss_dice_5: 1.607  loss_bbox_5: 0.415  loss_giou_5: 0.6986  loss_ce_dn_5: 0.0005338  loss_mask_dn_5: 0.4129  loss_dice_dn_5: 1.496  loss_bbox_dn_5: 0.2883  loss_giou_dn_5: 0.4958  loss_ce_6: 0.7733  loss_mask_6: 0.3816  loss_dice_6: 1.632  loss_bbox_6: 0.4255  loss_giou_6: 0.709  loss_ce_dn_6: 0.0005676  loss_mask_dn_6: 0.4031  loss_dice_dn_6: 1.478  loss_bbox_dn_6: 0.2838  loss_giou_dn_6: 0.4923  loss_ce_7: 0.7981  loss_mask_7: 0.3994  loss_dice_7: 1.574  loss_bbox_7: 0.3834  loss_giou_7: 0.6928  loss_ce_dn_7: 0.0004324  loss_mask_dn_7: 0.4119  loss_dice_dn_7: 1.499  loss_bbox_dn_7: 0.2864  loss_giou_dn_7: 0.4928  loss_ce_8: 0.8338  loss_mask_8: 0.3891  loss_dice_8: 1.58  loss_bbox_8: 0.3927  loss_giou_8: 0.6866  loss_ce_dn_8: 0.000379  loss_mask_dn_8: 0.4171  loss_dice_dn_8: 1.481  loss_bbox_dn_8: 0.288  loss_giou_dn_8: 0.4876  loss_ce_interm: 1.279  loss_mask_interm: 0.4214  loss_dice_interm: 1.678  loss_bbox_interm: 0.5701  loss_giou_interm: 0.8459    time: 0.4978  last_time: 0.4532  data_time: 0.0034  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:09:52 d2.utils.events]:  eta: 0:25:42  iter: 859  total_loss: 84.51  loss_ce: 0.7679  loss_mask: 0.5436  loss_dice: 1.57  loss_bbox: 0.3641  loss_giou: 0.6305  loss_ce_dn: 0.0005373  loss_mask_dn: 0.5538  loss_dice_dn: 1.586  loss_bbox_dn: 0.2776  loss_giou_dn: 0.5223  loss_ce_0: 1.244  loss_mask_0: 0.5379  loss_dice_0: 1.528  loss_bbox_0: 0.7798  loss_giou_0: 0.9625  loss_ce_dn_0: 0.1143  loss_mask_dn_0: 0.7901  loss_dice_dn_0: 3.124  loss_bbox_dn_0: 0.9271  loss_giou_dn_0: 0.8513  loss_ce_1: 1.182  loss_mask_1: 0.5927  loss_dice_1: 1.537  loss_bbox_1: 0.4534  loss_giou_1: 0.6825  loss_ce_dn_1: 0.005601  loss_mask_dn_1: 0.6051  loss_dice_dn_1: 1.626  loss_bbox_dn_1: 0.4956  loss_giou_dn_1: 0.6296  loss_ce_2: 0.911  loss_mask_2: 0.5291  loss_dice_2: 1.553  loss_bbox_2: 0.4374  loss_giou_2: 0.6229  loss_ce_dn_2: 0.0009818  loss_mask_dn_2: 0.5919  loss_dice_dn_2: 1.566  loss_bbox_dn_2: 0.371  loss_giou_dn_2: 0.5626  loss_ce_3: 0.9392  loss_mask_3: 0.5558  loss_dice_3: 1.516  loss_bbox_3: 0.4498  loss_giou_3: 0.6001  loss_ce_dn_3: 0.0004286  loss_mask_dn_3: 0.5712  loss_dice_dn_3: 1.56  loss_bbox_dn_3: 0.3378  loss_giou_dn_3: 0.5302  loss_ce_4: 0.75  loss_mask_4: 0.5655  loss_dice_4: 1.494  loss_bbox_4: 0.4622  loss_giou_4: 0.6092  loss_ce_dn_4: 0.0003852  loss_mask_dn_4: 0.5592  loss_dice_dn_4: 1.555  loss_bbox_dn_4: 0.3206  loss_giou_dn_4: 0.5314  loss_ce_5: 0.8136  loss_mask_5: 0.5633  loss_dice_5: 1.53  loss_bbox_5: 0.3992  loss_giou_5: 0.6327  loss_ce_dn_5: 0.0003911  loss_mask_dn_5: 0.5645  loss_dice_dn_5: 1.548  loss_bbox_dn_5: 0.2855  loss_giou_dn_5: 0.5182  loss_ce_6: 0.7916  loss_mask_6: 0.5576  loss_dice_6: 1.49  loss_bbox_6: 0.4773  loss_giou_6: 0.6328  loss_ce_dn_6: 0.0005244  loss_mask_dn_6: 0.5686  loss_dice_dn_6: 1.559  loss_bbox_dn_6: 0.2815  loss_giou_dn_6: 0.5144  loss_ce_7: 0.7839  loss_mask_7: 0.5375  loss_dice_7: 1.537  loss_bbox_7: 0.4103  loss_giou_7: 0.6208  loss_ce_dn_7: 0.0004511  loss_mask_dn_7: 0.5604  loss_dice_dn_7: 1.57  loss_bbox_dn_7: 0.2764  loss_giou_dn_7: 0.514  loss_ce_8: 0.7202  loss_mask_8: 0.5265  loss_dice_8: 1.531  loss_bbox_8: 0.4135  loss_giou_8: 0.626  loss_ce_dn_8: 0.0005058  loss_mask_dn_8: 0.5544  loss_dice_dn_8: 1.576  loss_bbox_dn_8: 0.2755  loss_giou_dn_8: 0.519  loss_ce_interm: 1.127  loss_mask_interm: 0.5397  loss_dice_interm: 1.576  loss_bbox_interm: 0.473  loss_giou_interm: 0.7409    time: 0.4974  last_time: 0.4721  data_time: 0.0035  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:02 d2.utils.events]:  eta: 0:25:32  iter: 879  total_loss: 86.31  loss_ce: 1.09  loss_mask: 0.5384  loss_dice: 1.547  loss_bbox: 0.4471  loss_giou: 0.759  loss_ce_dn: 0.0008573  loss_mask_dn: 0.4458  loss_dice_dn: 1.645  loss_bbox_dn: 0.4098  loss_giou_dn: 0.6031  loss_ce_0: 1.208  loss_mask_0: 0.5347  loss_dice_0: 1.679  loss_bbox_0: 0.5883  loss_giou_0: 1.034  loss_ce_dn_0: 0.05216  loss_mask_dn_0: 0.8463  loss_dice_dn_0: 3.274  loss_bbox_dn_0: 0.8604  loss_giou_dn_0: 0.8515  loss_ce_1: 1.043  loss_mask_1: 0.4728  loss_dice_1: 1.633  loss_bbox_1: 0.4916  loss_giou_1: 0.7677  loss_ce_dn_1: 0.001247  loss_mask_dn_1: 0.4927  loss_dice_dn_1: 1.661  loss_bbox_dn_1: 0.39  loss_giou_dn_1: 0.6322  loss_ce_2: 1.156  loss_mask_2: 0.4892  loss_dice_2: 1.624  loss_bbox_2: 0.4387  loss_giou_2: 0.717  loss_ce_dn_2: 0.001046  loss_mask_dn_2: 0.4555  loss_dice_dn_2: 1.686  loss_bbox_dn_2: 0.4153  loss_giou_dn_2: 0.5979  loss_ce_3: 1.377  loss_mask_3: 0.5651  loss_dice_3: 1.574  loss_bbox_3: 0.4973  loss_giou_3: 0.7276  loss_ce_dn_3: 0.001063  loss_mask_dn_3: 0.4466  loss_dice_dn_3: 1.655  loss_bbox_dn_3: 0.425  loss_giou_dn_3: 0.5957  loss_ce_4: 1.247  loss_mask_4: 0.4894  loss_dice_4: 1.583  loss_bbox_4: 0.4751  loss_giou_4: 0.6861  loss_ce_dn_4: 0.001097  loss_mask_dn_4: 0.4597  loss_dice_dn_4: 1.598  loss_bbox_dn_4: 0.4141  loss_giou_dn_4: 0.604  loss_ce_5: 1.176  loss_mask_5: 0.508  loss_dice_5: 1.536  loss_bbox_5: 0.4624  loss_giou_5: 0.7451  loss_ce_dn_5: 0.0009605  loss_mask_dn_5: 0.4511  loss_dice_dn_5: 1.607  loss_bbox_dn_5: 0.4186  loss_giou_dn_5: 0.6077  loss_ce_6: 1.074  loss_mask_6: 0.5418  loss_dice_6: 1.517  loss_bbox_6: 0.4389  loss_giou_6: 0.7523  loss_ce_dn_6: 0.001119  loss_mask_dn_6: 0.4462  loss_dice_dn_6: 1.592  loss_bbox_dn_6: 0.4102  loss_giou_dn_6: 0.6097  loss_ce_7: 0.99  loss_mask_7: 0.4774  loss_dice_7: 1.618  loss_bbox_7: 0.4507  loss_giou_7: 0.7539  loss_ce_dn_7: 0.0008271  loss_mask_dn_7: 0.4485  loss_dice_dn_7: 1.605  loss_bbox_dn_7: 0.4074  loss_giou_dn_7: 0.6075  loss_ce_8: 1.051  loss_mask_8: 0.5457  loss_dice_8: 1.517  loss_bbox_8: 0.4684  loss_giou_8: 0.7569  loss_ce_dn_8: 0.0009492  loss_mask_dn_8: 0.4501  loss_dice_dn_8: 1.641  loss_bbox_dn_8: 0.4118  loss_giou_dn_8: 0.6063  loss_ce_interm: 1.096  loss_mask_interm: 0.5594  loss_dice_interm: 1.678  loss_bbox_interm: 0.463  loss_giou_interm: 0.8151    time: 0.4972  last_time: 0.5099  data_time: 0.0035  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:12 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:10:12 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:10:12 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:10:12 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:10:13 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0836 s/iter. Eval: 0.0086 s/iter. Total: 0.0932 s/iter. ETA=0:00:05\n",
      "[03/14 17:10:18 d2.evaluation.evaluator]: Total inference time: 0:00:05.503841 (0.088772 s / iter per device, on 1 devices)\n",
      "[03/14 17:10:18 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077546 s / iter per device, on 1 devices)\n",
      "[03/14 17:10:18 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:10:18 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:10:19 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.644\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.311\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.365\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.530\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:10:19 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 31.345 | 64.377 | 31.085 | 36.456 | 34.167 |  nan  |\n",
      "[03/14 17:10:19 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:10:19 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.128\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.525\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.218\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.139\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.283\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.352\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:10:19 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 12.808 | 52.475 | 0.808  | 10.716 | 21.789 |  nan  |\n",
      "[03/14 17:10:19 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:10:19 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: 31.3454,64.3767,31.0848,36.4560,34.1670,nan\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:10:19 d2.evaluation.testing]: copypaste: 12.8080,52.4746,0.8080,10.7160,21.7889,nan\n",
      "[03/14 17:10:19 d2.utils.events]:  eta: 0:25:22  iter: 899  total_loss: 81.27  loss_ce: 0.7513  loss_mask: 0.4009  loss_dice: 1.525  loss_bbox: 0.3726  loss_giou: 0.7133  loss_ce_dn: 0.0004862  loss_mask_dn: 0.4122  loss_dice_dn: 1.652  loss_bbox_dn: 0.2899  loss_giou_dn: 0.5375  loss_ce_0: 1.068  loss_mask_0: 0.3974  loss_dice_0: 1.698  loss_bbox_0: 0.6069  loss_giou_0: 1.073  loss_ce_dn_0: 0.06964  loss_mask_dn_0: 0.6132  loss_dice_dn_0: 3.425  loss_bbox_dn_0: 0.6866  loss_giou_dn_0: 0.8496  loss_ce_1: 1.065  loss_mask_1: 0.3778  loss_dice_1: 1.595  loss_bbox_1: 0.4163  loss_giou_1: 0.7243  loss_ce_dn_1: 0.001365  loss_mask_dn_1: 0.4544  loss_dice_dn_1: 1.625  loss_bbox_dn_1: 0.3742  loss_giou_dn_1: 0.5937  loss_ce_2: 1.097  loss_mask_2: 0.3574  loss_dice_2: 1.677  loss_bbox_2: 0.3947  loss_giou_2: 0.7282  loss_ce_dn_2: 0.001035  loss_mask_dn_2: 0.4401  loss_dice_dn_2: 1.558  loss_bbox_dn_2: 0.3384  loss_giou_dn_2: 0.5644  loss_ce_3: 1.3  loss_mask_3: 0.3722  loss_dice_3: 1.6  loss_bbox_3: 0.3461  loss_giou_3: 0.6546  loss_ce_dn_3: 0.0003876  loss_mask_dn_3: 0.4182  loss_dice_dn_3: 1.575  loss_bbox_dn_3: 0.3184  loss_giou_dn_3: 0.5553  loss_ce_4: 0.9292  loss_mask_4: 0.4298  loss_dice_4: 1.548  loss_bbox_4: 0.4172  loss_giou_4: 0.6459  loss_ce_dn_4: 0.0003276  loss_mask_dn_4: 0.4237  loss_dice_dn_4: 1.612  loss_bbox_dn_4: 0.3016  loss_giou_dn_4: 0.5315  loss_ce_5: 0.8784  loss_mask_5: 0.3959  loss_dice_5: 1.65  loss_bbox_5: 0.3794  loss_giou_5: 0.7028  loss_ce_dn_5: 0.0003952  loss_mask_dn_5: 0.4127  loss_dice_dn_5: 1.607  loss_bbox_dn_5: 0.301  loss_giou_dn_5: 0.5404  loss_ce_6: 0.8312  loss_mask_6: 0.3965  loss_dice_6: 1.507  loss_bbox_6: 0.3689  loss_giou_6: 0.6693  loss_ce_dn_6: 0.00046  loss_mask_dn_6: 0.4211  loss_dice_dn_6: 1.661  loss_bbox_dn_6: 0.2971  loss_giou_dn_6: 0.5364  loss_ce_7: 0.7842  loss_mask_7: 0.3966  loss_dice_7: 1.586  loss_bbox_7: 0.3635  loss_giou_7: 0.6763  loss_ce_dn_7: 0.0003896  loss_mask_dn_7: 0.4184  loss_dice_dn_7: 1.652  loss_bbox_dn_7: 0.2953  loss_giou_dn_7: 0.5365  loss_ce_8: 0.7648  loss_mask_8: 0.4143  loss_dice_8: 1.549  loss_bbox_8: 0.3793  loss_giou_8: 0.6833  loss_ce_dn_8: 0.0004007  loss_mask_dn_8: 0.4157  loss_dice_dn_8: 1.651  loss_bbox_dn_8: 0.292  loss_giou_dn_8: 0.536  loss_ce_interm: 1.11  loss_mask_interm: 0.4414  loss_dice_interm: 1.717  loss_bbox_interm: 0.4642  loss_giou_interm: 0.8007    time: 0.4971  last_time: 0.4835  data_time: 0.0036  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:29 d2.utils.events]:  eta: 0:25:11  iter: 919  total_loss: 78.64  loss_ce: 0.6493  loss_mask: 0.4394  loss_dice: 1.466  loss_bbox: 0.4592  loss_giou: 0.8045  loss_ce_dn: 0.0007109  loss_mask_dn: 0.4276  loss_dice_dn: 1.392  loss_bbox_dn: 0.3752  loss_giou_dn: 0.5742  loss_ce_0: 1.187  loss_mask_0: 0.4852  loss_dice_0: 1.517  loss_bbox_0: 0.6863  loss_giou_0: 1.027  loss_ce_dn_0: 0.05045  loss_mask_dn_0: 0.8367  loss_dice_dn_0: 3.102  loss_bbox_dn_0: 0.754  loss_giou_dn_0: 0.8449  loss_ce_1: 1.229  loss_mask_1: 0.4013  loss_dice_1: 1.456  loss_bbox_1: 0.4398  loss_giou_1: 0.7873  loss_ce_dn_1: 0.001115  loss_mask_dn_1: 0.444  loss_dice_dn_1: 1.459  loss_bbox_dn_1: 0.4034  loss_giou_dn_1: 0.5922  loss_ce_2: 1.09  loss_mask_2: 0.4112  loss_dice_2: 1.554  loss_bbox_2: 0.4095  loss_giou_2: 0.6771  loss_ce_dn_2: 0.000807  loss_mask_dn_2: 0.4302  loss_dice_dn_2: 1.439  loss_bbox_dn_2: 0.3637  loss_giou_dn_2: 0.5716  loss_ce_3: 1.181  loss_mask_3: 0.4347  loss_dice_3: 1.421  loss_bbox_3: 0.3842  loss_giou_3: 0.6741  loss_ce_dn_3: 0.0003466  loss_mask_dn_3: 0.451  loss_dice_dn_3: 1.402  loss_bbox_dn_3: 0.3803  loss_giou_dn_3: 0.5549  loss_ce_4: 1.152  loss_mask_4: 0.4622  loss_dice_4: 1.419  loss_bbox_4: 0.4011  loss_giou_4: 0.6538  loss_ce_dn_4: 0.0005448  loss_mask_dn_4: 0.4561  loss_dice_dn_4: 1.377  loss_bbox_dn_4: 0.3799  loss_giou_dn_4: 0.5879  loss_ce_5: 0.7871  loss_mask_5: 0.4361  loss_dice_5: 1.467  loss_bbox_5: 0.4426  loss_giou_5: 0.7391  loss_ce_dn_5: 0.0007011  loss_mask_dn_5: 0.4377  loss_dice_dn_5: 1.425  loss_bbox_dn_5: 0.3786  loss_giou_dn_5: 0.5774  loss_ce_6: 0.6895  loss_mask_6: 0.4339  loss_dice_6: 1.426  loss_bbox_6: 0.4631  loss_giou_6: 0.8028  loss_ce_dn_6: 0.001308  loss_mask_dn_6: 0.4342  loss_dice_dn_6: 1.393  loss_bbox_dn_6: 0.3791  loss_giou_dn_6: 0.5754  loss_ce_7: 0.6471  loss_mask_7: 0.4313  loss_dice_7: 1.435  loss_bbox_7: 0.4577  loss_giou_7: 0.7969  loss_ce_dn_7: 0.0007947  loss_mask_dn_7: 0.429  loss_dice_dn_7: 1.396  loss_bbox_dn_7: 0.372  loss_giou_dn_7: 0.574  loss_ce_8: 0.6446  loss_mask_8: 0.4463  loss_dice_8: 1.465  loss_bbox_8: 0.4537  loss_giou_8: 0.8017  loss_ce_dn_8: 0.0007554  loss_mask_dn_8: 0.4229  loss_dice_dn_8: 1.397  loss_bbox_dn_8: 0.3748  loss_giou_dn_8: 0.5746  loss_ce_interm: 1.278  loss_mask_interm: 0.4213  loss_dice_interm: 1.476  loss_bbox_interm: 0.5  loss_giou_interm: 0.7907    time: 0.4967  last_time: 0.4557  data_time: 0.0036  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:39 d2.utils.events]:  eta: 0:25:02  iter: 939  total_loss: 84.2  loss_ce: 0.6258  loss_mask: 0.6364  loss_dice: 1.644  loss_bbox: 0.5202  loss_giou: 0.6903  loss_ce_dn: 0.0006643  loss_mask_dn: 0.5301  loss_dice_dn: 1.566  loss_bbox_dn: 0.3883  loss_giou_dn: 0.5619  loss_ce_0: 1.311  loss_mask_0: 0.6355  loss_dice_0: 1.666  loss_bbox_0: 0.8074  loss_giou_0: 0.8984  loss_ce_dn_0: 0.07977  loss_mask_dn_0: 0.9714  loss_dice_dn_0: 3.049  loss_bbox_dn_0: 0.8362  loss_giou_dn_0: 0.8572  loss_ce_1: 1.074  loss_mask_1: 0.6084  loss_dice_1: 1.619  loss_bbox_1: 0.5035  loss_giou_1: 0.7366  loss_ce_dn_1: 0.002165  loss_mask_dn_1: 0.5512  loss_dice_dn_1: 1.58  loss_bbox_dn_1: 0.4871  loss_giou_dn_1: 0.6204  loss_ce_2: 1  loss_mask_2: 0.6198  loss_dice_2: 1.736  loss_bbox_2: 0.5326  loss_giou_2: 0.7391  loss_ce_dn_2: 0.0007492  loss_mask_dn_2: 0.5431  loss_dice_dn_2: 1.614  loss_bbox_dn_2: 0.4547  loss_giou_dn_2: 0.5992  loss_ce_3: 1.237  loss_mask_3: 0.5686  loss_dice_3: 1.542  loss_bbox_3: 0.4162  loss_giou_3: 0.7509  loss_ce_dn_3: 0.0005429  loss_mask_dn_3: 0.5399  loss_dice_dn_3: 1.577  loss_bbox_dn_3: 0.4223  loss_giou_dn_3: 0.5859  loss_ce_4: 1.029  loss_mask_4: 0.6427  loss_dice_4: 1.579  loss_bbox_4: 0.4575  loss_giou_4: 0.7139  loss_ce_dn_4: 0.0007508  loss_mask_dn_4: 0.571  loss_dice_dn_4: 1.551  loss_bbox_dn_4: 0.4015  loss_giou_dn_4: 0.5667  loss_ce_5: 0.8344  loss_mask_5: 0.643  loss_dice_5: 1.656  loss_bbox_5: 0.4596  loss_giou_5: 0.7408  loss_ce_dn_5: 0.0008904  loss_mask_dn_5: 0.5721  loss_dice_dn_5: 1.554  loss_bbox_dn_5: 0.3941  loss_giou_dn_5: 0.5637  loss_ce_6: 0.7132  loss_mask_6: 0.6274  loss_dice_6: 1.631  loss_bbox_6: 0.4687  loss_giou_6: 0.6754  loss_ce_dn_6: 0.00111  loss_mask_dn_6: 0.5568  loss_dice_dn_6: 1.556  loss_bbox_dn_6: 0.3881  loss_giou_dn_6: 0.5632  loss_ce_7: 0.6094  loss_mask_7: 0.6027  loss_dice_7: 1.597  loss_bbox_7: 0.4974  loss_giou_7: 0.6866  loss_ce_dn_7: 0.000656  loss_mask_dn_7: 0.5384  loss_dice_dn_7: 1.558  loss_bbox_dn_7: 0.3897  loss_giou_dn_7: 0.5576  loss_ce_8: 0.6236  loss_mask_8: 0.6185  loss_dice_8: 1.629  loss_bbox_8: 0.523  loss_giou_8: 0.6948  loss_ce_dn_8: 0.0006256  loss_mask_dn_8: 0.5357  loss_dice_dn_8: 1.562  loss_bbox_dn_8: 0.3853  loss_giou_dn_8: 0.5569  loss_ce_interm: 1.424  loss_mask_interm: 0.6018  loss_dice_interm: 1.541  loss_bbox_interm: 0.566  loss_giou_interm: 0.8215    time: 0.4966  last_time: 0.5007  data_time: 0.0037  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:48 d2.utils.events]:  eta: 0:24:51  iter: 959  total_loss: 73.06  loss_ce: 0.812  loss_mask: 0.3781  loss_dice: 1.307  loss_bbox: 0.3985  loss_giou: 0.7022  loss_ce_dn: 0.000407  loss_mask_dn: 0.355  loss_dice_dn: 1.356  loss_bbox_dn: 0.3097  loss_giou_dn: 0.5027  loss_ce_0: 1.102  loss_mask_0: 0.3943  loss_dice_0: 1.472  loss_bbox_0: 0.7184  loss_giou_0: 1.089  loss_ce_dn_0: 0.05041  loss_mask_dn_0: 0.4714  loss_dice_dn_0: 2.305  loss_bbox_dn_0: 0.746  loss_giou_dn_0: 0.8546  loss_ce_1: 1.053  loss_mask_1: 0.393  loss_dice_1: 1.277  loss_bbox_1: 0.4332  loss_giou_1: 0.696  loss_ce_dn_1: 0.002785  loss_mask_dn_1: 0.3925  loss_dice_dn_1: 1.362  loss_bbox_dn_1: 0.3382  loss_giou_dn_1: 0.5631  loss_ce_2: 1.14  loss_mask_2: 0.385  loss_dice_2: 1.374  loss_bbox_2: 0.3267  loss_giou_2: 0.6044  loss_ce_dn_2: 0.001478  loss_mask_dn_2: 0.4184  loss_dice_dn_2: 1.351  loss_bbox_dn_2: 0.3316  loss_giou_dn_2: 0.534  loss_ce_3: 0.9626  loss_mask_3: 0.3722  loss_dice_3: 1.319  loss_bbox_3: 0.3496  loss_giou_3: 0.5993  loss_ce_dn_3: 0.0009202  loss_mask_dn_3: 0.3942  loss_dice_dn_3: 1.345  loss_bbox_dn_3: 0.3108  loss_giou_dn_3: 0.5021  loss_ce_4: 0.7989  loss_mask_4: 0.3811  loss_dice_4: 1.288  loss_bbox_4: 0.3985  loss_giou_4: 0.7172  loss_ce_dn_4: 0.0006801  loss_mask_dn_4: 0.3853  loss_dice_dn_4: 1.322  loss_bbox_dn_4: 0.3014  loss_giou_dn_4: 0.4934  loss_ce_5: 0.781  loss_mask_5: 0.3678  loss_dice_5: 1.313  loss_bbox_5: 0.3912  loss_giou_5: 0.7132  loss_ce_dn_5: 0.0008731  loss_mask_dn_5: 0.381  loss_dice_dn_5: 1.318  loss_bbox_dn_5: 0.3077  loss_giou_dn_5: 0.4986  loss_ce_6: 0.8115  loss_mask_6: 0.3465  loss_dice_6: 1.316  loss_bbox_6: 0.3837  loss_giou_6: 0.6838  loss_ce_dn_6: 0.0008336  loss_mask_dn_6: 0.3647  loss_dice_dn_6: 1.343  loss_bbox_dn_6: 0.3062  loss_giou_dn_6: 0.4995  loss_ce_7: 0.7776  loss_mask_7: 0.35  loss_dice_7: 1.35  loss_bbox_7: 0.4024  loss_giou_7: 0.6852  loss_ce_dn_7: 0.0004193  loss_mask_dn_7: 0.3683  loss_dice_dn_7: 1.331  loss_bbox_dn_7: 0.3088  loss_giou_dn_7: 0.4992  loss_ce_8: 0.7886  loss_mask_8: 0.3544  loss_dice_8: 1.401  loss_bbox_8: 0.3985  loss_giou_8: 0.6997  loss_ce_dn_8: 0.0003773  loss_mask_dn_8: 0.357  loss_dice_dn_8: 1.346  loss_bbox_dn_8: 0.3087  loss_giou_dn_8: 0.5006  loss_ce_interm: 1.135  loss_mask_interm: 0.3506  loss_dice_interm: 1.433  loss_bbox_interm: 0.4385  loss_giou_interm: 0.7515    time: 0.4963  last_time: 0.4796  data_time: 0.0036  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:10:58 d2.utils.events]:  eta: 0:24:41  iter: 979  total_loss: 81.36  loss_ce: 0.6925  loss_mask: 0.4557  loss_dice: 1.438  loss_bbox: 0.433  loss_giou: 0.6513  loss_ce_dn: 0.0006104  loss_mask_dn: 0.492  loss_dice_dn: 1.434  loss_bbox_dn: 0.3673  loss_giou_dn: 0.5344  loss_ce_0: 0.9093  loss_mask_0: 0.5039  loss_dice_0: 1.463  loss_bbox_0: 0.7249  loss_giou_0: 0.9959  loss_ce_dn_0: 0.07728  loss_mask_dn_0: 0.6832  loss_dice_dn_0: 3.216  loss_bbox_dn_0: 0.8593  loss_giou_dn_0: 0.8462  loss_ce_1: 1.146  loss_mask_1: 0.4748  loss_dice_1: 1.518  loss_bbox_1: 0.5097  loss_giou_1: 0.7866  loss_ce_dn_1: 0.003334  loss_mask_dn_1: 0.4914  loss_dice_dn_1: 1.48  loss_bbox_dn_1: 0.4652  loss_giou_dn_1: 0.604  loss_ce_2: 1.097  loss_mask_2: 0.4888  loss_dice_2: 1.476  loss_bbox_2: 0.3823  loss_giou_2: 0.6835  loss_ce_dn_2: 0.002334  loss_mask_dn_2: 0.5245  loss_dice_dn_2: 1.454  loss_bbox_dn_2: 0.3799  loss_giou_dn_2: 0.5367  loss_ce_3: 1.07  loss_mask_3: 0.5192  loss_dice_3: 1.402  loss_bbox_3: 0.4378  loss_giou_3: 0.6923  loss_ce_dn_3: 0.002108  loss_mask_dn_3: 0.5195  loss_dice_dn_3: 1.445  loss_bbox_dn_3: 0.3787  loss_giou_dn_3: 0.5333  loss_ce_4: 0.9156  loss_mask_4: 0.5155  loss_dice_4: 1.427  loss_bbox_4: 0.4768  loss_giou_4: 0.7198  loss_ce_dn_4: 0.001333  loss_mask_dn_4: 0.4908  loss_dice_dn_4: 1.454  loss_bbox_dn_4: 0.3704  loss_giou_dn_4: 0.5333  loss_ce_5: 0.8097  loss_mask_5: 0.4956  loss_dice_5: 1.435  loss_bbox_5: 0.4596  loss_giou_5: 0.6875  loss_ce_dn_5: 0.001098  loss_mask_dn_5: 0.4869  loss_dice_dn_5: 1.433  loss_bbox_dn_5: 0.3705  loss_giou_dn_5: 0.5324  loss_ce_6: 0.7085  loss_mask_6: 0.4839  loss_dice_6: 1.425  loss_bbox_6: 0.4415  loss_giou_6: 0.6769  loss_ce_dn_6: 0.001144  loss_mask_dn_6: 0.4995  loss_dice_dn_6: 1.427  loss_bbox_dn_6: 0.3615  loss_giou_dn_6: 0.5315  loss_ce_7: 0.7012  loss_mask_7: 0.4525  loss_dice_7: 1.46  loss_bbox_7: 0.4379  loss_giou_7: 0.6497  loss_ce_dn_7: 0.0007577  loss_mask_dn_7: 0.4983  loss_dice_dn_7: 1.416  loss_bbox_dn_7: 0.3592  loss_giou_dn_7: 0.5306  loss_ce_8: 0.701  loss_mask_8: 0.4747  loss_dice_8: 1.459  loss_bbox_8: 0.434  loss_giou_8: 0.6535  loss_ce_dn_8: 0.000625  loss_mask_dn_8: 0.4965  loss_dice_dn_8: 1.428  loss_bbox_dn_8: 0.3658  loss_giou_dn_8: 0.5346  loss_ce_interm: 1.279  loss_mask_interm: 0.5047  loss_dice_interm: 1.387  loss_bbox_interm: 0.5673  loss_giou_interm: 0.8404    time: 0.4961  last_time: 0.4662  data_time: 0.0036  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:11:09 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:11:09 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:11:09 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:11:09 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:11:10 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0795 s/iter. Eval: 0.0101 s/iter. Total: 0.0905 s/iter. ETA=0:00:05\n",
      "[03/14 17:11:15 d2.evaluation.evaluator]: Total inference time: 0:00:05.500555 (0.088719 s / iter per device, on 1 devices)\n",
      "[03/14 17:11:15 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076649 s / iter per device, on 1 devices)\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.663\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.340\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.375\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.540\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 29.450 | 66.307 | 25.790 | 33.954 | 37.525 |  nan  |\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:11:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.111\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.262\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.386\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 14.366 | 57.406 | 1.001  | 11.117 | 26.246 |  nan  |\n",
      "[03/14 17:11:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:11:15 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: 29.4500,66.3075,25.7899,33.9536,37.5252,nan\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:11:15 d2.evaluation.testing]: copypaste: 14.3656,57.4059,1.0010,11.1174,26.2463,nan\n",
      "[03/14 17:11:15 d2.utils.events]:  eta: 0:24:30  iter: 999  total_loss: 89.08  loss_ce: 0.8146  loss_mask: 0.4987  loss_dice: 1.669  loss_bbox: 0.4711  loss_giou: 0.7646  loss_ce_dn: 0.0006826  loss_mask_dn: 0.5404  loss_dice_dn: 1.793  loss_bbox_dn: 0.3503  loss_giou_dn: 0.6323  loss_ce_0: 1.277  loss_mask_0: 0.501  loss_dice_0: 1.743  loss_bbox_0: 0.8076  loss_giou_0: 1.163  loss_ce_dn_0: 0.09355  loss_mask_dn_0: 1.078  loss_dice_dn_0: 3.269  loss_bbox_dn_0: 0.8207  loss_giou_dn_0: 0.8461  loss_ce_1: 1.097  loss_mask_1: 0.5339  loss_dice_1: 1.753  loss_bbox_1: 0.5048  loss_giou_1: 0.9123  loss_ce_dn_1: 0.003279  loss_mask_dn_1: 0.5285  loss_dice_dn_1: 1.809  loss_bbox_dn_1: 0.455  loss_giou_dn_1: 0.6366  loss_ce_2: 1.087  loss_mask_2: 0.5368  loss_dice_2: 1.833  loss_bbox_2: 0.4929  loss_giou_2: 0.8171  loss_ce_dn_2: 0.002425  loss_mask_dn_2: 0.5146  loss_dice_dn_2: 1.76  loss_bbox_dn_2: 0.3933  loss_giou_dn_2: 0.6342  loss_ce_3: 1.095  loss_mask_3: 0.5137  loss_dice_3: 1.709  loss_bbox_3: 0.4392  loss_giou_3: 0.7887  loss_ce_dn_3: 0.002065  loss_mask_dn_3: 0.5389  loss_dice_dn_3: 1.682  loss_bbox_dn_3: 0.3769  loss_giou_dn_3: 0.6309  loss_ce_4: 0.9422  loss_mask_4: 0.5687  loss_dice_4: 1.723  loss_bbox_4: 0.4542  loss_giou_4: 0.8156  loss_ce_dn_4: 0.001262  loss_mask_dn_4: 0.5435  loss_dice_dn_4: 1.761  loss_bbox_dn_4: 0.3704  loss_giou_dn_4: 0.6274  loss_ce_5: 0.9483  loss_mask_5: 0.5156  loss_dice_5: 1.661  loss_bbox_5: 0.4604  loss_giou_5: 0.8011  loss_ce_dn_5: 0.001052  loss_mask_dn_5: 0.5254  loss_dice_dn_5: 1.762  loss_bbox_dn_5: 0.3472  loss_giou_dn_5: 0.6172  loss_ce_6: 0.837  loss_mask_6: 0.4967  loss_dice_6: 1.674  loss_bbox_6: 0.4687  loss_giou_6: 0.7517  loss_ce_dn_6: 0.0008846  loss_mask_dn_6: 0.5379  loss_dice_dn_6: 1.786  loss_bbox_dn_6: 0.3408  loss_giou_dn_6: 0.6198  loss_ce_7: 0.8218  loss_mask_7: 0.5001  loss_dice_7: 1.7  loss_bbox_7: 0.4965  loss_giou_7: 0.7863  loss_ce_dn_7: 0.0008435  loss_mask_dn_7: 0.5373  loss_dice_dn_7: 1.782  loss_bbox_dn_7: 0.3405  loss_giou_dn_7: 0.6239  loss_ce_8: 0.7863  loss_mask_8: 0.4881  loss_dice_8: 1.672  loss_bbox_8: 0.483  loss_giou_8: 0.7663  loss_ce_dn_8: 0.0007211  loss_mask_dn_8: 0.535  loss_dice_dn_8: 1.785  loss_bbox_dn_8: 0.3493  loss_giou_dn_8: 0.6308  loss_ce_interm: 1.486  loss_mask_interm: 0.4972  loss_dice_interm: 1.642  loss_bbox_interm: 0.657  loss_giou_interm: 0.937    time: 0.4957  last_time: 0.4871  data_time: 0.0036  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:11:25 d2.utils.events]:  eta: 0:24:19  iter: 1019  total_loss: 77.81  loss_ce: 0.6017  loss_mask: 0.4124  loss_dice: 1.499  loss_bbox: 0.3706  loss_giou: 0.7066  loss_ce_dn: 0.0004398  loss_mask_dn: 0.4197  loss_dice_dn: 1.434  loss_bbox_dn: 0.2647  loss_giou_dn: 0.5233  loss_ce_0: 1.191  loss_mask_0: 0.5026  loss_dice_0: 1.537  loss_bbox_0: 0.659  loss_giou_0: 1.025  loss_ce_dn_0: 0.1037  loss_mask_dn_0: 0.7655  loss_dice_dn_0: 3.222  loss_bbox_dn_0: 0.7777  loss_giou_dn_0: 0.8527  loss_ce_1: 0.9129  loss_mask_1: 0.4352  loss_dice_1: 1.49  loss_bbox_1: 0.4561  loss_giou_1: 0.7592  loss_ce_dn_1: 0.001842  loss_mask_dn_1: 0.444  loss_dice_dn_1: 1.459  loss_bbox_dn_1: 0.3533  loss_giou_dn_1: 0.6008  loss_ce_2: 0.7827  loss_mask_2: 0.4757  loss_dice_2: 1.444  loss_bbox_2: 0.4096  loss_giou_2: 0.6609  loss_ce_dn_2: 0.001471  loss_mask_dn_2: 0.4507  loss_dice_dn_2: 1.418  loss_bbox_dn_2: 0.2933  loss_giou_dn_2: 0.5309  loss_ce_3: 0.8368  loss_mask_3: 0.4383  loss_dice_3: 1.501  loss_bbox_3: 0.4202  loss_giou_3: 0.7219  loss_ce_dn_3: 0.0007679  loss_mask_dn_3: 0.4394  loss_dice_dn_3: 1.473  loss_bbox_dn_3: 0.2958  loss_giou_dn_3: 0.5139  loss_ce_4: 0.7345  loss_mask_4: 0.4213  loss_dice_4: 1.543  loss_bbox_4: 0.4486  loss_giou_4: 0.7371  loss_ce_dn_4: 0.0006536  loss_mask_dn_4: 0.4224  loss_dice_dn_4: 1.462  loss_bbox_dn_4: 0.2889  loss_giou_dn_4: 0.5159  loss_ce_5: 0.6259  loss_mask_5: 0.4048  loss_dice_5: 1.459  loss_bbox_5: 0.3954  loss_giou_5: 0.7321  loss_ce_dn_5: 0.0008637  loss_mask_dn_5: 0.4345  loss_dice_dn_5: 1.446  loss_bbox_dn_5: 0.2817  loss_giou_dn_5: 0.5152  loss_ce_6: 0.595  loss_mask_6: 0.4039  loss_dice_6: 1.481  loss_bbox_6: 0.4097  loss_giou_6: 0.7366  loss_ce_dn_6: 0.0007783  loss_mask_dn_6: 0.4256  loss_dice_dn_6: 1.446  loss_bbox_dn_6: 0.2755  loss_giou_dn_6: 0.518  loss_ce_7: 0.6057  loss_mask_7: 0.4255  loss_dice_7: 1.552  loss_bbox_7: 0.4094  loss_giou_7: 0.7247  loss_ce_dn_7: 0.0004361  loss_mask_dn_7: 0.4279  loss_dice_dn_7: 1.428  loss_bbox_dn_7: 0.2742  loss_giou_dn_7: 0.5151  loss_ce_8: 0.596  loss_mask_8: 0.4407  loss_dice_8: 1.496  loss_bbox_8: 0.3885  loss_giou_8: 0.7203  loss_ce_dn_8: 0.0004063  loss_mask_dn_8: 0.4261  loss_dice_dn_8: 1.437  loss_bbox_dn_8: 0.2685  loss_giou_dn_8: 0.5214  loss_ce_interm: 1.236  loss_mask_interm: 0.4727  loss_dice_interm: 1.445  loss_bbox_interm: 0.4713  loss_giou_interm: 0.8249    time: 0.4952  last_time: 0.4367  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:11:34 d2.utils.events]:  eta: 0:24:06  iter: 1039  total_loss: 74.8  loss_ce: 0.5704  loss_mask: 0.4457  loss_dice: 1.561  loss_bbox: 0.3763  loss_giou: 0.6898  loss_ce_dn: 0.0005935  loss_mask_dn: 0.3873  loss_dice_dn: 1.5  loss_bbox_dn: 0.316  loss_giou_dn: 0.5186  loss_ce_0: 1.179  loss_mask_0: 0.422  loss_dice_0: 1.559  loss_bbox_0: 0.6655  loss_giou_0: 1.131  loss_ce_dn_0: 0.1021  loss_mask_dn_0: 0.6692  loss_dice_dn_0: 3.284  loss_bbox_dn_0: 0.7547  loss_giou_dn_0: 0.8513  loss_ce_1: 0.9612  loss_mask_1: 0.4273  loss_dice_1: 1.496  loss_bbox_1: 0.4372  loss_giou_1: 0.7216  loss_ce_dn_1: 0.002021  loss_mask_dn_1: 0.3871  loss_dice_dn_1: 1.502  loss_bbox_dn_1: 0.441  loss_giou_dn_1: 0.5914  loss_ce_2: 0.9141  loss_mask_2: 0.4421  loss_dice_2: 1.568  loss_bbox_2: 0.4321  loss_giou_2: 0.7795  loss_ce_dn_2: 0.0009079  loss_mask_dn_2: 0.3825  loss_dice_dn_2: 1.506  loss_bbox_dn_2: 0.3739  loss_giou_dn_2: 0.5205  loss_ce_3: 0.8074  loss_mask_3: 0.4026  loss_dice_3: 1.547  loss_bbox_3: 0.4376  loss_giou_3: 0.7553  loss_ce_dn_3: 0.0005905  loss_mask_dn_3: 0.379  loss_dice_dn_3: 1.493  loss_bbox_dn_3: 0.3564  loss_giou_dn_3: 0.5298  loss_ce_4: 0.6708  loss_mask_4: 0.4097  loss_dice_4: 1.482  loss_bbox_4: 0.4596  loss_giou_4: 0.6949  loss_ce_dn_4: 0.0005539  loss_mask_dn_4: 0.3713  loss_dice_dn_4: 1.506  loss_bbox_dn_4: 0.3398  loss_giou_dn_4: 0.5336  loss_ce_5: 0.6081  loss_mask_5: 0.3987  loss_dice_5: 1.51  loss_bbox_5: 0.3777  loss_giou_5: 0.7317  loss_ce_dn_5: 0.0009273  loss_mask_dn_5: 0.3831  loss_dice_dn_5: 1.496  loss_bbox_dn_5: 0.318  loss_giou_dn_5: 0.5188  loss_ce_6: 0.6715  loss_mask_6: 0.3968  loss_dice_6: 1.499  loss_bbox_6: 0.3734  loss_giou_6: 0.7363  loss_ce_dn_6: 0.001064  loss_mask_dn_6: 0.3789  loss_dice_dn_6: 1.496  loss_bbox_dn_6: 0.3199  loss_giou_dn_6: 0.5219  loss_ce_7: 0.5986  loss_mask_7: 0.4073  loss_dice_7: 1.586  loss_bbox_7: 0.3747  loss_giou_7: 0.6894  loss_ce_dn_7: 0.0005848  loss_mask_dn_7: 0.3878  loss_dice_dn_7: 1.482  loss_bbox_dn_7: 0.3143  loss_giou_dn_7: 0.5158  loss_ce_8: 0.5749  loss_mask_8: 0.4435  loss_dice_8: 1.517  loss_bbox_8: 0.374  loss_giou_8: 0.6816  loss_ce_dn_8: 0.0005072  loss_mask_dn_8: 0.3862  loss_dice_dn_8: 1.495  loss_bbox_dn_8: 0.3179  loss_giou_dn_8: 0.5194  loss_ce_interm: 1.354  loss_mask_interm: 0.4382  loss_dice_interm: 1.387  loss_bbox_interm: 0.456  loss_giou_interm: 0.7816    time: 0.4948  last_time: 0.5175  data_time: 0.0036  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:11:44 d2.utils.events]:  eta: 0:23:53  iter: 1059  total_loss: 90.39  loss_ce: 0.8878  loss_mask: 0.4706  loss_dice: 1.838  loss_bbox: 0.4234  loss_giou: 0.8107  loss_ce_dn: 0.001001  loss_mask_dn: 0.4495  loss_dice_dn: 1.657  loss_bbox_dn: 0.3315  loss_giou_dn: 0.5701  loss_ce_0: 1.371  loss_mask_0: 0.4358  loss_dice_0: 1.933  loss_bbox_0: 0.7641  loss_giou_0: 1.083  loss_ce_dn_0: 0.0435  loss_mask_dn_0: 0.7321  loss_dice_dn_0: 3.358  loss_bbox_dn_0: 0.8081  loss_giou_dn_0: 0.8492  loss_ce_1: 1.181  loss_mask_1: 0.4592  loss_dice_1: 1.803  loss_bbox_1: 0.4332  loss_giou_1: 0.7554  loss_ce_dn_1: 0.001723  loss_mask_dn_1: 0.4564  loss_dice_dn_1: 1.67  loss_bbox_dn_1: 0.4019  loss_giou_dn_1: 0.6262  loss_ce_2: 1.003  loss_mask_2: 0.4631  loss_dice_2: 1.826  loss_bbox_2: 0.4374  loss_giou_2: 0.7711  loss_ce_dn_2: 0.001037  loss_mask_dn_2: 0.4576  loss_dice_dn_2: 1.668  loss_bbox_dn_2: 0.3631  loss_giou_dn_2: 0.6015  loss_ce_3: 1.112  loss_mask_3: 0.4448  loss_dice_3: 1.61  loss_bbox_3: 0.4413  loss_giou_3: 0.8073  loss_ce_dn_3: 0.001371  loss_mask_dn_3: 0.4495  loss_dice_dn_3: 1.65  loss_bbox_dn_3: 0.3605  loss_giou_dn_3: 0.5948  loss_ce_4: 1.031  loss_mask_4: 0.45  loss_dice_4: 1.776  loss_bbox_4: 0.4731  loss_giou_4: 0.8145  loss_ce_dn_4: 0.001407  loss_mask_dn_4: 0.442  loss_dice_dn_4: 1.64  loss_bbox_dn_4: 0.349  loss_giou_dn_4: 0.5865  loss_ce_5: 0.9848  loss_mask_5: 0.477  loss_dice_5: 1.796  loss_bbox_5: 0.4141  loss_giou_5: 0.801  loss_ce_dn_5: 0.001254  loss_mask_dn_5: 0.448  loss_dice_dn_5: 1.623  loss_bbox_dn_5: 0.3439  loss_giou_dn_5: 0.5775  loss_ce_6: 0.9682  loss_mask_6: 0.4474  loss_dice_6: 1.73  loss_bbox_6: 0.4252  loss_giou_6: 0.7933  loss_ce_dn_6: 0.001375  loss_mask_dn_6: 0.4445  loss_dice_dn_6: 1.652  loss_bbox_dn_6: 0.3335  loss_giou_dn_6: 0.5778  loss_ce_7: 0.8497  loss_mask_7: 0.4699  loss_dice_7: 1.866  loss_bbox_7: 0.4377  loss_giou_7: 0.7869  loss_ce_dn_7: 0.0008883  loss_mask_dn_7: 0.4438  loss_dice_dn_7: 1.631  loss_bbox_dn_7: 0.3342  loss_giou_dn_7: 0.5736  loss_ce_8: 0.8326  loss_mask_8: 0.4633  loss_dice_8: 1.876  loss_bbox_8: 0.4172  loss_giou_8: 0.7731  loss_ce_dn_8: 0.0008993  loss_mask_dn_8: 0.4449  loss_dice_dn_8: 1.649  loss_bbox_dn_8: 0.3337  loss_giou_dn_8: 0.5706  loss_ce_interm: 1.381  loss_mask_interm: 0.4596  loss_dice_interm: 1.651  loss_bbox_interm: 0.5494  loss_giou_interm: 0.8917    time: 0.4943  last_time: 0.4709  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:11:53 d2.utils.events]:  eta: 0:23:40  iter: 1079  total_loss: 87.83  loss_ce: 0.676  loss_mask: 0.5412  loss_dice: 1.669  loss_bbox: 0.5253  loss_giou: 0.63  loss_ce_dn: 0.0005649  loss_mask_dn: 0.5147  loss_dice_dn: 1.675  loss_bbox_dn: 0.3785  loss_giou_dn: 0.5494  loss_ce_0: 1.306  loss_mask_0: 0.6036  loss_dice_0: 1.863  loss_bbox_0: 0.9308  loss_giou_0: 1.041  loss_ce_dn_0: 0.07057  loss_mask_dn_0: 0.9698  loss_dice_dn_0: 3.042  loss_bbox_dn_0: 0.8386  loss_giou_dn_0: 0.8593  loss_ce_1: 1.224  loss_mask_1: 0.5484  loss_dice_1: 1.75  loss_bbox_1: 0.5786  loss_giou_1: 0.7724  loss_ce_dn_1: 0.002518  loss_mask_dn_1: 0.5573  loss_dice_dn_1: 1.757  loss_bbox_dn_1: 0.456  loss_giou_dn_1: 0.6267  loss_ce_2: 1.131  loss_mask_2: 0.5824  loss_dice_2: 1.65  loss_bbox_2: 0.5208  loss_giou_2: 0.6778  loss_ce_dn_2: 0.002202  loss_mask_dn_2: 0.5242  loss_dice_dn_2: 1.665  loss_bbox_dn_2: 0.399  loss_giou_dn_2: 0.5845  loss_ce_3: 0.9831  loss_mask_3: 0.5294  loss_dice_3: 1.689  loss_bbox_3: 0.5323  loss_giou_3: 0.6217  loss_ce_dn_3: 0.001716  loss_mask_dn_3: 0.5148  loss_dice_dn_3: 1.657  loss_bbox_dn_3: 0.3894  loss_giou_dn_3: 0.5648  loss_ce_4: 0.795  loss_mask_4: 0.5231  loss_dice_4: 1.678  loss_bbox_4: 0.5234  loss_giou_4: 0.6039  loss_ce_dn_4: 0.001143  loss_mask_dn_4: 0.5263  loss_dice_dn_4: 1.691  loss_bbox_dn_4: 0.3837  loss_giou_dn_4: 0.5556  loss_ce_5: 0.8649  loss_mask_5: 0.5116  loss_dice_5: 1.612  loss_bbox_5: 0.4957  loss_giou_5: 0.5932  loss_ce_dn_5: 0.001173  loss_mask_dn_5: 0.5218  loss_dice_dn_5: 1.665  loss_bbox_dn_5: 0.3814  loss_giou_dn_5: 0.5559  loss_ce_6: 0.7675  loss_mask_6: 0.5206  loss_dice_6: 1.676  loss_bbox_6: 0.5068  loss_giou_6: 0.614  loss_ce_dn_6: 0.001118  loss_mask_dn_6: 0.5239  loss_dice_dn_6: 1.679  loss_bbox_dn_6: 0.3859  loss_giou_dn_6: 0.5529  loss_ce_7: 0.6937  loss_mask_7: 0.5317  loss_dice_7: 1.633  loss_bbox_7: 0.5253  loss_giou_7: 0.6521  loss_ce_dn_7: 0.000822  loss_mask_dn_7: 0.5229  loss_dice_dn_7: 1.679  loss_bbox_dn_7: 0.3798  loss_giou_dn_7: 0.5539  loss_ce_8: 0.7502  loss_mask_8: 0.515  loss_dice_8: 1.698  loss_bbox_8: 0.5271  loss_giou_8: 0.6359  loss_ce_dn_8: 0.0005826  loss_mask_dn_8: 0.5128  loss_dice_dn_8: 1.694  loss_bbox_dn_8: 0.381  loss_giou_dn_8: 0.5469  loss_ce_interm: 1.224  loss_mask_interm: 0.571  loss_dice_interm: 1.765  loss_bbox_interm: 0.5518  loss_giou_interm: 0.8168    time: 0.4940  last_time: 0.4811  data_time: 0.0035  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:04 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:12:04 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:12:04 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:12:04 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:12:05 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0010 s/iter. Inference: 0.0852 s/iter. Eval: 0.0105 s/iter. Total: 0.0967 s/iter. ETA=0:00:05\n",
      "[03/14 17:12:10 d2.evaluation.evaluator]: Total inference time: 0:00:05.401867 (0.087127 s / iter per device, on 1 devices)\n",
      "[03/14 17:12:10 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.074892 s / iter per device, on 1 devices)\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.280\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.564\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.531\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 29.162 | 68.317 | 21.340 | 27.951 | 41.198 |  nan  |\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:12:10 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.545\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.112\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.141\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.277\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.386\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 14.247 | 54.489 | 0.852  | 11.209 | 24.961 |  nan  |\n",
      "[03/14 17:12:10 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:12:10 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: 29.1618,68.3166,21.3403,27.9514,41.1985,nan\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:12:10 d2.evaluation.testing]: copypaste: 14.2470,54.4894,0.8515,11.2089,24.9608,nan\n",
      "[03/14 17:12:10 d2.utils.events]:  eta: 0:23:28  iter: 1099  total_loss: 66.87  loss_ce: 0.4118  loss_mask: 0.4387  loss_dice: 1.257  loss_bbox: 0.4177  loss_giou: 0.5015  loss_ce_dn: 0.0005985  loss_mask_dn: 0.3972  loss_dice_dn: 1.326  loss_bbox_dn: 0.3552  loss_giou_dn: 0.4691  loss_ce_0: 0.9594  loss_mask_0: 0.4486  loss_dice_0: 1.313  loss_bbox_0: 0.7206  loss_giou_0: 0.8385  loss_ce_dn_0: 0.04173  loss_mask_dn_0: 0.6809  loss_dice_dn_0: 2.642  loss_bbox_dn_0: 0.8242  loss_giou_dn_0: 0.8514  loss_ce_1: 0.8021  loss_mask_1: 0.4394  loss_dice_1: 1.367  loss_bbox_1: 0.4049  loss_giou_1: 0.5549  loss_ce_dn_1: 0.001699  loss_mask_dn_1: 0.4538  loss_dice_dn_1: 1.373  loss_bbox_dn_1: 0.3944  loss_giou_dn_1: 0.5169  loss_ce_2: 0.7266  loss_mask_2: 0.4184  loss_dice_2: 1.198  loss_bbox_2: 0.4297  loss_giou_2: 0.5201  loss_ce_dn_2: 0.001604  loss_mask_dn_2: 0.4427  loss_dice_dn_2: 1.261  loss_bbox_dn_2: 0.354  loss_giou_dn_2: 0.494  loss_ce_3: 0.544  loss_mask_3: 0.4439  loss_dice_3: 1.242  loss_bbox_3: 0.4394  loss_giou_3: 0.5569  loss_ce_dn_3: 0.001422  loss_mask_dn_3: 0.4258  loss_dice_dn_3: 1.228  loss_bbox_dn_3: 0.3453  loss_giou_dn_3: 0.4676  loss_ce_4: 0.4728  loss_mask_4: 0.4324  loss_dice_4: 1.199  loss_bbox_4: 0.4578  loss_giou_4: 0.5397  loss_ce_dn_4: 0.000953  loss_mask_dn_4: 0.4004  loss_dice_dn_4: 1.23  loss_bbox_dn_4: 0.366  loss_giou_dn_4: 0.4725  loss_ce_5: 0.4443  loss_mask_5: 0.4595  loss_dice_5: 1.219  loss_bbox_5: 0.4551  loss_giou_5: 0.4839  loss_ce_dn_5: 0.001236  loss_mask_dn_5: 0.4091  loss_dice_dn_5: 1.267  loss_bbox_dn_5: 0.3515  loss_giou_dn_5: 0.4691  loss_ce_6: 0.4602  loss_mask_6: 0.4178  loss_dice_6: 1.288  loss_bbox_6: 0.4447  loss_giou_6: 0.4843  loss_ce_dn_6: 0.001222  loss_mask_dn_6: 0.3994  loss_dice_dn_6: 1.279  loss_bbox_dn_6: 0.3496  loss_giou_dn_6: 0.4685  loss_ce_7: 0.4259  loss_mask_7: 0.4418  loss_dice_7: 1.286  loss_bbox_7: 0.44  loss_giou_7: 0.4783  loss_ce_dn_7: 0.0007361  loss_mask_dn_7: 0.4026  loss_dice_dn_7: 1.269  loss_bbox_dn_7: 0.3502  loss_giou_dn_7: 0.4657  loss_ce_8: 0.4212  loss_mask_8: 0.4451  loss_dice_8: 1.29  loss_bbox_8: 0.4213  loss_giou_8: 0.4928  loss_ce_dn_8: 0.000597  loss_mask_dn_8: 0.3976  loss_dice_dn_8: 1.319  loss_bbox_dn_8: 0.3523  loss_giou_dn_8: 0.4671  loss_ce_interm: 1.06  loss_mask_interm: 0.4498  loss_dice_interm: 1.322  loss_bbox_interm: 0.4393  loss_giou_interm: 0.5978    time: 0.4939  last_time: 0.4681  data_time: 0.0036  last_data_time: 0.0041   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:20 d2.utils.events]:  eta: 0:23:18  iter: 1119  total_loss: 76.52  loss_ce: 0.525  loss_mask: 0.4015  loss_dice: 1.682  loss_bbox: 0.3967  loss_giou: 0.7036  loss_ce_dn: 0.0005115  loss_mask_dn: 0.3469  loss_dice_dn: 1.547  loss_bbox_dn: 0.3016  loss_giou_dn: 0.535  loss_ce_0: 0.9819  loss_mask_0: 0.3436  loss_dice_0: 1.644  loss_bbox_0: 0.7331  loss_giou_0: 1.14  loss_ce_dn_0: 0.04046  loss_mask_dn_0: 0.5657  loss_dice_dn_0: 3.149  loss_bbox_dn_0: 0.6492  loss_giou_dn_0: 0.8519  loss_ce_1: 0.9718  loss_mask_1: 0.3604  loss_dice_1: 1.595  loss_bbox_1: 0.3802  loss_giou_1: 0.6279  loss_ce_dn_1: 0.0006169  loss_mask_dn_1: 0.3759  loss_dice_dn_1: 1.617  loss_bbox_dn_1: 0.3402  loss_giou_dn_1: 0.5902  loss_ce_2: 0.7958  loss_mask_2: 0.3283  loss_dice_2: 1.606  loss_bbox_2: 0.4051  loss_giou_2: 0.6749  loss_ce_dn_2: 0.001057  loss_mask_dn_2: 0.3666  loss_dice_dn_2: 1.628  loss_bbox_dn_2: 0.3147  loss_giou_dn_2: 0.5535  loss_ce_3: 0.6029  loss_mask_3: 0.3914  loss_dice_3: 1.63  loss_bbox_3: 0.3861  loss_giou_3: 0.6706  loss_ce_dn_3: 0.0008  loss_mask_dn_3: 0.3574  loss_dice_dn_3: 1.571  loss_bbox_dn_3: 0.2985  loss_giou_dn_3: 0.5571  loss_ce_4: 0.5493  loss_mask_4: 0.3845  loss_dice_4: 1.657  loss_bbox_4: 0.374  loss_giou_4: 0.7138  loss_ce_dn_4: 0.0006673  loss_mask_dn_4: 0.3635  loss_dice_dn_4: 1.607  loss_bbox_dn_4: 0.2967  loss_giou_dn_4: 0.5462  loss_ce_5: 0.6051  loss_mask_5: 0.401  loss_dice_5: 1.665  loss_bbox_5: 0.3708  loss_giou_5: 0.7111  loss_ce_dn_5: 0.0007369  loss_mask_dn_5: 0.3755  loss_dice_dn_5: 1.566  loss_bbox_dn_5: 0.3095  loss_giou_dn_5: 0.5374  loss_ce_6: 0.5814  loss_mask_6: 0.376  loss_dice_6: 1.648  loss_bbox_6: 0.4021  loss_giou_6: 0.7144  loss_ce_dn_6: 0.0008155  loss_mask_dn_6: 0.3414  loss_dice_dn_6: 1.559  loss_bbox_dn_6: 0.3063  loss_giou_dn_6: 0.5309  loss_ce_7: 0.5858  loss_mask_7: 0.3717  loss_dice_7: 1.608  loss_bbox_7: 0.3915  loss_giou_7: 0.7036  loss_ce_dn_7: 0.0005133  loss_mask_dn_7: 0.3537  loss_dice_dn_7: 1.536  loss_bbox_dn_7: 0.2994  loss_giou_dn_7: 0.5355  loss_ce_8: 0.587  loss_mask_8: 0.3892  loss_dice_8: 1.627  loss_bbox_8: 0.3932  loss_giou_8: 0.7127  loss_ce_dn_8: 0.000553  loss_mask_dn_8: 0.3514  loss_dice_dn_8: 1.529  loss_bbox_dn_8: 0.3002  loss_giou_dn_8: 0.5345  loss_ce_interm: 1.065  loss_mask_interm: 0.3644  loss_dice_interm: 1.65  loss_bbox_interm: 0.4272  loss_giou_interm: 0.7557    time: 0.4939  last_time: 0.4918  data_time: 0.0037  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:30 d2.utils.events]:  eta: 0:23:07  iter: 1139  total_loss: 76.68  loss_ce: 0.7126  loss_mask: 0.4171  loss_dice: 1.6  loss_bbox: 0.3616  loss_giou: 0.7116  loss_ce_dn: 0.0003693  loss_mask_dn: 0.396  loss_dice_dn: 1.543  loss_bbox_dn: 0.281  loss_giou_dn: 0.4911  loss_ce_0: 1.135  loss_mask_0: 0.4754  loss_dice_0: 1.555  loss_bbox_0: 0.7532  loss_giou_0: 1.039  loss_ce_dn_0: 0.03992  loss_mask_dn_0: 0.766  loss_dice_dn_0: 3  loss_bbox_dn_0: 0.7892  loss_giou_dn_0: 0.8429  loss_ce_1: 0.8883  loss_mask_1: 0.4612  loss_dice_1: 1.508  loss_bbox_1: 0.4615  loss_giou_1: 0.7793  loss_ce_dn_1: 0.001025  loss_mask_dn_1: 0.418  loss_dice_dn_1: 1.56  loss_bbox_dn_1: 0.4103  loss_giou_dn_1: 0.5546  loss_ce_2: 0.8584  loss_mask_2: 0.4151  loss_dice_2: 1.515  loss_bbox_2: 0.4521  loss_giou_2: 0.6906  loss_ce_dn_2: 0.0007567  loss_mask_dn_2: 0.4142  loss_dice_dn_2: 1.51  loss_bbox_dn_2: 0.3101  loss_giou_dn_2: 0.542  loss_ce_3: 0.7659  loss_mask_3: 0.3989  loss_dice_3: 1.621  loss_bbox_3: 0.4141  loss_giou_3: 0.6657  loss_ce_dn_3: 0.0004482  loss_mask_dn_3: 0.4139  loss_dice_dn_3: 1.529  loss_bbox_dn_3: 0.2883  loss_giou_dn_3: 0.5187  loss_ce_4: 0.8065  loss_mask_4: 0.4039  loss_dice_4: 1.528  loss_bbox_4: 0.3928  loss_giou_4: 0.6742  loss_ce_dn_4: 0.000365  loss_mask_dn_4: 0.3998  loss_dice_dn_4: 1.5  loss_bbox_dn_4: 0.2806  loss_giou_dn_4: 0.5092  loss_ce_5: 0.7167  loss_mask_5: 0.3924  loss_dice_5: 1.547  loss_bbox_5: 0.377  loss_giou_5: 0.6806  loss_ce_dn_5: 0.0005059  loss_mask_dn_5: 0.3954  loss_dice_dn_5: 1.498  loss_bbox_dn_5: 0.2752  loss_giou_dn_5: 0.4963  loss_ce_6: 0.6684  loss_mask_6: 0.4315  loss_dice_6: 1.605  loss_bbox_6: 0.3875  loss_giou_6: 0.7365  loss_ce_dn_6: 0.000597  loss_mask_dn_6: 0.3932  loss_dice_dn_6: 1.518  loss_bbox_dn_6: 0.2744  loss_giou_dn_6: 0.4924  loss_ce_7: 0.7221  loss_mask_7: 0.4347  loss_dice_7: 1.518  loss_bbox_7: 0.3809  loss_giou_7: 0.7417  loss_ce_dn_7: 0.0003779  loss_mask_dn_7: 0.3904  loss_dice_dn_7: 1.538  loss_bbox_dn_7: 0.2777  loss_giou_dn_7: 0.4944  loss_ce_8: 0.6721  loss_mask_8: 0.4308  loss_dice_8: 1.565  loss_bbox_8: 0.3649  loss_giou_8: 0.7198  loss_ce_dn_8: 0.0003316  loss_mask_dn_8: 0.3904  loss_dice_dn_8: 1.528  loss_bbox_dn_8: 0.2813  loss_giou_dn_8: 0.4904  loss_ce_interm: 1.199  loss_mask_interm: 0.4846  loss_dice_interm: 1.655  loss_bbox_interm: 0.5059  loss_giou_interm: 0.8178    time: 0.4937  last_time: 0.4984  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:40 d2.utils.events]:  eta: 0:22:57  iter: 1159  total_loss: 81.25  loss_ce: 0.709  loss_mask: 0.4225  loss_dice: 1.605  loss_bbox: 0.396  loss_giou: 0.6736  loss_ce_dn: 0.0004409  loss_mask_dn: 0.4697  loss_dice_dn: 1.56  loss_bbox_dn: 0.322  loss_giou_dn: 0.523  loss_ce_0: 1.112  loss_mask_0: 0.501  loss_dice_0: 1.649  loss_bbox_0: 0.6748  loss_giou_0: 0.9943  loss_ce_dn_0: 0.09366  loss_mask_dn_0: 0.9167  loss_dice_dn_0: 3.007  loss_bbox_dn_0: 0.7588  loss_giou_dn_0: 0.8552  loss_ce_1: 1.022  loss_mask_1: 0.4842  loss_dice_1: 1.659  loss_bbox_1: 0.4365  loss_giou_1: 0.7153  loss_ce_dn_1: 0.001295  loss_mask_dn_1: 0.4672  loss_dice_dn_1: 1.571  loss_bbox_dn_1: 0.4212  loss_giou_dn_1: 0.6203  loss_ce_2: 0.8212  loss_mask_2: 0.4721  loss_dice_2: 1.627  loss_bbox_2: 0.4335  loss_giou_2: 0.6368  loss_ce_dn_2: 0.0008311  loss_mask_dn_2: 0.4496  loss_dice_dn_2: 1.568  loss_bbox_dn_2: 0.3815  loss_giou_dn_2: 0.5534  loss_ce_3: 0.6701  loss_mask_3: 0.445  loss_dice_3: 1.547  loss_bbox_3: 0.4112  loss_giou_3: 0.6916  loss_ce_dn_3: 0.0005243  loss_mask_dn_3: 0.4725  loss_dice_dn_3: 1.572  loss_bbox_dn_3: 0.3417  loss_giou_dn_3: 0.5361  loss_ce_4: 0.6699  loss_mask_4: 0.4379  loss_dice_4: 1.598  loss_bbox_4: 0.3905  loss_giou_4: 0.6831  loss_ce_dn_4: 0.0003295  loss_mask_dn_4: 0.4668  loss_dice_dn_4: 1.55  loss_bbox_dn_4: 0.3231  loss_giou_dn_4: 0.53  loss_ce_5: 0.6707  loss_mask_5: 0.4533  loss_dice_5: 1.593  loss_bbox_5: 0.4078  loss_giou_5: 0.679  loss_ce_dn_5: 0.0004305  loss_mask_dn_5: 0.462  loss_dice_dn_5: 1.547  loss_bbox_dn_5: 0.3272  loss_giou_dn_5: 0.5287  loss_ce_6: 0.689  loss_mask_6: 0.4565  loss_dice_6: 1.541  loss_bbox_6: 0.4041  loss_giou_6: 0.6649  loss_ce_dn_6: 0.000436  loss_mask_dn_6: 0.4668  loss_dice_dn_6: 1.558  loss_bbox_dn_6: 0.3207  loss_giou_dn_6: 0.5257  loss_ce_7: 0.6676  loss_mask_7: 0.4256  loss_dice_7: 1.542  loss_bbox_7: 0.4002  loss_giou_7: 0.6745  loss_ce_dn_7: 0.0003547  loss_mask_dn_7: 0.47  loss_dice_dn_7: 1.561  loss_bbox_dn_7: 0.316  loss_giou_dn_7: 0.5252  loss_ce_8: 0.6841  loss_mask_8: 0.4291  loss_dice_8: 1.504  loss_bbox_8: 0.3972  loss_giou_8: 0.68  loss_ce_dn_8: 0.0003809  loss_mask_dn_8: 0.4704  loss_dice_dn_8: 1.566  loss_bbox_dn_8: 0.321  loss_giou_dn_8: 0.5213  loss_ce_interm: 1.191  loss_mask_interm: 0.5345  loss_dice_interm: 1.545  loss_bbox_interm: 0.5038  loss_giou_interm: 0.8083    time: 0.4936  last_time: 0.5116  data_time: 0.0035  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:49 d2.utils.events]:  eta: 0:22:45  iter: 1179  total_loss: 87.07  loss_ce: 0.7555  loss_mask: 0.4618  loss_dice: 1.695  loss_bbox: 0.4199  loss_giou: 0.6564  loss_ce_dn: 0.0006033  loss_mask_dn: 0.4411  loss_dice_dn: 1.578  loss_bbox_dn: 0.3343  loss_giou_dn: 0.5008  loss_ce_0: 1.353  loss_mask_0: 0.4937  loss_dice_0: 1.588  loss_bbox_0: 0.7978  loss_giou_0: 1.03  loss_ce_dn_0: 0.03824  loss_mask_dn_0: 0.8494  loss_dice_dn_0: 3.211  loss_bbox_dn_0: 0.8174  loss_giou_dn_0: 0.8464  loss_ce_1: 1.043  loss_mask_1: 0.4452  loss_dice_1: 1.755  loss_bbox_1: 0.423  loss_giou_1: 0.7363  loss_ce_dn_1: 0.0007923  loss_mask_dn_1: 0.4707  loss_dice_dn_1: 1.645  loss_bbox_dn_1: 0.372  loss_giou_dn_1: 0.5734  loss_ce_2: 0.9673  loss_mask_2: 0.4811  loss_dice_2: 1.629  loss_bbox_2: 0.4236  loss_giou_2: 0.6728  loss_ce_dn_2: 0.0009603  loss_mask_dn_2: 0.4735  loss_dice_dn_2: 1.651  loss_bbox_dn_2: 0.3566  loss_giou_dn_2: 0.5362  loss_ce_3: 0.7552  loss_mask_3: 0.4744  loss_dice_3: 1.661  loss_bbox_3: 0.4108  loss_giou_3: 0.665  loss_ce_dn_3: 0.0007304  loss_mask_dn_3: 0.4733  loss_dice_dn_3: 1.645  loss_bbox_dn_3: 0.349  loss_giou_dn_3: 0.5282  loss_ce_4: 0.6971  loss_mask_4: 0.4556  loss_dice_4: 1.671  loss_bbox_4: 0.4261  loss_giou_4: 0.6658  loss_ce_dn_4: 0.0006865  loss_mask_dn_4: 0.4761  loss_dice_dn_4: 1.596  loss_bbox_dn_4: 0.3457  loss_giou_dn_4: 0.5058  loss_ce_5: 0.7235  loss_mask_5: 0.4625  loss_dice_5: 1.669  loss_bbox_5: 0.4156  loss_giou_5: 0.6578  loss_ce_dn_5: 0.000707  loss_mask_dn_5: 0.452  loss_dice_dn_5: 1.595  loss_bbox_dn_5: 0.3449  loss_giou_dn_5: 0.4999  loss_ce_6: 0.7954  loss_mask_6: 0.4567  loss_dice_6: 1.719  loss_bbox_6: 0.4262  loss_giou_6: 0.6548  loss_ce_dn_6: 0.0006469  loss_mask_dn_6: 0.4395  loss_dice_dn_6: 1.587  loss_bbox_dn_6: 0.3337  loss_giou_dn_6: 0.4997  loss_ce_7: 0.7605  loss_mask_7: 0.4454  loss_dice_7: 1.741  loss_bbox_7: 0.4183  loss_giou_7: 0.6428  loss_ce_dn_7: 0.0005006  loss_mask_dn_7: 0.4415  loss_dice_dn_7: 1.581  loss_bbox_dn_7: 0.3329  loss_giou_dn_7: 0.5009  loss_ce_8: 0.7398  loss_mask_8: 0.4553  loss_dice_8: 1.616  loss_bbox_8: 0.4207  loss_giou_8: 0.6541  loss_ce_dn_8: 0.0005333  loss_mask_dn_8: 0.4421  loss_dice_dn_8: 1.577  loss_bbox_dn_8: 0.331  loss_giou_dn_8: 0.4999  loss_ce_interm: 1.379  loss_mask_interm: 0.4768  loss_dice_interm: 1.68  loss_bbox_interm: 0.5277  loss_giou_interm: 0.8188    time: 0.4931  last_time: 0.4439  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:12:59 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:12:59 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:12:59 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:12:59 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:13:00 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0755 s/iter. Eval: 0.0092 s/iter. Total: 0.0854 s/iter. ETA=0:00:04\n",
      "[03/14 17:13:05 d2.evaluation.evaluator]: Total inference time: 0:00:05.525756 (0.089125 s / iter per device, on 1 devices)\n",
      "[03/14 17:13:05 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076859 s / iter per device, on 1 devices)\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.664\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.282\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.305\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.469\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.271\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.539\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.506\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.657\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 32.666 | 66.445 | 28.216 | 30.511 | 46.947 |  nan  |\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:13:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.168\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.570\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.276\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 16.776 | 57.042 | 2.377  | 12.377 | 30.048 |  nan  |\n",
      "[03/14 17:13:05 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:13:05 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: 32.6659,66.4451,28.2164,30.5112,46.9468,nan\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:13:05 d2.evaluation.testing]: copypaste: 16.7760,57.0418,2.3768,12.3774,30.0476,nan\n",
      "[03/14 17:13:05 d2.utils.events]:  eta: 0:22:34  iter: 1199  total_loss: 85.38  loss_ce: 0.6984  loss_mask: 0.3184  loss_dice: 1.805  loss_bbox: 0.3724  loss_giou: 0.8914  loss_ce_dn: 0.0007732  loss_mask_dn: 0.3239  loss_dice_dn: 1.8  loss_bbox_dn: 0.2561  loss_giou_dn: 0.5571  loss_ce_0: 0.9934  loss_mask_0: 0.3403  loss_dice_0: 1.845  loss_bbox_0: 0.7427  loss_giou_0: 1.264  loss_ce_dn_0: 0.05336  loss_mask_dn_0: 0.3985  loss_dice_dn_0: 3.414  loss_bbox_dn_0: 0.6537  loss_giou_dn_0: 0.8491  loss_ce_1: 0.9669  loss_mask_1: 0.3188  loss_dice_1: 1.908  loss_bbox_1: 0.4927  loss_giou_1: 0.9881  loss_ce_dn_1: 0.000899  loss_mask_dn_1: 0.3107  loss_dice_dn_1: 1.831  loss_bbox_dn_1: 0.3374  loss_giou_dn_1: 0.6226  loss_ce_2: 0.8871  loss_mask_2: 0.3405  loss_dice_2: 1.851  loss_bbox_2: 0.4584  loss_giou_2: 0.8569  loss_ce_dn_2: 0.001243  loss_mask_dn_2: 0.3095  loss_dice_dn_2: 1.816  loss_bbox_dn_2: 0.3018  loss_giou_dn_2: 0.581  loss_ce_3: 0.6557  loss_mask_3: 0.3293  loss_dice_3: 2.032  loss_bbox_3: 0.4262  loss_giou_3: 0.8569  loss_ce_dn_3: 0.0009229  loss_mask_dn_3: 0.3028  loss_dice_dn_3: 1.813  loss_bbox_dn_3: 0.2927  loss_giou_dn_3: 0.5743  loss_ce_4: 0.7508  loss_mask_4: 0.3284  loss_dice_4: 1.868  loss_bbox_4: 0.4213  loss_giou_4: 0.8641  loss_ce_dn_4: 0.001012  loss_mask_dn_4: 0.3146  loss_dice_dn_4: 1.833  loss_bbox_dn_4: 0.2796  loss_giou_dn_4: 0.5678  loss_ce_5: 0.7374  loss_mask_5: 0.3499  loss_dice_5: 1.891  loss_bbox_5: 0.3889  loss_giou_5: 0.8831  loss_ce_dn_5: 0.0008334  loss_mask_dn_5: 0.3161  loss_dice_dn_5: 1.845  loss_bbox_dn_5: 0.2685  loss_giou_dn_5: 0.5605  loss_ce_6: 0.6872  loss_mask_6: 0.3524  loss_dice_6: 1.83  loss_bbox_6: 0.3952  loss_giou_6: 0.8667  loss_ce_dn_6: 0.0008558  loss_mask_dn_6: 0.3148  loss_dice_dn_6: 1.817  loss_bbox_dn_6: 0.2629  loss_giou_dn_6: 0.5478  loss_ce_7: 0.7867  loss_mask_7: 0.3268  loss_dice_7: 1.755  loss_bbox_7: 0.3944  loss_giou_7: 0.8704  loss_ce_dn_7: 0.0005723  loss_mask_dn_7: 0.3216  loss_dice_dn_7: 1.806  loss_bbox_dn_7: 0.2561  loss_giou_dn_7: 0.5474  loss_ce_8: 0.7208  loss_mask_8: 0.345  loss_dice_8: 1.762  loss_bbox_8: 0.3686  loss_giou_8: 0.8825  loss_ce_dn_8: 0.0005884  loss_mask_dn_8: 0.318  loss_dice_dn_8: 1.803  loss_bbox_dn_8: 0.2574  loss_giou_dn_8: 0.552  loss_ce_interm: 1.133  loss_mask_interm: 0.3256  loss_dice_interm: 2.04  loss_bbox_interm: 0.3963  loss_giou_interm: 0.8536    time: 0.4928  last_time: 0.4726  data_time: 0.0036  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:13:15 d2.utils.events]:  eta: 0:22:24  iter: 1219  total_loss: 72.35  loss_ce: 0.356  loss_mask: 0.4228  loss_dice: 1.44  loss_bbox: 0.4761  loss_giou: 0.7244  loss_ce_dn: 0.0006944  loss_mask_dn: 0.4421  loss_dice_dn: 1.348  loss_bbox_dn: 0.3745  loss_giou_dn: 0.5181  loss_ce_0: 0.9643  loss_mask_0: 0.4919  loss_dice_0: 1.361  loss_bbox_0: 0.672  loss_giou_0: 0.8992  loss_ce_dn_0: 0.03638  loss_mask_dn_0: 0.6236  loss_dice_dn_0: 2.601  loss_bbox_dn_0: 0.8823  loss_giou_dn_0: 0.8582  loss_ce_1: 0.8611  loss_mask_1: 0.4351  loss_dice_1: 1.385  loss_bbox_1: 0.5173  loss_giou_1: 0.7783  loss_ce_dn_1: 0.001167  loss_mask_dn_1: 0.4557  loss_dice_dn_1: 1.358  loss_bbox_dn_1: 0.4059  loss_giou_dn_1: 0.5947  loss_ce_2: 0.713  loss_mask_2: 0.4352  loss_dice_2: 1.396  loss_bbox_2: 0.473  loss_giou_2: 0.6742  loss_ce_dn_2: 0.00189  loss_mask_dn_2: 0.4526  loss_dice_dn_2: 1.362  loss_bbox_dn_2: 0.4  loss_giou_dn_2: 0.5492  loss_ce_3: 0.4699  loss_mask_3: 0.4122  loss_dice_3: 1.372  loss_bbox_3: 0.5323  loss_giou_3: 0.7202  loss_ce_dn_3: 0.001144  loss_mask_dn_3: 0.4502  loss_dice_dn_3: 1.354  loss_bbox_dn_3: 0.3779  loss_giou_dn_3: 0.5378  loss_ce_4: 0.4494  loss_mask_4: 0.3941  loss_dice_4: 1.311  loss_bbox_4: 0.4847  loss_giou_4: 0.726  loss_ce_dn_4: 0.0008673  loss_mask_dn_4: 0.4347  loss_dice_dn_4: 1.328  loss_bbox_dn_4: 0.3719  loss_giou_dn_4: 0.5314  loss_ce_5: 0.3501  loss_mask_5: 0.4265  loss_dice_5: 1.377  loss_bbox_5: 0.4856  loss_giou_5: 0.7299  loss_ce_dn_5: 0.0008837  loss_mask_dn_5: 0.4292  loss_dice_dn_5: 1.318  loss_bbox_dn_5: 0.3706  loss_giou_dn_5: 0.5164  loss_ce_6: 0.3461  loss_mask_6: 0.4267  loss_dice_6: 1.406  loss_bbox_6: 0.4757  loss_giou_6: 0.7102  loss_ce_dn_6: 0.001222  loss_mask_dn_6: 0.43  loss_dice_dn_6: 1.325  loss_bbox_dn_6: 0.3714  loss_giou_dn_6: 0.5149  loss_ce_7: 0.3541  loss_mask_7: 0.4177  loss_dice_7: 1.418  loss_bbox_7: 0.4833  loss_giou_7: 0.7327  loss_ce_dn_7: 0.0007486  loss_mask_dn_7: 0.4397  loss_dice_dn_7: 1.337  loss_bbox_dn_7: 0.3748  loss_giou_dn_7: 0.5154  loss_ce_8: 0.3646  loss_mask_8: 0.4741  loss_dice_8: 1.426  loss_bbox_8: 0.4844  loss_giou_8: 0.7318  loss_ce_dn_8: 0.0006744  loss_mask_dn_8: 0.4405  loss_dice_dn_8: 1.328  loss_bbox_dn_8: 0.3719  loss_giou_dn_8: 0.5137  loss_ce_interm: 1.021  loss_mask_interm: 0.4801  loss_dice_interm: 1.388  loss_bbox_interm: 0.5622  loss_giou_interm: 0.7784    time: 0.4927  last_time: 0.4203  data_time: 0.0036  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:13:25 d2.utils.events]:  eta: 0:22:13  iter: 1239  total_loss: 68.6  loss_ce: 0.4596  loss_mask: 0.5314  loss_dice: 1.35  loss_bbox: 0.4436  loss_giou: 0.5822  loss_ce_dn: 0.0003031  loss_mask_dn: 0.5456  loss_dice_dn: 1.316  loss_bbox_dn: 0.3349  loss_giou_dn: 0.4522  loss_ce_0: 1.074  loss_mask_0: 0.5269  loss_dice_0: 1.299  loss_bbox_0: 0.7179  loss_giou_0: 0.984  loss_ce_dn_0: 0.08863  loss_mask_dn_0: 0.9015  loss_dice_dn_0: 3.036  loss_bbox_dn_0: 0.8587  loss_giou_dn_0: 0.8512  loss_ce_1: 0.9592  loss_mask_1: 0.4998  loss_dice_1: 1.422  loss_bbox_1: 0.474  loss_giou_1: 0.6318  loss_ce_dn_1: 0.001917  loss_mask_dn_1: 0.5621  loss_dice_dn_1: 1.296  loss_bbox_dn_1: 0.3646  loss_giou_dn_1: 0.4895  loss_ce_2: 0.7423  loss_mask_2: 0.5006  loss_dice_2: 1.379  loss_bbox_2: 0.4242  loss_giou_2: 0.5879  loss_ce_dn_2: 0.002423  loss_mask_dn_2: 0.5245  loss_dice_dn_2: 1.308  loss_bbox_dn_2: 0.3431  loss_giou_dn_2: 0.4573  loss_ce_3: 0.5722  loss_mask_3: 0.5222  loss_dice_3: 1.383  loss_bbox_3: 0.4427  loss_giou_3: 0.6226  loss_ce_dn_3: 0.001082  loss_mask_dn_3: 0.5342  loss_dice_dn_3: 1.313  loss_bbox_dn_3: 0.3546  loss_giou_dn_3: 0.4676  loss_ce_4: 0.4194  loss_mask_4: 0.5435  loss_dice_4: 1.362  loss_bbox_4: 0.4537  loss_giou_4: 0.6267  loss_ce_dn_4: 0.0007654  loss_mask_dn_4: 0.5494  loss_dice_dn_4: 1.281  loss_bbox_dn_4: 0.3356  loss_giou_dn_4: 0.458  loss_ce_5: 0.4564  loss_mask_5: 0.5332  loss_dice_5: 1.367  loss_bbox_5: 0.4615  loss_giou_5: 0.5993  loss_ce_dn_5: 0.0008801  loss_mask_dn_5: 0.525  loss_dice_dn_5: 1.303  loss_bbox_dn_5: 0.3423  loss_giou_dn_5: 0.449  loss_ce_6: 0.4362  loss_mask_6: 0.5472  loss_dice_6: 1.382  loss_bbox_6: 0.4768  loss_giou_6: 0.6021  loss_ce_dn_6: 0.0006887  loss_mask_dn_6: 0.5394  loss_dice_dn_6: 1.302  loss_bbox_dn_6: 0.339  loss_giou_dn_6: 0.4494  loss_ce_7: 0.4443  loss_mask_7: 0.5414  loss_dice_7: 1.444  loss_bbox_7: 0.4457  loss_giou_7: 0.6127  loss_ce_dn_7: 0.0003768  loss_mask_dn_7: 0.5247  loss_dice_dn_7: 1.299  loss_bbox_dn_7: 0.3363  loss_giou_dn_7: 0.4477  loss_ce_8: 0.4478  loss_mask_8: 0.5426  loss_dice_8: 1.374  loss_bbox_8: 0.4221  loss_giou_8: 0.59  loss_ce_dn_8: 0.0003139  loss_mask_dn_8: 0.5467  loss_dice_dn_8: 1.309  loss_bbox_dn_8: 0.3321  loss_giou_dn_8: 0.4506  loss_ce_interm: 1.004  loss_mask_interm: 0.5288  loss_dice_interm: 1.4  loss_bbox_interm: 0.4947  loss_giou_interm: 0.6696    time: 0.4923  last_time: 0.4634  data_time: 0.0035  last_data_time: 0.0028   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:13:34 d2.utils.events]:  eta: 0:22:05  iter: 1259  total_loss: 83.4  loss_ce: 0.4498  loss_mask: 0.5775  loss_dice: 1.603  loss_bbox: 0.482  loss_giou: 0.7085  loss_ce_dn: 0.0005163  loss_mask_dn: 0.5451  loss_dice_dn: 1.473  loss_bbox_dn: 0.3472  loss_giou_dn: 0.5338  loss_ce_0: 1.015  loss_mask_0: 0.5885  loss_dice_0: 1.514  loss_bbox_0: 0.864  loss_giou_0: 0.9429  loss_ce_dn_0: 0.08732  loss_mask_dn_0: 0.782  loss_dice_dn_0: 2.817  loss_bbox_dn_0: 0.8417  loss_giou_dn_0: 0.8501  loss_ce_1: 0.7978  loss_mask_1: 0.5865  loss_dice_1: 1.58  loss_bbox_1: 0.4953  loss_giou_1: 0.7203  loss_ce_dn_1: 0.001425  loss_mask_dn_1: 0.6119  loss_dice_dn_1: 1.525  loss_bbox_dn_1: 0.461  loss_giou_dn_1: 0.6055  loss_ce_2: 0.6661  loss_mask_2: 0.5604  loss_dice_2: 1.579  loss_bbox_2: 0.481  loss_giou_2: 0.7138  loss_ce_dn_2: 0.001217  loss_mask_dn_2: 0.5736  loss_dice_dn_2: 1.498  loss_bbox_dn_2: 0.3911  loss_giou_dn_2: 0.5443  loss_ce_3: 0.528  loss_mask_3: 0.5808  loss_dice_3: 1.598  loss_bbox_3: 0.5126  loss_giou_3: 0.7017  loss_ce_dn_3: 0.0008541  loss_mask_dn_3: 0.5621  loss_dice_dn_3: 1.489  loss_bbox_dn_3: 0.3607  loss_giou_dn_3: 0.5285  loss_ce_4: 0.49  loss_mask_4: 0.5563  loss_dice_4: 1.561  loss_bbox_4: 0.5167  loss_giou_4: 0.7155  loss_ce_dn_4: 0.0006891  loss_mask_dn_4: 0.5444  loss_dice_dn_4: 1.494  loss_bbox_dn_4: 0.3518  loss_giou_dn_4: 0.5253  loss_ce_5: 0.4539  loss_mask_5: 0.5793  loss_dice_5: 1.615  loss_bbox_5: 0.5006  loss_giou_5: 0.7032  loss_ce_dn_5: 0.0009505  loss_mask_dn_5: 0.5539  loss_dice_dn_5: 1.479  loss_bbox_dn_5: 0.3604  loss_giou_dn_5: 0.5306  loss_ce_6: 0.4236  loss_mask_6: 0.5644  loss_dice_6: 1.559  loss_bbox_6: 0.5177  loss_giou_6: 0.7173  loss_ce_dn_6: 0.0008939  loss_mask_dn_6: 0.5499  loss_dice_dn_6: 1.469  loss_bbox_dn_6: 0.3547  loss_giou_dn_6: 0.5373  loss_ce_7: 0.4448  loss_mask_7: 0.5762  loss_dice_7: 1.568  loss_bbox_7: 0.5178  loss_giou_7: 0.7306  loss_ce_dn_7: 0.0004774  loss_mask_dn_7: 0.5525  loss_dice_dn_7: 1.478  loss_bbox_dn_7: 0.3527  loss_giou_dn_7: 0.5341  loss_ce_8: 0.4366  loss_mask_8: 0.577  loss_dice_8: 1.581  loss_bbox_8: 0.4734  loss_giou_8: 0.7106  loss_ce_dn_8: 0.0004173  loss_mask_dn_8: 0.5476  loss_dice_dn_8: 1.484  loss_bbox_dn_8: 0.3492  loss_giou_dn_8: 0.5342  loss_ce_interm: 1.147  loss_mask_interm: 0.5766  loss_dice_interm: 1.542  loss_bbox_interm: 0.5244  loss_giou_interm: 0.7406    time: 0.4922  last_time: 0.5220  data_time: 0.0036  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:13:44 d2.utils.events]:  eta: 0:21:54  iter: 1279  total_loss: 78.4  loss_ce: 0.5975  loss_mask: 0.5528  loss_dice: 1.38  loss_bbox: 0.4375  loss_giou: 0.5994  loss_ce_dn: 0.0004571  loss_mask_dn: 0.5228  loss_dice_dn: 1.254  loss_bbox_dn: 0.3829  loss_giou_dn: 0.5155  loss_ce_0: 1.003  loss_mask_0: 0.5142  loss_dice_0: 1.415  loss_bbox_0: 0.7097  loss_giou_0: 0.8699  loss_ce_dn_0: 0.03446  loss_mask_dn_0: 0.8727  loss_dice_dn_0: 2.978  loss_bbox_dn_0: 1.022  loss_giou_dn_0: 0.8496  loss_ce_1: 1.037  loss_mask_1: 0.5458  loss_dice_1: 1.323  loss_bbox_1: 0.5006  loss_giou_1: 0.7294  loss_ce_dn_1: 0.002119  loss_mask_dn_1: 0.5282  loss_dice_dn_1: 1.312  loss_bbox_dn_1: 0.4641  loss_giou_dn_1: 0.5478  loss_ce_2: 0.6694  loss_mask_2: 0.5438  loss_dice_2: 1.291  loss_bbox_2: 0.4998  loss_giou_2: 0.6772  loss_ce_dn_2: 0.002286  loss_mask_dn_2: 0.5218  loss_dice_dn_2: 1.307  loss_bbox_dn_2: 0.432  loss_giou_dn_2: 0.509  loss_ce_3: 0.7289  loss_mask_3: 0.5393  loss_dice_3: 1.368  loss_bbox_3: 0.51  loss_giou_3: 0.7268  loss_ce_dn_3: 0.002064  loss_mask_dn_3: 0.5257  loss_dice_dn_3: 1.284  loss_bbox_dn_3: 0.4359  loss_giou_dn_3: 0.5107  loss_ce_4: 0.553  loss_mask_4: 0.5447  loss_dice_4: 1.363  loss_bbox_4: 0.4253  loss_giou_4: 0.5945  loss_ce_dn_4: 0.0007308  loss_mask_dn_4: 0.5277  loss_dice_dn_4: 1.264  loss_bbox_dn_4: 0.4267  loss_giou_dn_4: 0.4987  loss_ce_5: 0.6196  loss_mask_5: 0.5039  loss_dice_5: 1.325  loss_bbox_5: 0.3959  loss_giou_5: 0.5653  loss_ce_dn_5: 0.001093  loss_mask_dn_5: 0.5162  loss_dice_dn_5: 1.263  loss_bbox_dn_5: 0.4041  loss_giou_dn_5: 0.494  loss_ce_6: 0.6981  loss_mask_6: 0.5364  loss_dice_6: 1.458  loss_bbox_6: 0.4569  loss_giou_6: 0.5852  loss_ce_dn_6: 0.0009397  loss_mask_dn_6: 0.5292  loss_dice_dn_6: 1.24  loss_bbox_dn_6: 0.3986  loss_giou_dn_6: 0.5048  loss_ce_7: 0.6329  loss_mask_7: 0.5578  loss_dice_7: 1.449  loss_bbox_7: 0.4186  loss_giou_7: 0.5798  loss_ce_dn_7: 0.0005544  loss_mask_dn_7: 0.5215  loss_dice_dn_7: 1.251  loss_bbox_dn_7: 0.3888  loss_giou_dn_7: 0.5107  loss_ce_8: 0.6428  loss_mask_8: 0.5362  loss_dice_8: 1.431  loss_bbox_8: 0.4286  loss_giou_8: 0.5939  loss_ce_dn_8: 0.0004201  loss_mask_dn_8: 0.5212  loss_dice_dn_8: 1.245  loss_bbox_dn_8: 0.3869  loss_giou_dn_8: 0.5133  loss_ce_interm: 1.064  loss_mask_interm: 0.5053  loss_dice_interm: 1.349  loss_bbox_interm: 0.5824  loss_giou_interm: 0.734    time: 0.4921  last_time: 0.4489  data_time: 0.0037  last_data_time: 0.0030   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:13:55 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:13:55 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:13:55 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:13:55 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:13:56 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0007 s/iter. Inference: 0.0716 s/iter. Eval: 0.0095 s/iter. Total: 0.0818 s/iter. ETA=0:00:04\n",
      "[03/14 17:14:01 d2.evaluation.evaluator]: Total inference time: 0:00:05.479669 (0.088382 s / iter per device, on 1 devices)\n",
      "[03/14 17:14:01 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076515 s / iter per device, on 1 devices)\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.402\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.757\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.441\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.497\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.512\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.590\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.570\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.662\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 40.221 | 75.739 | 44.126 | 39.972 | 49.746 |  nan  |\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:14:01 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.197\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.674\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.173\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.294\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 19.716 | 67.414 | 1.390  | 17.262 | 28.390 |  nan  |\n",
      "[03/14 17:14:01 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:14:01 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: 40.2206,75.7392,44.1258,39.9718,49.7463,nan\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:14:01 d2.evaluation.testing]: copypaste: 19.7158,67.4138,1.3896,17.2622,28.3903,nan\n",
      "[03/14 17:14:01 d2.utils.events]:  eta: 0:21:45  iter: 1299  total_loss: 76.93  loss_ce: 0.6841  loss_mask: 0.5174  loss_dice: 1.57  loss_bbox: 0.4627  loss_giou: 0.7273  loss_ce_dn: 0.0008659  loss_mask_dn: 0.497  loss_dice_dn: 1.486  loss_bbox_dn: 0.3264  loss_giou_dn: 0.518  loss_ce_0: 1.037  loss_mask_0: 0.5222  loss_dice_0: 1.644  loss_bbox_0: 0.7172  loss_giou_0: 1.005  loss_ce_dn_0: 0.04913  loss_mask_dn_0: 0.7808  loss_dice_dn_0: 3.235  loss_bbox_dn_0: 0.8972  loss_giou_dn_0: 0.8576  loss_ce_1: 1.027  loss_mask_1: 0.5091  loss_dice_1: 1.413  loss_bbox_1: 0.4427  loss_giou_1: 0.6699  loss_ce_dn_1: 0.001895  loss_mask_dn_1: 0.4923  loss_dice_dn_1: 1.469  loss_bbox_dn_1: 0.4278  loss_giou_dn_1: 0.598  loss_ce_2: 0.8034  loss_mask_2: 0.5009  loss_dice_2: 1.543  loss_bbox_2: 0.4677  loss_giou_2: 0.7526  loss_ce_dn_2: 0.002002  loss_mask_dn_2: 0.5031  loss_dice_dn_2: 1.469  loss_bbox_dn_2: 0.3695  loss_giou_dn_2: 0.5628  loss_ce_3: 0.7617  loss_mask_3: 0.5151  loss_dice_3: 1.367  loss_bbox_3: 0.4584  loss_giou_3: 0.7954  loss_ce_dn_3: 0.001159  loss_mask_dn_3: 0.5038  loss_dice_dn_3: 1.491  loss_bbox_dn_3: 0.3588  loss_giou_dn_3: 0.5569  loss_ce_4: 0.7047  loss_mask_4: 0.5115  loss_dice_4: 1.487  loss_bbox_4: 0.5029  loss_giou_4: 0.7133  loss_ce_dn_4: 0.0008931  loss_mask_dn_4: 0.5098  loss_dice_dn_4: 1.487  loss_bbox_dn_4: 0.3389  loss_giou_dn_4: 0.5401  loss_ce_5: 0.7385  loss_mask_5: 0.5053  loss_dice_5: 1.554  loss_bbox_5: 0.5147  loss_giou_5: 0.6716  loss_ce_dn_5: 0.00112  loss_mask_dn_5: 0.4993  loss_dice_dn_5: 1.497  loss_bbox_dn_5: 0.3354  loss_giou_dn_5: 0.5235  loss_ce_6: 0.6821  loss_mask_6: 0.5254  loss_dice_6: 1.513  loss_bbox_6: 0.5257  loss_giou_6: 0.6853  loss_ce_dn_6: 0.0009792  loss_mask_dn_6: 0.4946  loss_dice_dn_6: 1.481  loss_bbox_dn_6: 0.3364  loss_giou_dn_6: 0.5273  loss_ce_7: 0.6719  loss_mask_7: 0.5077  loss_dice_7: 1.528  loss_bbox_7: 0.5223  loss_giou_7: 0.7267  loss_ce_dn_7: 0.0008635  loss_mask_dn_7: 0.495  loss_dice_dn_7: 1.496  loss_bbox_dn_7: 0.3329  loss_giou_dn_7: 0.5206  loss_ce_8: 0.6518  loss_mask_8: 0.5319  loss_dice_8: 1.527  loss_bbox_8: 0.4724  loss_giou_8: 0.725  loss_ce_dn_8: 0.0007477  loss_mask_dn_8: 0.498  loss_dice_dn_8: 1.486  loss_bbox_dn_8: 0.3239  loss_giou_dn_8: 0.5198  loss_ce_interm: 1.162  loss_mask_interm: 0.5153  loss_dice_interm: 1.551  loss_bbox_interm: 0.5297  loss_giou_interm: 0.789    time: 0.4922  last_time: 0.4829  data_time: 0.0036  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:14:11 d2.utils.events]:  eta: 0:21:34  iter: 1319  total_loss: 78.07  loss_ce: 0.6283  loss_mask: 0.4915  loss_dice: 1.374  loss_bbox: 0.408  loss_giou: 0.4835  loss_ce_dn: 0.001311  loss_mask_dn: 0.458  loss_dice_dn: 1.393  loss_bbox_dn: 0.3638  loss_giou_dn: 0.4676  loss_ce_0: 1  loss_mask_0: 0.4406  loss_dice_0: 1.368  loss_bbox_0: 0.7544  loss_giou_0: 0.9547  loss_ce_dn_0: 0.03298  loss_mask_dn_0: 0.7998  loss_dice_dn_0: 2.898  loss_bbox_dn_0: 0.932  loss_giou_dn_0: 0.8574  loss_ce_1: 1.042  loss_mask_1: 0.4761  loss_dice_1: 1.412  loss_bbox_1: 0.3619  loss_giou_1: 0.5674  loss_ce_dn_1: 0.001995  loss_mask_dn_1: 0.4918  loss_dice_dn_1: 1.376  loss_bbox_dn_1: 0.4559  loss_giou_dn_1: 0.5687  loss_ce_2: 0.8601  loss_mask_2: 0.4747  loss_dice_2: 1.396  loss_bbox_2: 0.3422  loss_giou_2: 0.5697  loss_ce_dn_2: 0.002102  loss_mask_dn_2: 0.4689  loss_dice_dn_2: 1.356  loss_bbox_dn_2: 0.395  loss_giou_dn_2: 0.5149  loss_ce_3: 0.6693  loss_mask_3: 0.4562  loss_dice_3: 1.399  loss_bbox_3: 0.4094  loss_giou_3: 0.5669  loss_ce_dn_3: 0.001213  loss_mask_dn_3: 0.4684  loss_dice_dn_3: 1.371  loss_bbox_dn_3: 0.3918  loss_giou_dn_3: 0.4856  loss_ce_4: 0.8214  loss_mask_4: 0.4701  loss_dice_4: 1.438  loss_bbox_4: 0.3695  loss_giou_4: 0.525  loss_ce_dn_4: 0.001205  loss_mask_dn_4: 0.4629  loss_dice_dn_4: 1.363  loss_bbox_dn_4: 0.3606  loss_giou_dn_4: 0.473  loss_ce_5: 0.7346  loss_mask_5: 0.4526  loss_dice_5: 1.374  loss_bbox_5: 0.3711  loss_giou_5: 0.4917  loss_ce_dn_5: 0.001704  loss_mask_dn_5: 0.4606  loss_dice_dn_5: 1.375  loss_bbox_dn_5: 0.3917  loss_giou_dn_5: 0.4754  loss_ce_6: 0.783  loss_mask_6: 0.4561  loss_dice_6: 1.367  loss_bbox_6: 0.3906  loss_giou_6: 0.4941  loss_ce_dn_6: 0.001862  loss_mask_dn_6: 0.453  loss_dice_dn_6: 1.367  loss_bbox_dn_6: 0.3836  loss_giou_dn_6: 0.4768  loss_ce_7: 0.6451  loss_mask_7: 0.4603  loss_dice_7: 1.378  loss_bbox_7: 0.3953  loss_giou_7: 0.4897  loss_ce_dn_7: 0.001376  loss_mask_dn_7: 0.4516  loss_dice_dn_7: 1.378  loss_bbox_dn_7: 0.3686  loss_giou_dn_7: 0.4715  loss_ce_8: 0.642  loss_mask_8: 0.4835  loss_dice_8: 1.321  loss_bbox_8: 0.4119  loss_giou_8: 0.492  loss_ce_dn_8: 0.001195  loss_mask_dn_8: 0.4536  loss_dice_dn_8: 1.393  loss_bbox_dn_8: 0.3671  loss_giou_dn_8: 0.4696  loss_ce_interm: 1.035  loss_mask_interm: 0.4728  loss_dice_interm: 1.317  loss_bbox_interm: 0.5381  loss_giou_interm: 0.6912    time: 0.4922  last_time: 0.4403  data_time: 0.0035  last_data_time: 0.0029   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:14:21 d2.utils.events]:  eta: 0:21:23  iter: 1339  total_loss: 82.72  loss_ce: 0.7662  loss_mask: 0.4176  loss_dice: 1.748  loss_bbox: 0.3807  loss_giou: 0.6015  loss_ce_dn: 0.001242  loss_mask_dn: 0.4096  loss_dice_dn: 1.757  loss_bbox_dn: 0.3124  loss_giou_dn: 0.5199  loss_ce_0: 0.9507  loss_mask_0: 0.4221  loss_dice_0: 1.736  loss_bbox_0: 0.8312  loss_giou_0: 1.08  loss_ce_dn_0: 0.08364  loss_mask_dn_0: 0.6979  loss_dice_dn_0: 3.128  loss_bbox_dn_0: 0.6988  loss_giou_dn_0: 0.8494  loss_ce_1: 0.9856  loss_mask_1: 0.4385  loss_dice_1: 1.719  loss_bbox_1: 0.5089  loss_giou_1: 0.7164  loss_ce_dn_1: 0.001884  loss_mask_dn_1: 0.4467  loss_dice_dn_1: 1.835  loss_bbox_dn_1: 0.3794  loss_giou_dn_1: 0.573  loss_ce_2: 0.9612  loss_mask_2: 0.4341  loss_dice_2: 1.737  loss_bbox_2: 0.4688  loss_giou_2: 0.731  loss_ce_dn_2: 0.001266  loss_mask_dn_2: 0.4331  loss_dice_dn_2: 1.819  loss_bbox_dn_2: 0.3664  loss_giou_dn_2: 0.5621  loss_ce_3: 0.8338  loss_mask_3: 0.4243  loss_dice_3: 1.75  loss_bbox_3: 0.4454  loss_giou_3: 0.6949  loss_ce_dn_3: 0.001177  loss_mask_dn_3: 0.4199  loss_dice_dn_3: 1.738  loss_bbox_dn_3: 0.3471  loss_giou_dn_3: 0.545  loss_ce_4: 0.724  loss_mask_4: 0.4363  loss_dice_4: 1.745  loss_bbox_4: 0.4391  loss_giou_4: 0.6825  loss_ce_dn_4: 0.001069  loss_mask_dn_4: 0.4123  loss_dice_dn_4: 1.712  loss_bbox_dn_4: 0.3355  loss_giou_dn_4: 0.5347  loss_ce_5: 0.8317  loss_mask_5: 0.423  loss_dice_5: 1.713  loss_bbox_5: 0.375  loss_giou_5: 0.6229  loss_ce_dn_5: 0.001276  loss_mask_dn_5: 0.4127  loss_dice_dn_5: 1.695  loss_bbox_dn_5: 0.3124  loss_giou_dn_5: 0.5308  loss_ce_6: 0.8076  loss_mask_6: 0.4288  loss_dice_6: 1.711  loss_bbox_6: 0.3976  loss_giou_6: 0.6146  loss_ce_dn_6: 0.001301  loss_mask_dn_6: 0.4071  loss_dice_dn_6: 1.731  loss_bbox_dn_6: 0.3133  loss_giou_dn_6: 0.5294  loss_ce_7: 0.7274  loss_mask_7: 0.4115  loss_dice_7: 1.813  loss_bbox_7: 0.3643  loss_giou_7: 0.6122  loss_ce_dn_7: 0.001322  loss_mask_dn_7: 0.4076  loss_dice_dn_7: 1.737  loss_bbox_dn_7: 0.3097  loss_giou_dn_7: 0.5278  loss_ce_8: 0.7047  loss_mask_8: 0.4179  loss_dice_8: 1.824  loss_bbox_8: 0.3528  loss_giou_8: 0.6042  loss_ce_dn_8: 0.001236  loss_mask_dn_8: 0.4147  loss_dice_dn_8: 1.752  loss_bbox_dn_8: 0.3136  loss_giou_dn_8: 0.5251  loss_ce_interm: 1  loss_mask_interm: 0.4142  loss_dice_interm: 1.735  loss_bbox_interm: 0.4287  loss_giou_interm: 0.7534    time: 0.4919  last_time: 0.5253  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:14:31 d2.utils.events]:  eta: 0:21:14  iter: 1359  total_loss: 78.81  loss_ce: 0.7679  loss_mask: 0.3728  loss_dice: 1.715  loss_bbox: 0.3259  loss_giou: 0.7804  loss_ce_dn: 0.001791  loss_mask_dn: 0.3829  loss_dice_dn: 1.688  loss_bbox_dn: 0.2855  loss_giou_dn: 0.5741  loss_ce_0: 0.9923  loss_mask_0: 0.4249  loss_dice_0: 1.821  loss_bbox_0: 0.65  loss_giou_0: 1.099  loss_ce_dn_0: 0.08374  loss_mask_dn_0: 0.6324  loss_dice_dn_0: 3.277  loss_bbox_dn_0: 0.6843  loss_giou_dn_0: 0.8539  loss_ce_1: 1.267  loss_mask_1: 0.3947  loss_dice_1: 1.662  loss_bbox_1: 0.3984  loss_giou_1: 0.8368  loss_ce_dn_1: 0.003407  loss_mask_dn_1: 0.3592  loss_dice_dn_1: 1.635  loss_bbox_dn_1: 0.3696  loss_giou_dn_1: 0.6211  loss_ce_2: 1.007  loss_mask_2: 0.3729  loss_dice_2: 1.59  loss_bbox_2: 0.3972  loss_giou_2: 0.7684  loss_ce_dn_2: 0.0008826  loss_mask_dn_2: 0.3781  loss_dice_dn_2: 1.593  loss_bbox_dn_2: 0.3038  loss_giou_dn_2: 0.5901  loss_ce_3: 0.7724  loss_mask_3: 0.3832  loss_dice_3: 1.588  loss_bbox_3: 0.4361  loss_giou_3: 0.7565  loss_ce_dn_3: 0.001253  loss_mask_dn_3: 0.3819  loss_dice_dn_3: 1.613  loss_bbox_dn_3: 0.3029  loss_giou_dn_3: 0.5813  loss_ce_4: 0.6859  loss_mask_4: 0.3958  loss_dice_4: 1.692  loss_bbox_4: 0.3818  loss_giou_4: 0.7544  loss_ce_dn_4: 0.001191  loss_mask_dn_4: 0.3889  loss_dice_dn_4: 1.642  loss_bbox_dn_4: 0.2983  loss_giou_dn_4: 0.5786  loss_ce_5: 0.6839  loss_mask_5: 0.3742  loss_dice_5: 1.644  loss_bbox_5: 0.3628  loss_giou_5: 0.7105  loss_ce_dn_5: 0.001863  loss_mask_dn_5: 0.3728  loss_dice_dn_5: 1.67  loss_bbox_dn_5: 0.302  loss_giou_dn_5: 0.579  loss_ce_6: 0.6571  loss_mask_6: 0.382  loss_dice_6: 1.637  loss_bbox_6: 0.3215  loss_giou_6: 0.7099  loss_ce_dn_6: 0.001824  loss_mask_dn_6: 0.3783  loss_dice_dn_6: 1.648  loss_bbox_dn_6: 0.3031  loss_giou_dn_6: 0.5753  loss_ce_7: 0.6689  loss_mask_7: 0.3702  loss_dice_7: 1.747  loss_bbox_7: 0.3592  loss_giou_7: 0.7836  loss_ce_dn_7: 0.001732  loss_mask_dn_7: 0.3843  loss_dice_dn_7: 1.677  loss_bbox_dn_7: 0.2934  loss_giou_dn_7: 0.5771  loss_ce_8: 0.7168  loss_mask_8: 0.3738  loss_dice_8: 1.694  loss_bbox_8: 0.3734  loss_giou_8: 0.7833  loss_ce_dn_8: 0.001645  loss_mask_dn_8: 0.3805  loss_dice_dn_8: 1.688  loss_bbox_dn_8: 0.2847  loss_giou_dn_8: 0.5735  loss_ce_interm: 1.192  loss_mask_interm: 0.4324  loss_dice_interm: 1.747  loss_bbox_interm: 0.4443  loss_giou_interm: 0.7864    time: 0.4919  last_time: 0.4905  data_time: 0.0035  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:14:40 d2.utils.events]:  eta: 0:21:04  iter: 1379  total_loss: 82.04  loss_ce: 0.5304  loss_mask: 0.5802  loss_dice: 1.388  loss_bbox: 0.4756  loss_giou: 0.6386  loss_ce_dn: 0.001068  loss_mask_dn: 0.531  loss_dice_dn: 1.506  loss_bbox_dn: 0.2814  loss_giou_dn: 0.4899  loss_ce_0: 0.9752  loss_mask_0: 0.6293  loss_dice_0: 1.392  loss_bbox_0: 0.8163  loss_giou_0: 1.112  loss_ce_dn_0: 0.03188  loss_mask_dn_0: 0.8447  loss_dice_dn_0: 3.005  loss_bbox_dn_0: 0.8253  loss_giou_dn_0: 0.8547  loss_ce_1: 1.134  loss_mask_1: 0.4981  loss_dice_1: 1.349  loss_bbox_1: 0.4765  loss_giou_1: 0.6584  loss_ce_dn_1: 0.001698  loss_mask_dn_1: 0.5108  loss_dice_dn_1: 1.573  loss_bbox_dn_1: 0.4235  loss_giou_dn_1: 0.512  loss_ce_2: 0.9107  loss_mask_2: 0.5878  loss_dice_2: 1.357  loss_bbox_2: 0.4074  loss_giou_2: 0.6178  loss_ce_dn_2: 0.001093  loss_mask_dn_2: 0.5334  loss_dice_dn_2: 1.515  loss_bbox_dn_2: 0.3728  loss_giou_dn_2: 0.4832  loss_ce_3: 0.7009  loss_mask_3: 0.5889  loss_dice_3: 1.317  loss_bbox_3: 0.3956  loss_giou_3: 0.6315  loss_ce_dn_3: 0.0009141  loss_mask_dn_3: 0.535  loss_dice_dn_3: 1.588  loss_bbox_dn_3: 0.311  loss_giou_dn_3: 0.4894  loss_ce_4: 0.6624  loss_mask_4: 0.6106  loss_dice_4: 1.366  loss_bbox_4: 0.377  loss_giou_4: 0.6057  loss_ce_dn_4: 0.000737  loss_mask_dn_4: 0.5271  loss_dice_dn_4: 1.541  loss_bbox_dn_4: 0.2923  loss_giou_dn_4: 0.4707  loss_ce_5: 0.567  loss_mask_5: 0.5631  loss_dice_5: 1.393  loss_bbox_5: 0.4016  loss_giou_5: 0.6346  loss_ce_dn_5: 0.00114  loss_mask_dn_5: 0.5262  loss_dice_dn_5: 1.543  loss_bbox_dn_5: 0.2802  loss_giou_dn_5: 0.4705  loss_ce_6: 0.5395  loss_mask_6: 0.5663  loss_dice_6: 1.384  loss_bbox_6: 0.4727  loss_giou_6: 0.6214  loss_ce_dn_6: 0.001143  loss_mask_dn_6: 0.5407  loss_dice_dn_6: 1.536  loss_bbox_dn_6: 0.2866  loss_giou_dn_6: 0.4686  loss_ce_7: 0.5334  loss_mask_7: 0.5686  loss_dice_7: 1.398  loss_bbox_7: 0.4579  loss_giou_7: 0.6308  loss_ce_dn_7: 0.001192  loss_mask_dn_7: 0.5393  loss_dice_dn_7: 1.534  loss_bbox_dn_7: 0.2895  loss_giou_dn_7: 0.4774  loss_ce_8: 0.5387  loss_mask_8: 0.591  loss_dice_8: 1.356  loss_bbox_8: 0.4594  loss_giou_8: 0.6365  loss_ce_dn_8: 0.0009812  loss_mask_dn_8: 0.5347  loss_dice_dn_8: 1.519  loss_bbox_dn_8: 0.2846  loss_giou_dn_8: 0.4848  loss_ce_interm: 1.146  loss_mask_interm: 0.497  loss_dice_interm: 1.364  loss_bbox_interm: 0.5553  loss_giou_interm: 0.7391    time: 0.4918  last_time: 0.4773  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:14:51 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:14:51 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:14:51 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:14:51 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:14:52 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0839 s/iter. Eval: 0.0096 s/iter. Total: 0.0944 s/iter. ETA=0:00:05\n",
      "[03/14 17:14:57 d2.evaluation.evaluator]: Total inference time: 0:00:05.517409 (0.088990 s / iter per device, on 1 devices)\n",
      "[03/14 17:14:57 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076903 s / iter per device, on 1 devices)\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.381\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.557\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.296\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.553\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.710\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 39.743 | 75.063 | 38.105 | 40.678 | 55.695 |  nan  |\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:14:57 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.641\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.146\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.154\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.258\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.257\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 18.202 | 64.133 | 2.118  | 14.597 | 31.405 |  nan  |\n",
      "[03/14 17:14:57 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:14:57 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: 39.7434,75.0630,38.1047,40.6778,55.6948,nan\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:14:57 d2.evaluation.testing]: copypaste: 18.2017,64.1326,2.1182,14.5974,31.4049,nan\n",
      "[03/14 17:14:57 d2.utils.events]:  eta: 0:20:52  iter: 1399  total_loss: 74.29  loss_ce: 0.5732  loss_mask: 0.523  loss_dice: 1.39  loss_bbox: 0.4528  loss_giou: 0.5923  loss_ce_dn: 0.0006756  loss_mask_dn: 0.5186  loss_dice_dn: 1.379  loss_bbox_dn: 0.3765  loss_giou_dn: 0.5097  loss_ce_0: 0.92  loss_mask_0: 0.524  loss_dice_0: 1.499  loss_bbox_0: 0.7185  loss_giou_0: 0.8172  loss_ce_dn_0: 0.03102  loss_mask_dn_0: 0.646  loss_dice_dn_0: 2.585  loss_bbox_dn_0: 0.9098  loss_giou_dn_0: 0.8594  loss_ce_1: 0.8714  loss_mask_1: 0.5154  loss_dice_1: 1.449  loss_bbox_1: 0.4581  loss_giou_1: 0.5692  loss_ce_dn_1: 0.002858  loss_mask_dn_1: 0.5152  loss_dice_dn_1: 1.44  loss_bbox_dn_1: 0.4425  loss_giou_dn_1: 0.533  loss_ce_2: 0.6214  loss_mask_2: 0.5343  loss_dice_2: 1.362  loss_bbox_2: 0.4624  loss_giou_2: 0.578  loss_ce_dn_2: 0.001203  loss_mask_dn_2: 0.5001  loss_dice_dn_2: 1.415  loss_bbox_dn_2: 0.398  loss_giou_dn_2: 0.487  loss_ce_3: 0.4205  loss_mask_3: 0.5311  loss_dice_3: 1.395  loss_bbox_3: 0.4788  loss_giou_3: 0.5843  loss_ce_dn_3: 0.0008  loss_mask_dn_3: 0.519  loss_dice_dn_3: 1.401  loss_bbox_dn_3: 0.382  loss_giou_dn_3: 0.5045  loss_ce_4: 0.4032  loss_mask_4: 0.5253  loss_dice_4: 1.433  loss_bbox_4: 0.4985  loss_giou_4: 0.5946  loss_ce_dn_4: 0.000659  loss_mask_dn_4: 0.5292  loss_dice_dn_4: 1.414  loss_bbox_dn_4: 0.3816  loss_giou_dn_4: 0.5139  loss_ce_5: 0.4229  loss_mask_5: 0.5093  loss_dice_5: 1.406  loss_bbox_5: 0.4618  loss_giou_5: 0.592  loss_ce_dn_5: 0.0007767  loss_mask_dn_5: 0.5263  loss_dice_dn_5: 1.429  loss_bbox_dn_5: 0.3824  loss_giou_dn_5: 0.5105  loss_ce_6: 0.4669  loss_mask_6: 0.5242  loss_dice_6: 1.439  loss_bbox_6: 0.4507  loss_giou_6: 0.6075  loss_ce_dn_6: 0.0008593  loss_mask_dn_6: 0.5247  loss_dice_dn_6: 1.39  loss_bbox_dn_6: 0.3823  loss_giou_dn_6: 0.5081  loss_ce_7: 0.5578  loss_mask_7: 0.5151  loss_dice_7: 1.414  loss_bbox_7: 0.4571  loss_giou_7: 0.5948  loss_ce_dn_7: 0.0008227  loss_mask_dn_7: 0.5283  loss_dice_dn_7: 1.376  loss_bbox_dn_7: 0.3814  loss_giou_dn_7: 0.5088  loss_ce_8: 0.5698  loss_mask_8: 0.5328  loss_dice_8: 1.408  loss_bbox_8: 0.4547  loss_giou_8: 0.5909  loss_ce_dn_8: 0.00074  loss_mask_dn_8: 0.5272  loss_dice_dn_8: 1.368  loss_bbox_dn_8: 0.3803  loss_giou_dn_8: 0.5089  loss_ce_interm: 0.9899  loss_mask_interm: 0.5224  loss_dice_interm: 1.538  loss_bbox_interm: 0.6104  loss_giou_interm: 0.6908    time: 0.4918  last_time: 0.4563  data_time: 0.0036  last_data_time: 0.0038   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:15:07 d2.utils.events]:  eta: 0:20:41  iter: 1419  total_loss: 74.45  loss_ce: 0.3854  loss_mask: 0.5608  loss_dice: 1.334  loss_bbox: 0.4769  loss_giou: 0.6445  loss_ce_dn: 0.0007412  loss_mask_dn: 0.5466  loss_dice_dn: 1.353  loss_bbox_dn: 0.334  loss_giou_dn: 0.5079  loss_ce_0: 1.039  loss_mask_0: 0.549  loss_dice_0: 1.273  loss_bbox_0: 0.7421  loss_giou_0: 0.8239  loss_ce_dn_0: 0.03037  loss_mask_dn_0: 0.8418  loss_dice_dn_0: 2.518  loss_bbox_dn_0: 0.894  loss_giou_dn_0: 0.8482  loss_ce_1: 0.927  loss_mask_1: 0.5386  loss_dice_1: 1.295  loss_bbox_1: 0.4551  loss_giou_1: 0.6556  loss_ce_dn_1: 0.002217  loss_mask_dn_1: 0.5164  loss_dice_dn_1: 1.354  loss_bbox_dn_1: 0.4045  loss_giou_dn_1: 0.5628  loss_ce_2: 0.7918  loss_mask_2: 0.5363  loss_dice_2: 1.356  loss_bbox_2: 0.43  loss_giou_2: 0.638  loss_ce_dn_2: 0.002136  loss_mask_dn_2: 0.5449  loss_dice_dn_2: 1.35  loss_bbox_dn_2: 0.3968  loss_giou_dn_2: 0.5365  loss_ce_3: 0.5324  loss_mask_3: 0.5365  loss_dice_3: 1.31  loss_bbox_3: 0.4829  loss_giou_3: 0.6565  loss_ce_dn_3: 0.001266  loss_mask_dn_3: 0.5313  loss_dice_dn_3: 1.345  loss_bbox_dn_3: 0.3725  loss_giou_dn_3: 0.5052  loss_ce_4: 0.4686  loss_mask_4: 0.5825  loss_dice_4: 1.459  loss_bbox_4: 0.4871  loss_giou_4: 0.6676  loss_ce_dn_4: 0.00104  loss_mask_dn_4: 0.5108  loss_dice_dn_4: 1.315  loss_bbox_dn_4: 0.3582  loss_giou_dn_4: 0.5113  loss_ce_5: 0.4067  loss_mask_5: 0.5813  loss_dice_5: 1.397  loss_bbox_5: 0.4964  loss_giou_5: 0.6417  loss_ce_dn_5: 0.001248  loss_mask_dn_5: 0.5325  loss_dice_dn_5: 1.331  loss_bbox_dn_5: 0.3334  loss_giou_dn_5: 0.4933  loss_ce_6: 0.412  loss_mask_6: 0.5953  loss_dice_6: 1.381  loss_bbox_6: 0.484  loss_giou_6: 0.6465  loss_ce_dn_6: 0.001893  loss_mask_dn_6: 0.5387  loss_dice_dn_6: 1.36  loss_bbox_dn_6: 0.3397  loss_giou_dn_6: 0.4988  loss_ce_7: 0.4183  loss_mask_7: 0.5741  loss_dice_7: 1.456  loss_bbox_7: 0.4936  loss_giou_7: 0.6471  loss_ce_dn_7: 0.001054  loss_mask_dn_7: 0.5347  loss_dice_dn_7: 1.348  loss_bbox_dn_7: 0.3315  loss_giou_dn_7: 0.5038  loss_ce_8: 0.3795  loss_mask_8: 0.5673  loss_dice_8: 1.36  loss_bbox_8: 0.4896  loss_giou_8: 0.6466  loss_ce_dn_8: 0.0008102  loss_mask_dn_8: 0.5384  loss_dice_dn_8: 1.348  loss_bbox_dn_8: 0.3338  loss_giou_dn_8: 0.5055  loss_ce_interm: 0.8962  loss_mask_interm: 0.5541  loss_dice_interm: 1.504  loss_bbox_interm: 0.6258  loss_giou_interm: 0.7637    time: 0.4914  last_time: 0.4825  data_time: 0.0035  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:15:16 d2.utils.events]:  eta: 0:20:30  iter: 1439  total_loss: 76.62  loss_ce: 0.5142  loss_mask: 0.3105  loss_dice: 1.707  loss_bbox: 0.391  loss_giou: 0.7659  loss_ce_dn: 0.000503  loss_mask_dn: 0.3175  loss_dice_dn: 1.701  loss_bbox_dn: 0.2756  loss_giou_dn: 0.5948  loss_ce_0: 0.9408  loss_mask_0: 0.3114  loss_dice_0: 1.877  loss_bbox_0: 0.7574  loss_giou_0: 1.158  loss_ce_dn_0: 0.02954  loss_mask_dn_0: 0.4634  loss_dice_dn_0: 3.249  loss_bbox_dn_0: 0.6027  loss_giou_dn_0: 0.8568  loss_ce_1: 0.7704  loss_mask_1: 0.3155  loss_dice_1: 1.902  loss_bbox_1: 0.4207  loss_giou_1: 0.8842  loss_ce_dn_1: 0.000848  loss_mask_dn_1: 0.3218  loss_dice_dn_1: 1.693  loss_bbox_dn_1: 0.3367  loss_giou_dn_1: 0.6361  loss_ce_2: 0.6539  loss_mask_2: 0.3209  loss_dice_2: 1.814  loss_bbox_2: 0.3999  loss_giou_2: 0.8253  loss_ce_dn_2: 0.0008293  loss_mask_dn_2: 0.3355  loss_dice_dn_2: 1.726  loss_bbox_dn_2: 0.2987  loss_giou_dn_2: 0.5991  loss_ce_3: 0.4879  loss_mask_3: 0.3203  loss_dice_3: 1.791  loss_bbox_3: 0.3973  loss_giou_3: 0.8178  loss_ce_dn_3: 0.000411  loss_mask_dn_3: 0.335  loss_dice_dn_3: 1.716  loss_bbox_dn_3: 0.2804  loss_giou_dn_3: 0.5798  loss_ce_4: 0.4583  loss_mask_4: 0.3209  loss_dice_4: 1.738  loss_bbox_4: 0.3811  loss_giou_4: 0.8147  loss_ce_dn_4: 0.0004074  loss_mask_dn_4: 0.3409  loss_dice_dn_4: 1.692  loss_bbox_dn_4: 0.2832  loss_giou_dn_4: 0.5904  loss_ce_5: 0.4907  loss_mask_5: 0.3151  loss_dice_5: 1.759  loss_bbox_5: 0.3945  loss_giou_5: 0.7848  loss_ce_dn_5: 0.0006871  loss_mask_dn_5: 0.3338  loss_dice_dn_5: 1.698  loss_bbox_dn_5: 0.2789  loss_giou_dn_5: 0.5887  loss_ce_6: 0.483  loss_mask_6: 0.3162  loss_dice_6: 1.743  loss_bbox_6: 0.3893  loss_giou_6: 0.7883  loss_ce_dn_6: 0.001032  loss_mask_dn_6: 0.327  loss_dice_dn_6: 1.671  loss_bbox_dn_6: 0.2768  loss_giou_dn_6: 0.5902  loss_ce_7: 0.5691  loss_mask_7: 0.3179  loss_dice_7: 1.727  loss_bbox_7: 0.3942  loss_giou_7: 0.7908  loss_ce_dn_7: 0.0006548  loss_mask_dn_7: 0.3249  loss_dice_dn_7: 1.69  loss_bbox_dn_7: 0.2746  loss_giou_dn_7: 0.5921  loss_ce_8: 0.4947  loss_mask_8: 0.3124  loss_dice_8: 1.801  loss_bbox_8: 0.3939  loss_giou_8: 0.8013  loss_ce_dn_8: 0.000484  loss_mask_dn_8: 0.3189  loss_dice_dn_8: 1.712  loss_bbox_dn_8: 0.2757  loss_giou_dn_8: 0.5909  loss_ce_interm: 0.9898  loss_mask_interm: 0.3198  loss_dice_interm: 1.807  loss_bbox_interm: 0.4085  loss_giou_interm: 0.9139    time: 0.4911  last_time: 0.4844  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:15:26 d2.utils.events]:  eta: 0:20:20  iter: 1459  total_loss: 73.85  loss_ce: 0.332  loss_mask: 0.4928  loss_dice: 1.492  loss_bbox: 0.4194  loss_giou: 0.7074  loss_ce_dn: 0.0002812  loss_mask_dn: 0.3995  loss_dice_dn: 1.417  loss_bbox_dn: 0.3413  loss_giou_dn: 0.5165  loss_ce_0: 0.9225  loss_mask_0: 0.4685  loss_dice_0: 1.338  loss_bbox_0: 0.7771  loss_giou_0: 0.8943  loss_ce_dn_0: 0.04444  loss_mask_dn_0: 0.9801  loss_dice_dn_0: 3.291  loss_bbox_dn_0: 0.8477  loss_giou_dn_0: 0.8468  loss_ce_1: 1.088  loss_mask_1: 0.4162  loss_dice_1: 1.415  loss_bbox_1: 0.39  loss_giou_1: 0.5569  loss_ce_dn_1: 0.001376  loss_mask_dn_1: 0.4244  loss_dice_dn_1: 1.407  loss_bbox_dn_1: 0.398  loss_giou_dn_1: 0.5719  loss_ce_2: 0.8688  loss_mask_2: 0.4706  loss_dice_2: 1.455  loss_bbox_2: 0.3886  loss_giou_2: 0.6161  loss_ce_dn_2: 0.001017  loss_mask_dn_2: 0.4158  loss_dice_dn_2: 1.403  loss_bbox_dn_2: 0.3768  loss_giou_dn_2: 0.5104  loss_ce_3: 0.6912  loss_mask_3: 0.4885  loss_dice_3: 1.472  loss_bbox_3: 0.4185  loss_giou_3: 0.6726  loss_ce_dn_3: 0.0006909  loss_mask_dn_3: 0.4147  loss_dice_dn_3: 1.399  loss_bbox_dn_3: 0.3576  loss_giou_dn_3: 0.497  loss_ce_4: 0.5959  loss_mask_4: 0.4923  loss_dice_4: 1.528  loss_bbox_4: 0.4313  loss_giou_4: 0.7165  loss_ce_dn_4: 0.0004853  loss_mask_dn_4: 0.4192  loss_dice_dn_4: 1.385  loss_bbox_dn_4: 0.3398  loss_giou_dn_4: 0.4955  loss_ce_5: 0.4289  loss_mask_5: 0.5042  loss_dice_5: 1.507  loss_bbox_5: 0.4137  loss_giou_5: 0.695  loss_ce_dn_5: 0.0005205  loss_mask_dn_5: 0.3949  loss_dice_dn_5: 1.396  loss_bbox_dn_5: 0.3405  loss_giou_dn_5: 0.4928  loss_ce_6: 0.3562  loss_mask_6: 0.5119  loss_dice_6: 1.542  loss_bbox_6: 0.4081  loss_giou_6: 0.6988  loss_ce_dn_6: 0.0005162  loss_mask_dn_6: 0.393  loss_dice_dn_6: 1.405  loss_bbox_dn_6: 0.334  loss_giou_dn_6: 0.5  loss_ce_7: 0.3142  loss_mask_7: 0.4948  loss_dice_7: 1.512  loss_bbox_7: 0.41  loss_giou_7: 0.6792  loss_ce_dn_7: 0.0002781  loss_mask_dn_7: 0.4003  loss_dice_dn_7: 1.42  loss_bbox_dn_7: 0.3322  loss_giou_dn_7: 0.5033  loss_ce_8: 0.3222  loss_mask_8: 0.4886  loss_dice_8: 1.503  loss_bbox_8: 0.4132  loss_giou_8: 0.701  loss_ce_dn_8: 0.0002745  loss_mask_dn_8: 0.3985  loss_dice_dn_8: 1.404  loss_bbox_dn_8: 0.3335  loss_giou_dn_8: 0.5106  loss_ce_interm: 1.002  loss_mask_interm: 0.4166  loss_dice_interm: 1.391  loss_bbox_interm: 0.5977  loss_giou_interm: 0.8117    time: 0.4910  last_time: 0.4984  data_time: 0.0034  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:15:36 d2.utils.events]:  eta: 0:20:11  iter: 1479  total_loss: 75.94  loss_ce: 0.6123  loss_mask: 0.4626  loss_dice: 1.38  loss_bbox: 0.4576  loss_giou: 0.682  loss_ce_dn: 0.0003621  loss_mask_dn: 0.5012  loss_dice_dn: 1.304  loss_bbox_dn: 0.3109  loss_giou_dn: 0.5051  loss_ce_0: 1.254  loss_mask_0: 0.4756  loss_dice_0: 1.45  loss_bbox_0: 0.7553  loss_giou_0: 0.9515  loss_ce_dn_0: 0.02858  loss_mask_dn_0: 0.7857  loss_dice_dn_0: 2.988  loss_bbox_dn_0: 0.8134  loss_giou_dn_0: 0.8482  loss_ce_1: 0.9837  loss_mask_1: 0.4631  loss_dice_1: 1.399  loss_bbox_1: 0.4648  loss_giou_1: 0.6342  loss_ce_dn_1: 0.0008093  loss_mask_dn_1: 0.488  loss_dice_dn_1: 1.343  loss_bbox_dn_1: 0.4292  loss_giou_dn_1: 0.5662  loss_ce_2: 1.004  loss_mask_2: 0.4899  loss_dice_2: 1.507  loss_bbox_2: 0.4717  loss_giou_2: 0.643  loss_ce_dn_2: 0.000978  loss_mask_dn_2: 0.4758  loss_dice_dn_2: 1.321  loss_bbox_dn_2: 0.3578  loss_giou_dn_2: 0.5163  loss_ce_3: 0.6402  loss_mask_3: 0.5001  loss_dice_3: 1.432  loss_bbox_3: 0.4627  loss_giou_3: 0.7562  loss_ce_dn_3: 0.0004069  loss_mask_dn_3: 0.5042  loss_dice_dn_3: 1.371  loss_bbox_dn_3: 0.3327  loss_giou_dn_3: 0.5113  loss_ce_4: 0.589  loss_mask_4: 0.4775  loss_dice_4: 1.448  loss_bbox_4: 0.4666  loss_giou_4: 0.7018  loss_ce_dn_4: 0.0002955  loss_mask_dn_4: 0.4988  loss_dice_dn_4: 1.316  loss_bbox_dn_4: 0.3153  loss_giou_dn_4: 0.4976  loss_ce_5: 0.6965  loss_mask_5: 0.4707  loss_dice_5: 1.366  loss_bbox_5: 0.4515  loss_giou_5: 0.7304  loss_ce_dn_5: 0.0004185  loss_mask_dn_5: 0.5045  loss_dice_dn_5: 1.314  loss_bbox_dn_5: 0.3087  loss_giou_dn_5: 0.5036  loss_ce_6: 0.6632  loss_mask_6: 0.4858  loss_dice_6: 1.396  loss_bbox_6: 0.4403  loss_giou_6: 0.6901  loss_ce_dn_6: 0.0005343  loss_mask_dn_6: 0.51  loss_dice_dn_6: 1.327  loss_bbox_dn_6: 0.3073  loss_giou_dn_6: 0.5021  loss_ce_7: 0.6354  loss_mask_7: 0.4605  loss_dice_7: 1.463  loss_bbox_7: 0.4502  loss_giou_7: 0.6725  loss_ce_dn_7: 0.0003563  loss_mask_dn_7: 0.5061  loss_dice_dn_7: 1.299  loss_bbox_dn_7: 0.3053  loss_giou_dn_7: 0.5031  loss_ce_8: 0.6137  loss_mask_8: 0.4818  loss_dice_8: 1.428  loss_bbox_8: 0.4544  loss_giou_8: 0.6842  loss_ce_dn_8: 0.0003803  loss_mask_dn_8: 0.4989  loss_dice_dn_8: 1.303  loss_bbox_dn_8: 0.31  loss_giou_dn_8: 0.5027  loss_ce_interm: 1.28  loss_mask_interm: 0.4548  loss_dice_interm: 1.409  loss_bbox_interm: 0.574  loss_giou_interm: 0.746    time: 0.4910  last_time: 0.4775  data_time: 0.0035  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:15:47 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:15:47 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:15:47 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:15:47 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:15:48 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0885 s/iter. Eval: 0.0085 s/iter. Total: 0.0978 s/iter. ETA=0:00:05\n",
      "[03/14 17:15:53 d2.evaluation.evaluator]: Total inference time: 0:00:05.522259 (0.089069 s / iter per device, on 1 devices)\n",
      "[03/14 17:15:53 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077608 s / iter per device, on 1 devices)\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.792\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.582\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.549\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.700\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 42.655 | 79.166 | 45.185 | 41.700 | 54.144 |  nan  |\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:15:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.643\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.184\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.307\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.299\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 20.972 | 64.332 | 3.662  | 18.351 | 30.666 |  nan  |\n",
      "[03/14 17:15:53 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:15:53 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: 42.6549,79.1659,45.1850,41.7002,54.1438,nan\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:15:53 d2.evaluation.testing]: copypaste: 20.9721,64.3321,3.6617,18.3511,30.6664,nan\n",
      "[03/14 17:15:53 d2.utils.events]:  eta: 0:20:02  iter: 1499  total_loss: 70.64  loss_ce: 0.4445  loss_mask: 0.4355  loss_dice: 1.227  loss_bbox: 0.4251  loss_giou: 0.5454  loss_ce_dn: 0.0005016  loss_mask_dn: 0.4488  loss_dice_dn: 1.29  loss_bbox_dn: 0.2883  loss_giou_dn: 0.4562  loss_ce_0: 0.9046  loss_mask_0: 0.4285  loss_dice_0: 1.401  loss_bbox_0: 0.8485  loss_giou_0: 0.9416  loss_ce_dn_0: 0.02795  loss_mask_dn_0: 0.8029  loss_dice_dn_0: 3.3  loss_bbox_dn_0: 0.8591  loss_giou_dn_0: 0.8502  loss_ce_1: 0.9436  loss_mask_1: 0.4106  loss_dice_1: 1.422  loss_bbox_1: 0.4305  loss_giou_1: 0.6492  loss_ce_dn_1: 0.0016  loss_mask_dn_1: 0.4191  loss_dice_dn_1: 1.353  loss_bbox_dn_1: 0.3328  loss_giou_dn_1: 0.5344  loss_ce_2: 0.7315  loss_mask_2: 0.4538  loss_dice_2: 1.375  loss_bbox_2: 0.4413  loss_giou_2: 0.6123  loss_ce_dn_2: 0.001001  loss_mask_dn_2: 0.4404  loss_dice_dn_2: 1.316  loss_bbox_dn_2: 0.312  loss_giou_dn_2: 0.4727  loss_ce_3: 0.5416  loss_mask_3: 0.4138  loss_dice_3: 1.385  loss_bbox_3: 0.4451  loss_giou_3: 0.5723  loss_ce_dn_3: 0.0007624  loss_mask_dn_3: 0.4241  loss_dice_dn_3: 1.328  loss_bbox_dn_3: 0.3032  loss_giou_dn_3: 0.4742  loss_ce_4: 0.611  loss_mask_4: 0.4114  loss_dice_4: 1.355  loss_bbox_4: 0.4425  loss_giou_4: 0.5267  loss_ce_dn_4: 0.0005341  loss_mask_dn_4: 0.4202  loss_dice_dn_4: 1.315  loss_bbox_dn_4: 0.2963  loss_giou_dn_4: 0.4727  loss_ce_5: 0.4591  loss_mask_5: 0.4154  loss_dice_5: 1.265  loss_bbox_5: 0.424  loss_giou_5: 0.5462  loss_ce_dn_5: 0.0008156  loss_mask_dn_5: 0.418  loss_dice_dn_5: 1.314  loss_bbox_dn_5: 0.2927  loss_giou_dn_5: 0.4644  loss_ce_6: 0.4809  loss_mask_6: 0.42  loss_dice_6: 1.268  loss_bbox_6: 0.4448  loss_giou_6: 0.5398  loss_ce_dn_6: 0.0008773  loss_mask_dn_6: 0.43  loss_dice_dn_6: 1.3  loss_bbox_dn_6: 0.2916  loss_giou_dn_6: 0.4636  loss_ce_7: 0.4506  loss_mask_7: 0.4245  loss_dice_7: 1.361  loss_bbox_7: 0.4465  loss_giou_7: 0.5437  loss_ce_dn_7: 0.0007436  loss_mask_dn_7: 0.4387  loss_dice_dn_7: 1.277  loss_bbox_dn_7: 0.2941  loss_giou_dn_7: 0.4598  loss_ce_8: 0.4198  loss_mask_8: 0.4396  loss_dice_8: 1.261  loss_bbox_8: 0.4237  loss_giou_8: 0.5454  loss_ce_dn_8: 0.0006049  loss_mask_dn_8: 0.4461  loss_dice_dn_8: 1.283  loss_bbox_dn_8: 0.291  loss_giou_dn_8: 0.4557  loss_ce_interm: 1.023  loss_mask_interm: 0.4328  loss_dice_interm: 1.297  loss_bbox_interm: 0.4598  loss_giou_interm: 0.6567    time: 0.4910  last_time: 0.4742  data_time: 0.0036  last_data_time: 0.0040   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:03 d2.utils.events]:  eta: 0:19:51  iter: 1519  total_loss: 70.95  loss_ce: 0.4563  loss_mask: 0.4527  loss_dice: 1.528  loss_bbox: 0.3667  loss_giou: 0.6809  loss_ce_dn: 0.0005201  loss_mask_dn: 0.445  loss_dice_dn: 1.467  loss_bbox_dn: 0.2934  loss_giou_dn: 0.5248  loss_ce_0: 0.9467  loss_mask_0: 0.4492  loss_dice_0: 1.427  loss_bbox_0: 0.7022  loss_giou_0: 1.056  loss_ce_dn_0: 0.02743  loss_mask_dn_0: 0.5466  loss_dice_dn_0: 2.802  loss_bbox_dn_0: 0.8396  loss_giou_dn_0: 0.8496  loss_ce_1: 0.9359  loss_mask_1: 0.4256  loss_dice_1: 1.504  loss_bbox_1: 0.3748  loss_giou_1: 0.6489  loss_ce_dn_1: 0.001018  loss_mask_dn_1: 0.4255  loss_dice_dn_1: 1.394  loss_bbox_dn_1: 0.3831  loss_giou_dn_1: 0.564  loss_ce_2: 0.7612  loss_mask_2: 0.4348  loss_dice_2: 1.484  loss_bbox_2: 0.3937  loss_giou_2: 0.6634  loss_ce_dn_2: 0.001549  loss_mask_dn_2: 0.4223  loss_dice_dn_2: 1.355  loss_bbox_dn_2: 0.3205  loss_giou_dn_2: 0.5431  loss_ce_3: 0.6557  loss_mask_3: 0.4167  loss_dice_3: 1.456  loss_bbox_3: 0.3641  loss_giou_3: 0.6766  loss_ce_dn_3: 0.0006008  loss_mask_dn_3: 0.4322  loss_dice_dn_3: 1.381  loss_bbox_dn_3: 0.3189  loss_giou_dn_3: 0.5333  loss_ce_4: 0.461  loss_mask_4: 0.4591  loss_dice_4: 1.481  loss_bbox_4: 0.3513  loss_giou_4: 0.6349  loss_ce_dn_4: 0.0004751  loss_mask_dn_4: 0.4437  loss_dice_dn_4: 1.385  loss_bbox_dn_4: 0.332  loss_giou_dn_4: 0.5326  loss_ce_5: 0.4399  loss_mask_5: 0.4293  loss_dice_5: 1.361  loss_bbox_5: 0.3676  loss_giou_5: 0.6633  loss_ce_dn_5: 0.0007858  loss_mask_dn_5: 0.4258  loss_dice_dn_5: 1.391  loss_bbox_dn_5: 0.3182  loss_giou_dn_5: 0.5214  loss_ce_6: 0.391  loss_mask_6: 0.4516  loss_dice_6: 1.501  loss_bbox_6: 0.3732  loss_giou_6: 0.6887  loss_ce_dn_6: 0.0009144  loss_mask_dn_6: 0.4446  loss_dice_dn_6: 1.416  loss_bbox_dn_6: 0.309  loss_giou_dn_6: 0.5325  loss_ce_7: 0.3753  loss_mask_7: 0.427  loss_dice_7: 1.536  loss_bbox_7: 0.394  loss_giou_7: 0.7176  loss_ce_dn_7: 0.0005649  loss_mask_dn_7: 0.4508  loss_dice_dn_7: 1.449  loss_bbox_dn_7: 0.3025  loss_giou_dn_7: 0.53  loss_ce_8: 0.4515  loss_mask_8: 0.4487  loss_dice_8: 1.557  loss_bbox_8: 0.3737  loss_giou_8: 0.6871  loss_ce_dn_8: 0.00049  loss_mask_dn_8: 0.4506  loss_dice_dn_8: 1.463  loss_bbox_dn_8: 0.2951  loss_giou_dn_8: 0.5261  loss_ce_interm: 1.026  loss_mask_interm: 0.4397  loss_dice_interm: 1.528  loss_bbox_interm: 0.5249  loss_giou_interm: 0.8464    time: 0.4908  last_time: 0.5197  data_time: 0.0034  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:13 d2.utils.events]:  eta: 0:19:41  iter: 1539  total_loss: 83.23  loss_ce: 0.7918  loss_mask: 0.4395  loss_dice: 1.953  loss_bbox: 0.4297  loss_giou: 0.6919  loss_ce_dn: 0.0007633  loss_mask_dn: 0.4099  loss_dice_dn: 1.683  loss_bbox_dn: 0.3062  loss_giou_dn: 0.5345  loss_ce_0: 1.355  loss_mask_0: 0.4508  loss_dice_0: 1.714  loss_bbox_0: 0.7601  loss_giou_0: 1.06  loss_ce_dn_0: 0.05108  loss_mask_dn_0: 0.7731  loss_dice_dn_0: 3.426  loss_bbox_dn_0: 0.7096  loss_giou_dn_0: 0.8528  loss_ce_1: 1.485  loss_mask_1: 0.4258  loss_dice_1: 1.589  loss_bbox_1: 0.3507  loss_giou_1: 0.583  loss_ce_dn_1: 0.001013  loss_mask_dn_1: 0.4472  loss_dice_dn_1: 1.697  loss_bbox_dn_1: 0.376  loss_giou_dn_1: 0.6091  loss_ce_2: 1.081  loss_mask_2: 0.4465  loss_dice_2: 1.676  loss_bbox_2: 0.3744  loss_giou_2: 0.5882  loss_ce_dn_2: 0.0009141  loss_mask_dn_2: 0.425  loss_dice_dn_2: 1.687  loss_bbox_dn_2: 0.3428  loss_giou_dn_2: 0.5677  loss_ce_3: 1.087  loss_mask_3: 0.4642  loss_dice_3: 1.727  loss_bbox_3: 0.3565  loss_giou_3: 0.6885  loss_ce_dn_3: 0.0009463  loss_mask_dn_3: 0.4234  loss_dice_dn_3: 1.698  loss_bbox_dn_3: 0.3477  loss_giou_dn_3: 0.5504  loss_ce_4: 0.995  loss_mask_4: 0.4261  loss_dice_4: 1.763  loss_bbox_4: 0.413  loss_giou_4: 0.6484  loss_ce_dn_4: 0.0009265  loss_mask_dn_4: 0.4187  loss_dice_dn_4: 1.658  loss_bbox_dn_4: 0.3213  loss_giou_dn_4: 0.5388  loss_ce_5: 0.9118  loss_mask_5: 0.4474  loss_dice_5: 1.84  loss_bbox_5: 0.3714  loss_giou_5: 0.6611  loss_ce_dn_5: 0.001074  loss_mask_dn_5: 0.4279  loss_dice_dn_5: 1.663  loss_bbox_dn_5: 0.3218  loss_giou_dn_5: 0.5351  loss_ce_6: 0.8574  loss_mask_6: 0.454  loss_dice_6: 1.847  loss_bbox_6: 0.4024  loss_giou_6: 0.6662  loss_ce_dn_6: 0.001238  loss_mask_dn_6: 0.4171  loss_dice_dn_6: 1.656  loss_bbox_dn_6: 0.3127  loss_giou_dn_6: 0.5359  loss_ce_7: 0.8077  loss_mask_7: 0.4491  loss_dice_7: 1.808  loss_bbox_7: 0.3963  loss_giou_7: 0.6724  loss_ce_dn_7: 0.000978  loss_mask_dn_7: 0.417  loss_dice_dn_7: 1.664  loss_bbox_dn_7: 0.3082  loss_giou_dn_7: 0.5348  loss_ce_8: 0.8042  loss_mask_8: 0.4508  loss_dice_8: 1.838  loss_bbox_8: 0.4255  loss_giou_8: 0.6791  loss_ce_dn_8: 0.0008252  loss_mask_dn_8: 0.4091  loss_dice_dn_8: 1.678  loss_bbox_dn_8: 0.3108  loss_giou_dn_8: 0.5354  loss_ce_interm: 1.333  loss_mask_interm: 0.4709  loss_dice_interm: 1.754  loss_bbox_interm: 0.5868  loss_giou_interm: 0.8732    time: 0.4908  last_time: 0.5179  data_time: 0.0036  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:23 d2.utils.events]:  eta: 0:19:32  iter: 1559  total_loss: 72.75  loss_ce: 0.5739  loss_mask: 0.5151  loss_dice: 1.458  loss_bbox: 0.4244  loss_giou: 0.6675  loss_ce_dn: 0.0004209  loss_mask_dn: 0.4488  loss_dice_dn: 1.346  loss_bbox_dn: 0.3173  loss_giou_dn: 0.4832  loss_ce_0: 0.9378  loss_mask_0: 0.4947  loss_dice_0: 1.367  loss_bbox_0: 0.7211  loss_giou_0: 1.027  loss_ce_dn_0: 0.0265  loss_mask_dn_0: 0.6969  loss_dice_dn_0: 3.051  loss_bbox_dn_0: 0.821  loss_giou_dn_0: 0.8445  loss_ce_1: 1.056  loss_mask_1: 0.4434  loss_dice_1: 1.379  loss_bbox_1: 0.4209  loss_giou_1: 0.6466  loss_ce_dn_1: 0.00144  loss_mask_dn_1: 0.4774  loss_dice_dn_1: 1.401  loss_bbox_dn_1: 0.3666  loss_giou_dn_1: 0.5701  loss_ce_2: 0.7935  loss_mask_2: 0.5013  loss_dice_2: 1.413  loss_bbox_2: 0.4277  loss_giou_2: 0.6174  loss_ce_dn_2: 0.0009428  loss_mask_dn_2: 0.4653  loss_dice_dn_2: 1.356  loss_bbox_dn_2: 0.327  loss_giou_dn_2: 0.5107  loss_ce_3: 0.6796  loss_mask_3: 0.5055  loss_dice_3: 1.422  loss_bbox_3: 0.4129  loss_giou_3: 0.6753  loss_ce_dn_3: 0.000802  loss_mask_dn_3: 0.4713  loss_dice_dn_3: 1.382  loss_bbox_dn_3: 0.3128  loss_giou_dn_3: 0.4859  loss_ce_4: 0.637  loss_mask_4: 0.4998  loss_dice_4: 1.529  loss_bbox_4: 0.4425  loss_giou_4: 0.6915  loss_ce_dn_4: 0.0007187  loss_mask_dn_4: 0.4509  loss_dice_dn_4: 1.38  loss_bbox_dn_4: 0.3125  loss_giou_dn_4: 0.4749  loss_ce_5: 0.6482  loss_mask_5: 0.5052  loss_dice_5: 1.442  loss_bbox_5: 0.4409  loss_giou_5: 0.6584  loss_ce_dn_5: 0.001034  loss_mask_dn_5: 0.4452  loss_dice_dn_5: 1.384  loss_bbox_dn_5: 0.3257  loss_giou_dn_5: 0.4923  loss_ce_6: 0.6533  loss_mask_6: 0.4878  loss_dice_6: 1.536  loss_bbox_6: 0.3999  loss_giou_6: 0.6639  loss_ce_dn_6: 0.0008595  loss_mask_dn_6: 0.449  loss_dice_dn_6: 1.394  loss_bbox_dn_6: 0.3184  loss_giou_dn_6: 0.4867  loss_ce_7: 0.6351  loss_mask_7: 0.476  loss_dice_7: 1.504  loss_bbox_7: 0.3973  loss_giou_7: 0.6631  loss_ce_dn_7: 0.0006047  loss_mask_dn_7: 0.4487  loss_dice_dn_7: 1.368  loss_bbox_dn_7: 0.3181  loss_giou_dn_7: 0.4841  loss_ce_8: 0.6331  loss_mask_8: 0.4803  loss_dice_8: 1.434  loss_bbox_8: 0.3948  loss_giou_8: 0.6606  loss_ce_dn_8: 0.0005193  loss_mask_dn_8: 0.454  loss_dice_dn_8: 1.349  loss_bbox_dn_8: 0.3157  loss_giou_dn_8: 0.4832  loss_ce_interm: 1.029  loss_mask_interm: 0.5793  loss_dice_interm: 1.459  loss_bbox_interm: 0.4618  loss_giou_interm: 0.6853    time: 0.4909  last_time: 0.5143  data_time: 0.0035  last_data_time: 0.0041   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:33 d2.utils.events]:  eta: 0:19:24  iter: 1579  total_loss: 74.29  loss_ce: 0.2904  loss_mask: 0.4486  loss_dice: 1.457  loss_bbox: 0.4044  loss_giou: 0.5991  loss_ce_dn: 0.0002164  loss_mask_dn: 0.4108  loss_dice_dn: 1.318  loss_bbox_dn: 0.3181  loss_giou_dn: 0.4896  loss_ce_0: 0.8904  loss_mask_0: 0.4381  loss_dice_0: 1.277  loss_bbox_0: 0.715  loss_giou_0: 0.9449  loss_ce_dn_0: 0.05021  loss_mask_dn_0: 0.5411  loss_dice_dn_0: 2.547  loss_bbox_dn_0: 0.8711  loss_giou_dn_0: 0.8516  loss_ce_1: 0.7429  loss_mask_1: 0.4144  loss_dice_1: 1.316  loss_bbox_1: 0.4076  loss_giou_1: 0.5413  loss_ce_dn_1: 0.001405  loss_mask_dn_1: 0.4625  loss_dice_dn_1: 1.304  loss_bbox_dn_1: 0.4153  loss_giou_dn_1: 0.5277  loss_ce_2: 0.4949  loss_mask_2: 0.4719  loss_dice_2: 1.293  loss_bbox_2: 0.4153  loss_giou_2: 0.5842  loss_ce_dn_2: 0.0009269  loss_mask_dn_2: 0.4451  loss_dice_dn_2: 1.312  loss_bbox_dn_2: 0.3261  loss_giou_dn_2: 0.5051  loss_ce_3: 0.3965  loss_mask_3: 0.454  loss_dice_3: 1.421  loss_bbox_3: 0.4213  loss_giou_3: 0.5752  loss_ce_dn_3: 0.0006371  loss_mask_dn_3: 0.4445  loss_dice_dn_3: 1.343  loss_bbox_dn_3: 0.3173  loss_giou_dn_3: 0.4995  loss_ce_4: 0.3342  loss_mask_4: 0.4571  loss_dice_4: 1.381  loss_bbox_4: 0.3969  loss_giou_4: 0.6068  loss_ce_dn_4: 0.0004405  loss_mask_dn_4: 0.4285  loss_dice_dn_4: 1.335  loss_bbox_dn_4: 0.3304  loss_giou_dn_4: 0.4919  loss_ce_5: 0.331  loss_mask_5: 0.4541  loss_dice_5: 1.425  loss_bbox_5: 0.4018  loss_giou_5: 0.5865  loss_ce_dn_5: 0.0007428  loss_mask_dn_5: 0.4127  loss_dice_dn_5: 1.318  loss_bbox_dn_5: 0.3185  loss_giou_dn_5: 0.4863  loss_ce_6: 0.3205  loss_mask_6: 0.4707  loss_dice_6: 1.381  loss_bbox_6: 0.4054  loss_giou_6: 0.5945  loss_ce_dn_6: 0.0006369  loss_mask_dn_6: 0.4197  loss_dice_dn_6: 1.302  loss_bbox_dn_6: 0.3267  loss_giou_dn_6: 0.4942  loss_ce_7: 0.2744  loss_mask_7: 0.47  loss_dice_7: 1.359  loss_bbox_7: 0.4038  loss_giou_7: 0.596  loss_ce_dn_7: 0.0003722  loss_mask_dn_7: 0.4148  loss_dice_dn_7: 1.315  loss_bbox_dn_7: 0.3197  loss_giou_dn_7: 0.4908  loss_ce_8: 0.2981  loss_mask_8: 0.4353  loss_dice_8: 1.336  loss_bbox_8: 0.4051  loss_giou_8: 0.5986  loss_ce_dn_8: 0.0002997  loss_mask_dn_8: 0.4109  loss_dice_dn_8: 1.317  loss_bbox_dn_8: 0.3188  loss_giou_dn_8: 0.4915  loss_ce_interm: 0.8694  loss_mask_interm: 0.4917  loss_dice_interm: 1.407  loss_bbox_interm: 0.4797  loss_giou_interm: 0.8179    time: 0.4908  last_time: 0.4602  data_time: 0.0036  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:43 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:16:43 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:16:43 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:16:43 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:16:44 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0007 s/iter. Inference: 0.0733 s/iter. Eval: 0.0111 s/iter. Total: 0.0851 s/iter. ETA=0:00:04\n",
      "[03/14 17:16:49 d2.evaluation.evaluator]: Total inference time: 0:00:05.359845 (0.086449 s / iter per device, on 1 devices)\n",
      "[03/14 17:16:49 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.074330 s / iter per device, on 1 devices)\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.406\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.786\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.375\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.413\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.483\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.531\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.582\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.555\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.681\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 40.612 | 78.555 | 37.521 | 41.345 | 48.316 |  nan  |\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:16:49 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.649\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.181\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.305\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.298\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.304\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 20.549 | 64.924 | 1.288  | 18.066 | 30.509 |  nan  |\n",
      "[03/14 17:16:49 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:16:49 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: 40.6124,78.5550,37.5212,41.3451,48.3156,nan\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:16:49 d2.evaluation.testing]: copypaste: 20.5486,64.9235,1.2883,18.0657,30.5087,nan\n",
      "[03/14 17:16:49 d2.utils.events]:  eta: 0:19:14  iter: 1599  total_loss: 75.73  loss_ce: 0.5108  loss_mask: 0.3215  loss_dice: 1.386  loss_bbox: 0.3696  loss_giou: 0.6079  loss_ce_dn: 0.0005631  loss_mask_dn: 0.2975  loss_dice_dn: 1.321  loss_bbox_dn: 0.3036  loss_giou_dn: 0.5131  loss_ce_0: 0.9537  loss_mask_0: 0.3371  loss_dice_0: 1.378  loss_bbox_0: 0.7194  loss_giou_0: 0.9785  loss_ce_dn_0: 0.04047  loss_mask_dn_0: 0.7049  loss_dice_dn_0: 2.989  loss_bbox_dn_0: 0.8054  loss_giou_dn_0: 0.8517  loss_ce_1: 0.9322  loss_mask_1: 0.3539  loss_dice_1: 1.477  loss_bbox_1: 0.4761  loss_giou_1: 0.6694  loss_ce_dn_1: 0.001449  loss_mask_dn_1: 0.3039  loss_dice_dn_1: 1.401  loss_bbox_dn_1: 0.3726  loss_giou_dn_1: 0.5627  loss_ce_2: 0.8107  loss_mask_2: 0.361  loss_dice_2: 1.345  loss_bbox_2: 0.4494  loss_giou_2: 0.6187  loss_ce_dn_2: 0.001254  loss_mask_dn_2: 0.3071  loss_dice_dn_2: 1.328  loss_bbox_dn_2: 0.3335  loss_giou_dn_2: 0.515  loss_ce_3: 0.6342  loss_mask_3: 0.3288  loss_dice_3: 1.391  loss_bbox_3: 0.4332  loss_giou_3: 0.6303  loss_ce_dn_3: 0.001079  loss_mask_dn_3: 0.312  loss_dice_dn_3: 1.347  loss_bbox_dn_3: 0.3003  loss_giou_dn_3: 0.5097  loss_ce_4: 0.595  loss_mask_4: 0.3531  loss_dice_4: 1.416  loss_bbox_4: 0.4236  loss_giou_4: 0.6437  loss_ce_dn_4: 0.0008761  loss_mask_dn_4: 0.3111  loss_dice_dn_4: 1.337  loss_bbox_dn_4: 0.3095  loss_giou_dn_4: 0.5098  loss_ce_5: 0.5689  loss_mask_5: 0.3104  loss_dice_5: 1.513  loss_bbox_5: 0.3867  loss_giou_5: 0.6204  loss_ce_dn_5: 0.00172  loss_mask_dn_5: 0.2996  loss_dice_dn_5: 1.317  loss_bbox_dn_5: 0.3079  loss_giou_dn_5: 0.5125  loss_ce_6: 0.507  loss_mask_6: 0.3284  loss_dice_6: 1.416  loss_bbox_6: 0.3734  loss_giou_6: 0.6053  loss_ce_dn_6: 0.001373  loss_mask_dn_6: 0.3021  loss_dice_dn_6: 1.313  loss_bbox_dn_6: 0.3041  loss_giou_dn_6: 0.511  loss_ce_7: 0.5317  loss_mask_7: 0.3195  loss_dice_7: 1.399  loss_bbox_7: 0.374  loss_giou_7: 0.6058  loss_ce_dn_7: 0.001025  loss_mask_dn_7: 0.2965  loss_dice_dn_7: 1.321  loss_bbox_dn_7: 0.3153  loss_giou_dn_7: 0.5106  loss_ce_8: 0.5063  loss_mask_8: 0.3262  loss_dice_8: 1.465  loss_bbox_8: 0.3614  loss_giou_8: 0.6084  loss_ce_dn_8: 0.0008143  loss_mask_dn_8: 0.298  loss_dice_dn_8: 1.311  loss_bbox_dn_8: 0.3035  loss_giou_dn_8: 0.5088  loss_ce_interm: 1.12  loss_mask_interm: 0.3677  loss_dice_interm: 1.333  loss_bbox_interm: 0.5043  loss_giou_interm: 0.763    time: 0.4908  last_time: 0.4808  data_time: 0.0036  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:16:59 d2.utils.events]:  eta: 0:19:05  iter: 1619  total_loss: 74.91  loss_ce: 0.493  loss_mask: 0.4086  loss_dice: 1.449  loss_bbox: 0.4333  loss_giou: 0.5759  loss_ce_dn: 0.0005406  loss_mask_dn: 0.4221  loss_dice_dn: 1.458  loss_bbox_dn: 0.3283  loss_giou_dn: 0.4983  loss_ce_0: 0.9672  loss_mask_0: 0.4725  loss_dice_0: 1.49  loss_bbox_0: 0.7255  loss_giou_0: 0.973  loss_ce_dn_0: 0.0736  loss_mask_dn_0: 0.8594  loss_dice_dn_0: 3.07  loss_bbox_dn_0: 0.882  loss_giou_dn_0: 0.8499  loss_ce_1: 1.014  loss_mask_1: 0.4243  loss_dice_1: 1.54  loss_bbox_1: 0.4453  loss_giou_1: 0.6483  loss_ce_dn_1: 0.001902  loss_mask_dn_1: 0.422  loss_dice_dn_1: 1.513  loss_bbox_dn_1: 0.3966  loss_giou_dn_1: 0.5545  loss_ce_2: 0.8654  loss_mask_2: 0.4033  loss_dice_2: 1.494  loss_bbox_2: 0.4497  loss_giou_2: 0.6368  loss_ce_dn_2: 0.0016  loss_mask_dn_2: 0.4238  loss_dice_dn_2: 1.471  loss_bbox_dn_2: 0.3639  loss_giou_dn_2: 0.5271  loss_ce_3: 0.946  loss_mask_3: 0.443  loss_dice_3: 1.562  loss_bbox_3: 0.4297  loss_giou_3: 0.6343  loss_ce_dn_3: 0.001653  loss_mask_dn_3: 0.4105  loss_dice_dn_3: 1.455  loss_bbox_dn_3: 0.3348  loss_giou_dn_3: 0.5096  loss_ce_4: 0.6445  loss_mask_4: 0.4163  loss_dice_4: 1.492  loss_bbox_4: 0.4344  loss_giou_4: 0.6527  loss_ce_dn_4: 0.001003  loss_mask_dn_4: 0.4177  loss_dice_dn_4: 1.48  loss_bbox_dn_4: 0.3492  loss_giou_dn_4: 0.4981  loss_ce_5: 0.5544  loss_mask_5: 0.4207  loss_dice_5: 1.436  loss_bbox_5: 0.41  loss_giou_5: 0.6738  loss_ce_dn_5: 0.001645  loss_mask_dn_5: 0.4166  loss_dice_dn_5: 1.458  loss_bbox_dn_5: 0.3474  loss_giou_dn_5: 0.4945  loss_ce_6: 0.4646  loss_mask_6: 0.43  loss_dice_6: 1.402  loss_bbox_6: 0.4173  loss_giou_6: 0.6613  loss_ce_dn_6: 0.001125  loss_mask_dn_6: 0.4144  loss_dice_dn_6: 1.479  loss_bbox_dn_6: 0.3356  loss_giou_dn_6: 0.4931  loss_ce_7: 0.5317  loss_mask_7: 0.4188  loss_dice_7: 1.48  loss_bbox_7: 0.406  loss_giou_7: 0.5892  loss_ce_dn_7: 0.0007187  loss_mask_dn_7: 0.4171  loss_dice_dn_7: 1.473  loss_bbox_dn_7: 0.3299  loss_giou_dn_7: 0.4917  loss_ce_8: 0.4908  loss_mask_8: 0.4304  loss_dice_8: 1.456  loss_bbox_8: 0.4025  loss_giou_8: 0.5724  loss_ce_dn_8: 0.0006504  loss_mask_dn_8: 0.4228  loss_dice_dn_8: 1.461  loss_bbox_dn_8: 0.3294  loss_giou_dn_8: 0.4974  loss_ce_interm: 1.141  loss_mask_interm: 0.5539  loss_dice_interm: 1.463  loss_bbox_interm: 0.4288  loss_giou_interm: 0.7382    time: 0.4905  last_time: 0.4735  data_time: 0.0035  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:17:08 d2.utils.events]:  eta: 0:18:55  iter: 1639  total_loss: 72.83  loss_ce: 0.3955  loss_mask: 0.3406  loss_dice: 1.555  loss_bbox: 0.3152  loss_giou: 0.557  loss_ce_dn: 0.0003667  loss_mask_dn: 0.2852  loss_dice_dn: 1.462  loss_bbox_dn: 0.2248  loss_giou_dn: 0.4685  loss_ce_0: 0.9175  loss_mask_0: 0.3264  loss_dice_0: 1.756  loss_bbox_0: 0.7669  loss_giou_0: 1.09  loss_ce_dn_0: 0.02529  loss_mask_dn_0: 0.5483  loss_dice_dn_0: 2.828  loss_bbox_dn_0: 0.6413  loss_giou_dn_0: 0.8552  loss_ce_1: 0.908  loss_mask_1: 0.3264  loss_dice_1: 1.616  loss_bbox_1: 0.3872  loss_giou_1: 0.7364  loss_ce_dn_1: 0.001278  loss_mask_dn_1: 0.3154  loss_dice_dn_1: 1.545  loss_bbox_dn_1: 0.2932  loss_giou_dn_1: 0.5217  loss_ce_2: 0.5796  loss_mask_2: 0.321  loss_dice_2: 1.536  loss_bbox_2: 0.3605  loss_giou_2: 0.6209  loss_ce_dn_2: 0.001046  loss_mask_dn_2: 0.3111  loss_dice_dn_2: 1.467  loss_bbox_dn_2: 0.2526  loss_giou_dn_2: 0.519  loss_ce_3: 0.5463  loss_mask_3: 0.3267  loss_dice_3: 1.553  loss_bbox_3: 0.3589  loss_giou_3: 0.5935  loss_ce_dn_3: 0.001102  loss_mask_dn_3: 0.3119  loss_dice_dn_3: 1.451  loss_bbox_dn_3: 0.2195  loss_giou_dn_3: 0.5095  loss_ce_4: 0.4706  loss_mask_4: 0.3018  loss_dice_4: 1.486  loss_bbox_4: 0.3434  loss_giou_4: 0.547  loss_ce_dn_4: 0.00061  loss_mask_dn_4: 0.2987  loss_dice_dn_4: 1.468  loss_bbox_dn_4: 0.2281  loss_giou_dn_4: 0.4825  loss_ce_5: 0.4726  loss_mask_5: 0.3253  loss_dice_5: 1.496  loss_bbox_5: 0.3194  loss_giou_5: 0.5146  loss_ce_dn_5: 0.0007619  loss_mask_dn_5: 0.2883  loss_dice_dn_5: 1.471  loss_bbox_dn_5: 0.234  loss_giou_dn_5: 0.492  loss_ce_6: 0.4392  loss_mask_6: 0.3248  loss_dice_6: 1.605  loss_bbox_6: 0.3081  loss_giou_6: 0.6007  loss_ce_dn_6: 0.0007469  loss_mask_dn_6: 0.274  loss_dice_dn_6: 1.471  loss_bbox_dn_6: 0.2268  loss_giou_dn_6: 0.4724  loss_ce_7: 0.4039  loss_mask_7: 0.3247  loss_dice_7: 1.509  loss_bbox_7: 0.3118  loss_giou_7: 0.567  loss_ce_dn_7: 0.0004989  loss_mask_dn_7: 0.2854  loss_dice_dn_7: 1.461  loss_bbox_dn_7: 0.226  loss_giou_dn_7: 0.4752  loss_ce_8: 0.3869  loss_mask_8: 0.3206  loss_dice_8: 1.54  loss_bbox_8: 0.317  loss_giou_8: 0.5997  loss_ce_dn_8: 0.0004369  loss_mask_dn_8: 0.2849  loss_dice_dn_8: 1.456  loss_bbox_dn_8: 0.2228  loss_giou_dn_8: 0.4666  loss_ce_interm: 0.9439  loss_mask_interm: 0.3283  loss_dice_interm: 1.625  loss_bbox_interm: 0.4648  loss_giou_interm: 0.8606    time: 0.4904  last_time: 0.5231  data_time: 0.0035  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:17:18 d2.utils.events]:  eta: 0:18:46  iter: 1659  total_loss: 82.55  loss_ce: 0.4227  loss_mask: 0.4723  loss_dice: 1.596  loss_bbox: 0.4  loss_giou: 0.8188  loss_ce_dn: 0.0001889  loss_mask_dn: 0.4306  loss_dice_dn: 1.439  loss_bbox_dn: 0.3143  loss_giou_dn: 0.4937  loss_ce_0: 1.082  loss_mask_0: 0.4275  loss_dice_0: 1.784  loss_bbox_0: 0.6055  loss_giou_0: 0.8808  loss_ce_dn_0: 0.02473  loss_mask_dn_0: 0.714  loss_dice_dn_0: 2.977  loss_bbox_dn_0: 0.8634  loss_giou_dn_0: 0.8474  loss_ce_1: 1.247  loss_mask_1: 0.4003  loss_dice_1: 1.717  loss_bbox_1: 0.3752  loss_giou_1: 0.6823  loss_ce_dn_1: 0.001132  loss_mask_dn_1: 0.4427  loss_dice_dn_1: 1.551  loss_bbox_dn_1: 0.4129  loss_giou_dn_1: 0.5822  loss_ce_2: 0.8261  loss_mask_2: 0.3987  loss_dice_2: 1.541  loss_bbox_2: 0.4235  loss_giou_2: 0.6867  loss_ce_dn_2: 0.0006526  loss_mask_dn_2: 0.4535  loss_dice_dn_2: 1.405  loss_bbox_dn_2: 0.3608  loss_giou_dn_2: 0.5176  loss_ce_3: 0.703  loss_mask_3: 0.4122  loss_dice_3: 1.672  loss_bbox_3: 0.4262  loss_giou_3: 0.6791  loss_ce_dn_3: 0.0009605  loss_mask_dn_3: 0.4219  loss_dice_dn_3: 1.364  loss_bbox_dn_3: 0.3363  loss_giou_dn_3: 0.4872  loss_ce_4: 0.6294  loss_mask_4: 0.4292  loss_dice_4: 1.625  loss_bbox_4: 0.4391  loss_giou_4: 0.7398  loss_ce_dn_4: 0.000392  loss_mask_dn_4: 0.4334  loss_dice_dn_4: 1.354  loss_bbox_dn_4: 0.3154  loss_giou_dn_4: 0.4965  loss_ce_5: 0.592  loss_mask_5: 0.4095  loss_dice_5: 1.643  loss_bbox_5: 0.3956  loss_giou_5: 0.7807  loss_ce_dn_5: 0.0006138  loss_mask_dn_5: 0.4181  loss_dice_dn_5: 1.395  loss_bbox_dn_5: 0.3225  loss_giou_dn_5: 0.4804  loss_ce_6: 0.4835  loss_mask_6: 0.4026  loss_dice_6: 1.538  loss_bbox_6: 0.3866  loss_giou_6: 0.8085  loss_ce_dn_6: 0.0006744  loss_mask_dn_6: 0.4189  loss_dice_dn_6: 1.415  loss_bbox_dn_6: 0.3106  loss_giou_dn_6: 0.4883  loss_ce_7: 0.4599  loss_mask_7: 0.4049  loss_dice_7: 1.649  loss_bbox_7: 0.4005  loss_giou_7: 0.823  loss_ce_dn_7: 0.0004729  loss_mask_dn_7: 0.4247  loss_dice_dn_7: 1.456  loss_bbox_dn_7: 0.3103  loss_giou_dn_7: 0.4906  loss_ce_8: 0.4175  loss_mask_8: 0.4515  loss_dice_8: 1.682  loss_bbox_8: 0.3967  loss_giou_8: 0.8329  loss_ce_dn_8: 0.0002756  loss_mask_dn_8: 0.4269  loss_dice_dn_8: 1.426  loss_bbox_dn_8: 0.3107  loss_giou_dn_8: 0.4924  loss_ce_interm: 1.077  loss_mask_interm: 0.4341  loss_dice_interm: 1.648  loss_bbox_interm: 0.5323  loss_giou_interm: 0.8225    time: 0.4904  last_time: 0.4967  data_time: 0.0037  last_data_time: 0.0039   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:17:28 d2.utils.events]:  eta: 0:18:37  iter: 1679  total_loss: 76.54  loss_ce: 0.6999  loss_mask: 0.3986  loss_dice: 2.067  loss_bbox: 0.324  loss_giou: 0.7588  loss_ce_dn: 0.0006203  loss_mask_dn: 0.3621  loss_dice_dn: 1.583  loss_bbox_dn: 0.2554  loss_giou_dn: 0.5535  loss_ce_0: 0.9607  loss_mask_0: 0.3353  loss_dice_0: 1.635  loss_bbox_0: 0.6883  loss_giou_0: 1.215  loss_ce_dn_0: 0.02522  loss_mask_dn_0: 0.5625  loss_dice_dn_0: 2.841  loss_bbox_dn_0: 0.654  loss_giou_dn_0: 0.8494  loss_ce_1: 0.9347  loss_mask_1: 0.3706  loss_dice_1: 1.613  loss_bbox_1: 0.4063  loss_giou_1: 0.8783  loss_ce_dn_1: 0.001468  loss_mask_dn_1: 0.3843  loss_dice_dn_1: 1.657  loss_bbox_dn_1: 0.3082  loss_giou_dn_1: 0.5981  loss_ce_2: 0.8612  loss_mask_2: 0.3804  loss_dice_2: 1.649  loss_bbox_2: 0.4018  loss_giou_2: 0.8878  loss_ce_dn_2: 0.001152  loss_mask_dn_2: 0.406  loss_dice_dn_2: 1.632  loss_bbox_dn_2: 0.2633  loss_giou_dn_2: 0.5729  loss_ce_3: 0.6273  loss_mask_3: 0.36  loss_dice_3: 1.792  loss_bbox_3: 0.3821  loss_giou_3: 0.8899  loss_ce_dn_3: 0.001212  loss_mask_dn_3: 0.3759  loss_dice_dn_3: 1.579  loss_bbox_dn_3: 0.2605  loss_giou_dn_3: 0.5533  loss_ce_4: 0.7867  loss_mask_4: 0.3632  loss_dice_4: 1.787  loss_bbox_4: 0.3313  loss_giou_4: 0.8626  loss_ce_dn_4: 0.0008131  loss_mask_dn_4: 0.3826  loss_dice_dn_4: 1.589  loss_bbox_dn_4: 0.2558  loss_giou_dn_4: 0.5502  loss_ce_5: 0.7524  loss_mask_5: 0.3972  loss_dice_5: 1.682  loss_bbox_5: 0.3358  loss_giou_5: 0.8257  loss_ce_dn_5: 0.001341  loss_mask_dn_5: 0.3788  loss_dice_dn_5: 1.614  loss_bbox_dn_5: 0.2572  loss_giou_dn_5: 0.5468  loss_ce_6: 0.6987  loss_mask_6: 0.3947  loss_dice_6: 1.807  loss_bbox_6: 0.3413  loss_giou_6: 0.8236  loss_ce_dn_6: 0.001359  loss_mask_dn_6: 0.3732  loss_dice_dn_6: 1.572  loss_bbox_dn_6: 0.255  loss_giou_dn_6: 0.5531  loss_ce_7: 0.6863  loss_mask_7: 0.3742  loss_dice_7: 1.852  loss_bbox_7: 0.3403  loss_giou_7: 0.7358  loss_ce_dn_7: 0.0008819  loss_mask_dn_7: 0.3648  loss_dice_dn_7: 1.561  loss_bbox_dn_7: 0.2554  loss_giou_dn_7: 0.5495  loss_ce_8: 0.6974  loss_mask_8: 0.386  loss_dice_8: 1.88  loss_bbox_8: 0.3251  loss_giou_8: 0.8103  loss_ce_dn_8: 0.0006496  loss_mask_dn_8: 0.3589  loss_dice_dn_8: 1.588  loss_bbox_dn_8: 0.2548  loss_giou_dn_8: 0.549  loss_ce_interm: 0.9817  loss_mask_interm: 0.3743  loss_dice_interm: 1.732  loss_bbox_interm: 0.4861  loss_giou_interm: 0.923    time: 0.4905  last_time: 0.4577  data_time: 0.0034  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:17:38 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:17:38 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:17:38 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:17:38 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:17:40 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0747 s/iter. Eval: 0.0106 s/iter. Total: 0.0863 s/iter. ETA=0:00:04\n",
      "[03/14 17:17:44 d2.evaluation.evaluator]: Total inference time: 0:00:05.448761 (0.087883 s / iter per device, on 1 devices)\n",
      "[03/14 17:17:44 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075198 s / iter per device, on 1 devices)\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.395\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.404\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.301\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.511\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.608\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.574\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.733\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 39.543 | 71.882 | 40.741 | 40.388 | 42.273 |  nan  |\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:17:45 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.200\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.634\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.177\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.307\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.291\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.288\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 19.975 | 63.434 | 3.332  | 17.705 | 30.669 |  nan  |\n",
      "[03/14 17:17:45 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:17:45 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: 39.5435,71.8821,40.7408,40.3880,42.2730,nan\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:17:45 d2.evaluation.testing]: copypaste: 19.9751,63.4345,3.3319,17.7048,30.6687,nan\n",
      "[03/14 17:17:45 d2.utils.events]:  eta: 0:18:28  iter: 1699  total_loss: 71.43  loss_ce: 0.5051  loss_mask: 0.3521  loss_dice: 1.384  loss_bbox: 0.3195  loss_giou: 0.6245  loss_ce_dn: 0.0002423  loss_mask_dn: 0.295  loss_dice_dn: 1.291  loss_bbox_dn: 0.2929  loss_giou_dn: 0.4902  loss_ce_0: 0.823  loss_mask_0: 0.3961  loss_dice_0: 1.411  loss_bbox_0: 0.7208  loss_giou_0: 0.9611  loss_ce_dn_0: 0.06145  loss_mask_dn_0: 0.6673  loss_dice_dn_0: 2.915  loss_bbox_dn_0: 0.788  loss_giou_dn_0: 0.8534  loss_ce_1: 0.9402  loss_mask_1: 0.3328  loss_dice_1: 1.295  loss_bbox_1: 0.3478  loss_giou_1: 0.6384  loss_ce_dn_1: 0.001593  loss_mask_dn_1: 0.2994  loss_dice_dn_1: 1.337  loss_bbox_dn_1: 0.3221  loss_giou_dn_1: 0.5423  loss_ce_2: 0.6574  loss_mask_2: 0.3466  loss_dice_2: 1.279  loss_bbox_2: 0.3161  loss_giou_2: 0.6183  loss_ce_dn_2: 0.0008339  loss_mask_dn_2: 0.3052  loss_dice_dn_2: 1.311  loss_bbox_dn_2: 0.3158  loss_giou_dn_2: 0.5081  loss_ce_3: 0.5198  loss_mask_3: 0.4335  loss_dice_3: 1.314  loss_bbox_3: 0.3461  loss_giou_3: 0.5858  loss_ce_dn_3: 0.000866  loss_mask_dn_3: 0.2983  loss_dice_dn_3: 1.291  loss_bbox_dn_3: 0.3047  loss_giou_dn_3: 0.4988  loss_ce_4: 0.4756  loss_mask_4: 0.4115  loss_dice_4: 1.276  loss_bbox_4: 0.336  loss_giou_4: 0.6001  loss_ce_dn_4: 0.0004816  loss_mask_dn_4: 0.2953  loss_dice_dn_4: 1.282  loss_bbox_dn_4: 0.2901  loss_giou_dn_4: 0.4907  loss_ce_5: 0.4991  loss_mask_5: 0.3493  loss_dice_5: 1.346  loss_bbox_5: 0.3391  loss_giou_5: 0.6155  loss_ce_dn_5: 0.0005699  loss_mask_dn_5: 0.2925  loss_dice_dn_5: 1.281  loss_bbox_dn_5: 0.2918  loss_giou_dn_5: 0.4838  loss_ce_6: 0.517  loss_mask_6: 0.3559  loss_dice_6: 1.332  loss_bbox_6: 0.3289  loss_giou_6: 0.5783  loss_ce_dn_6: 0.0004684  loss_mask_dn_6: 0.2986  loss_dice_dn_6: 1.282  loss_bbox_dn_6: 0.2918  loss_giou_dn_6: 0.4812  loss_ce_7: 0.5055  loss_mask_7: 0.3615  loss_dice_7: 1.335  loss_bbox_7: 0.3342  loss_giou_7: 0.6289  loss_ce_dn_7: 0.0002649  loss_mask_dn_7: 0.2914  loss_dice_dn_7: 1.281  loss_bbox_dn_7: 0.2866  loss_giou_dn_7: 0.4909  loss_ce_8: 0.5265  loss_mask_8: 0.3422  loss_dice_8: 1.345  loss_bbox_8: 0.3158  loss_giou_8: 0.6258  loss_ce_dn_8: 0.0002381  loss_mask_dn_8: 0.2913  loss_dice_dn_8: 1.283  loss_bbox_dn_8: 0.2942  loss_giou_dn_8: 0.4867  loss_ce_interm: 0.9312  loss_mask_interm: 0.3411  loss_dice_interm: 1.331  loss_bbox_interm: 0.4353  loss_giou_interm: 0.6696    time: 0.4904  last_time: 0.4723  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:17:54 d2.utils.events]:  eta: 0:18:20  iter: 1719  total_loss: 83.47  loss_ce: 0.6546  loss_mask: 0.3274  loss_dice: 1.521  loss_bbox: 0.3765  loss_giou: 0.7218  loss_ce_dn: 0.0003696  loss_mask_dn: 0.2953  loss_dice_dn: 1.619  loss_bbox_dn: 0.2682  loss_giou_dn: 0.5688  loss_ce_0: 1.239  loss_mask_0: 0.3301  loss_dice_0: 1.571  loss_bbox_0: 0.6929  loss_giou_0: 1.192  loss_ce_dn_0: 0.04771  loss_mask_dn_0: 0.675  loss_dice_dn_0: 3.489  loss_bbox_dn_0: 0.6989  loss_giou_dn_0: 0.8523  loss_ce_1: 1.101  loss_mask_1: 0.3307  loss_dice_1: 1.671  loss_bbox_1: 0.3963  loss_giou_1: 0.8302  loss_ce_dn_1: 0.0007782  loss_mask_dn_1: 0.3138  loss_dice_dn_1: 1.699  loss_bbox_dn_1: 0.3395  loss_giou_dn_1: 0.6452  loss_ce_2: 0.991  loss_mask_2: 0.3318  loss_dice_2: 1.634  loss_bbox_2: 0.3972  loss_giou_2: 0.7882  loss_ce_dn_2: 0.0007741  loss_mask_dn_2: 0.3213  loss_dice_dn_2: 1.612  loss_bbox_dn_2: 0.3127  loss_giou_dn_2: 0.6094  loss_ce_3: 1.002  loss_mask_3: 0.3133  loss_dice_3: 1.579  loss_bbox_3: 0.3778  loss_giou_3: 0.7973  loss_ce_dn_3: 0.0008001  loss_mask_dn_3: 0.3043  loss_dice_dn_3: 1.601  loss_bbox_dn_3: 0.2776  loss_giou_dn_3: 0.5919  loss_ce_4: 0.8983  loss_mask_4: 0.322  loss_dice_4: 1.589  loss_bbox_4: 0.3831  loss_giou_4: 0.7515  loss_ce_dn_4: 0.0005296  loss_mask_dn_4: 0.2983  loss_dice_dn_4: 1.57  loss_bbox_dn_4: 0.2864  loss_giou_dn_4: 0.5782  loss_ce_5: 0.8574  loss_mask_5: 0.3195  loss_dice_5: 1.564  loss_bbox_5: 0.4002  loss_giou_5: 0.747  loss_ce_dn_5: 0.0005054  loss_mask_dn_5: 0.2951  loss_dice_dn_5: 1.585  loss_bbox_dn_5: 0.2711  loss_giou_dn_5: 0.5773  loss_ce_6: 0.7683  loss_mask_6: 0.3623  loss_dice_6: 1.453  loss_bbox_6: 0.3815  loss_giou_6: 0.7918  loss_ce_dn_6: 0.0005387  loss_mask_dn_6: 0.3004  loss_dice_dn_6: 1.594  loss_bbox_dn_6: 0.2717  loss_giou_dn_6: 0.5692  loss_ce_7: 0.6857  loss_mask_7: 0.3287  loss_dice_7: 1.578  loss_bbox_7: 0.3872  loss_giou_7: 0.7653  loss_ce_dn_7: 0.0003595  loss_mask_dn_7: 0.3008  loss_dice_dn_7: 1.616  loss_bbox_dn_7: 0.266  loss_giou_dn_7: 0.5711  loss_ce_8: 0.757  loss_mask_8: 0.3361  loss_dice_8: 1.657  loss_bbox_8: 0.377  loss_giou_8: 0.7611  loss_ce_dn_8: 0.00042  loss_mask_dn_8: 0.2961  loss_dice_dn_8: 1.617  loss_bbox_dn_8: 0.2715  loss_giou_dn_8: 0.5694  loss_ce_interm: 1.221  loss_mask_interm: 0.3451  loss_dice_interm: 1.853  loss_bbox_interm: 0.5049  loss_giou_interm: 0.8038    time: 0.4902  last_time: 0.5035  data_time: 0.0035  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:18:04 d2.utils.events]:  eta: 0:18:10  iter: 1739  total_loss: 76.82  loss_ce: 0.5165  loss_mask: 0.5196  loss_dice: 1.491  loss_bbox: 0.4464  loss_giou: 0.7854  loss_ce_dn: 0.0006876  loss_mask_dn: 0.4395  loss_dice_dn: 1.467  loss_bbox_dn: 0.3552  loss_giou_dn: 0.552  loss_ce_0: 0.9395  loss_mask_0: 0.5315  loss_dice_0: 1.555  loss_bbox_0: 0.7328  loss_giou_0: 0.9362  loss_ce_dn_0: 0.07076  loss_mask_dn_0: 0.8385  loss_dice_dn_0: 2.646  loss_bbox_dn_0: 0.8515  loss_giou_dn_0: 0.859  loss_ce_1: 0.9904  loss_mask_1: 0.491  loss_dice_1: 1.547  loss_bbox_1: 0.4612  loss_giou_1: 0.6928  loss_ce_dn_1: 0.0008941  loss_mask_dn_1: 0.5209  loss_dice_dn_1: 1.498  loss_bbox_dn_1: 0.3976  loss_giou_dn_1: 0.5823  loss_ce_2: 0.8039  loss_mask_2: 0.5064  loss_dice_2: 1.496  loss_bbox_2: 0.4096  loss_giou_2: 0.7475  loss_ce_dn_2: 0.0008408  loss_mask_dn_2: 0.4565  loss_dice_dn_2: 1.47  loss_bbox_dn_2: 0.3588  loss_giou_dn_2: 0.5541  loss_ce_3: 0.6876  loss_mask_3: 0.4746  loss_dice_3: 1.442  loss_bbox_3: 0.4029  loss_giou_3: 0.7229  loss_ce_dn_3: 0.001162  loss_mask_dn_3: 0.4442  loss_dice_dn_3: 1.446  loss_bbox_dn_3: 0.338  loss_giou_dn_3: 0.5604  loss_ce_4: 0.6169  loss_mask_4: 0.4832  loss_dice_4: 1.459  loss_bbox_4: 0.4275  loss_giou_4: 0.7431  loss_ce_dn_4: 0.0007231  loss_mask_dn_4: 0.4411  loss_dice_dn_4: 1.434  loss_bbox_dn_4: 0.3374  loss_giou_dn_4: 0.5412  loss_ce_5: 0.6098  loss_mask_5: 0.5141  loss_dice_5: 1.474  loss_bbox_5: 0.4284  loss_giou_5: 0.7349  loss_ce_dn_5: 0.0008075  loss_mask_dn_5: 0.4326  loss_dice_dn_5: 1.459  loss_bbox_dn_5: 0.3485  loss_giou_dn_5: 0.5421  loss_ce_6: 0.5239  loss_mask_6: 0.5061  loss_dice_6: 1.486  loss_bbox_6: 0.4295  loss_giou_6: 0.7465  loss_ce_dn_6: 0.0006263  loss_mask_dn_6: 0.4329  loss_dice_dn_6: 1.463  loss_bbox_dn_6: 0.3549  loss_giou_dn_6: 0.5446  loss_ce_7: 0.5363  loss_mask_7: 0.5114  loss_dice_7: 1.463  loss_bbox_7: 0.4438  loss_giou_7: 0.7597  loss_ce_dn_7: 0.0005913  loss_mask_dn_7: 0.4328  loss_dice_dn_7: 1.461  loss_bbox_dn_7: 0.3532  loss_giou_dn_7: 0.55  loss_ce_8: 0.5132  loss_mask_8: 0.5135  loss_dice_8: 1.441  loss_bbox_8: 0.4501  loss_giou_8: 0.7775  loss_ce_dn_8: 0.0006298  loss_mask_dn_8: 0.4359  loss_dice_dn_8: 1.462  loss_bbox_dn_8: 0.353  loss_giou_dn_8: 0.5475  loss_ce_interm: 0.9303  loss_mask_interm: 0.5454  loss_dice_interm: 1.484  loss_bbox_interm: 0.5596  loss_giou_interm: 0.7885    time: 0.4901  last_time: 0.4615  data_time: 0.0036  last_data_time: 0.0034   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:18:14 d2.utils.events]:  eta: 0:18:00  iter: 1759  total_loss: 76.27  loss_ce: 0.6219  loss_mask: 0.2935  loss_dice: 1.759  loss_bbox: 0.3418  loss_giou: 0.7586  loss_ce_dn: 0.0004294  loss_mask_dn: 0.2831  loss_dice_dn: 1.599  loss_bbox_dn: 0.2648  loss_giou_dn: 0.4922  loss_ce_0: 0.8927  loss_mask_0: 0.3114  loss_dice_0: 1.693  loss_bbox_0: 0.6709  loss_giou_0: 1.088  loss_ce_dn_0: 0.04715  loss_mask_dn_0: 0.5388  loss_dice_dn_0: 3.017  loss_bbox_dn_0: 0.6264  loss_giou_dn_0: 0.8532  loss_ce_1: 1.071  loss_mask_1: 0.3202  loss_dice_1: 1.712  loss_bbox_1: 0.3626  loss_giou_1: 0.764  loss_ce_dn_1: 0.0007427  loss_mask_dn_1: 0.2918  loss_dice_dn_1: 1.623  loss_bbox_dn_1: 0.3221  loss_giou_dn_1: 0.5666  loss_ce_2: 0.9724  loss_mask_2: 0.3032  loss_dice_2: 1.731  loss_bbox_2: 0.3178  loss_giou_2: 0.7587  loss_ce_dn_2: 0.0008132  loss_mask_dn_2: 0.2913  loss_dice_dn_2: 1.7  loss_bbox_dn_2: 0.2814  loss_giou_dn_2: 0.5178  loss_ce_3: 0.7041  loss_mask_3: 0.3192  loss_dice_3: 1.697  loss_bbox_3: 0.3523  loss_giou_3: 0.777  loss_ce_dn_3: 0.0008286  loss_mask_dn_3: 0.2876  loss_dice_dn_3: 1.691  loss_bbox_dn_3: 0.2714  loss_giou_dn_3: 0.5019  loss_ce_4: 0.8164  loss_mask_4: 0.3138  loss_dice_4: 1.729  loss_bbox_4: 0.3594  loss_giou_4: 0.8059  loss_ce_dn_4: 0.0003741  loss_mask_dn_4: 0.2867  loss_dice_dn_4: 1.609  loss_bbox_dn_4: 0.2705  loss_giou_dn_4: 0.4931  loss_ce_5: 0.603  loss_mask_5: 0.3063  loss_dice_5: 1.763  loss_bbox_5: 0.3616  loss_giou_5: 0.7874  loss_ce_dn_5: 0.0004953  loss_mask_dn_5: 0.2801  loss_dice_dn_5: 1.641  loss_bbox_dn_5: 0.267  loss_giou_dn_5: 0.4893  loss_ce_6: 0.6252  loss_mask_6: 0.3055  loss_dice_6: 1.691  loss_bbox_6: 0.3578  loss_giou_6: 0.8022  loss_ce_dn_6: 0.0004993  loss_mask_dn_6: 0.2831  loss_dice_dn_6: 1.625  loss_bbox_dn_6: 0.2623  loss_giou_dn_6: 0.4799  loss_ce_7: 0.6137  loss_mask_7: 0.2957  loss_dice_7: 1.706  loss_bbox_7: 0.3676  loss_giou_7: 0.8065  loss_ce_dn_7: 0.000396  loss_mask_dn_7: 0.2841  loss_dice_dn_7: 1.605  loss_bbox_dn_7: 0.2684  loss_giou_dn_7: 0.489  loss_ce_8: 0.6306  loss_mask_8: 0.2993  loss_dice_8: 1.782  loss_bbox_8: 0.3595  loss_giou_8: 0.7589  loss_ce_dn_8: 0.0004226  loss_mask_dn_8: 0.2822  loss_dice_dn_8: 1.605  loss_bbox_dn_8: 0.2658  loss_giou_dn_8: 0.4908  loss_ce_interm: 0.9439  loss_mask_interm: 0.33  loss_dice_interm: 1.681  loss_bbox_interm: 0.478  loss_giou_interm: 0.8562    time: 0.4901  last_time: 0.4781  data_time: 0.0036  last_data_time: 0.0037   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:18:23 d2.utils.events]:  eta: 0:17:50  iter: 1779  total_loss: 72.34  loss_ce: 0.5185  loss_mask: 0.4098  loss_dice: 1.541  loss_bbox: 0.3888  loss_giou: 0.7014  loss_ce_dn: 0.000292  loss_mask_dn: 0.4477  loss_dice_dn: 1.392  loss_bbox_dn: 0.2905  loss_giou_dn: 0.5114  loss_ce_0: 0.9261  loss_mask_0: 0.4442  loss_dice_0: 1.457  loss_bbox_0: 0.6867  loss_giou_0: 1.001  loss_ce_dn_0: 0.02378  loss_mask_dn_0: 0.5255  loss_dice_dn_0: 2.695  loss_bbox_dn_0: 0.7802  loss_giou_dn_0: 0.8488  loss_ce_1: 0.8924  loss_mask_1: 0.4437  loss_dice_1: 1.557  loss_bbox_1: 0.4004  loss_giou_1: 0.6716  loss_ce_dn_1: 0.0008921  loss_mask_dn_1: 0.4124  loss_dice_dn_1: 1.491  loss_bbox_dn_1: 0.3478  loss_giou_dn_1: 0.5463  loss_ce_2: 0.716  loss_mask_2: 0.4146  loss_dice_2: 1.48  loss_bbox_2: 0.4185  loss_giou_2: 0.7147  loss_ce_dn_2: 0.000845  loss_mask_dn_2: 0.4243  loss_dice_dn_2: 1.408  loss_bbox_dn_2: 0.3227  loss_giou_dn_2: 0.5025  loss_ce_3: 0.5797  loss_mask_3: 0.4477  loss_dice_3: 1.505  loss_bbox_3: 0.4544  loss_giou_3: 0.7054  loss_ce_dn_3: 0.0011  loss_mask_dn_3: 0.4337  loss_dice_dn_3: 1.414  loss_bbox_dn_3: 0.3067  loss_giou_dn_3: 0.5197  loss_ce_4: 0.5005  loss_mask_4: 0.4269  loss_dice_4: 1.575  loss_bbox_4: 0.3999  loss_giou_4: 0.6899  loss_ce_dn_4: 0.0006477  loss_mask_dn_4: 0.4321  loss_dice_dn_4: 1.392  loss_bbox_dn_4: 0.2976  loss_giou_dn_4: 0.5086  loss_ce_5: 0.5084  loss_mask_5: 0.4327  loss_dice_5: 1.588  loss_bbox_5: 0.4119  loss_giou_5: 0.6782  loss_ce_dn_5: 0.0006489  loss_mask_dn_5: 0.4305  loss_dice_dn_5: 1.392  loss_bbox_dn_5: 0.2893  loss_giou_dn_5: 0.5058  loss_ce_6: 0.4915  loss_mask_6: 0.4391  loss_dice_6: 1.537  loss_bbox_6: 0.4093  loss_giou_6: 0.6926  loss_ce_dn_6: 0.0005602  loss_mask_dn_6: 0.4361  loss_dice_dn_6: 1.38  loss_bbox_dn_6: 0.2887  loss_giou_dn_6: 0.508  loss_ce_7: 0.5203  loss_mask_7: 0.4354  loss_dice_7: 1.599  loss_bbox_7: 0.4039  loss_giou_7: 0.6798  loss_ce_dn_7: 0.0004446  loss_mask_dn_7: 0.4464  loss_dice_dn_7: 1.38  loss_bbox_dn_7: 0.2883  loss_giou_dn_7: 0.5093  loss_ce_8: 0.5181  loss_mask_8: 0.4367  loss_dice_8: 1.576  loss_bbox_8: 0.3834  loss_giou_8: 0.7018  loss_ce_dn_8: 0.0002791  loss_mask_dn_8: 0.4456  loss_dice_dn_8: 1.393  loss_bbox_dn_8: 0.2967  loss_giou_dn_8: 0.5112  loss_ce_interm: 0.8983  loss_mask_interm: 0.4408  loss_dice_interm: 1.471  loss_bbox_interm: 0.4801  loss_giou_interm: 0.8677    time: 0.4899  last_time: 0.4399  data_time: 0.0035  last_data_time: 0.0031   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:18:33 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:18:33 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:18:33 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:18:33 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:18:35 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0015 s/iter. Inference: 0.0808 s/iter. Eval: 0.0113 s/iter. Total: 0.0936 s/iter. ETA=0:00:05\n",
      "[03/14 17:18:40 d2.evaluation.evaluator]: Total inference time: 0:00:05.558260 (0.089649 s / iter per device, on 1 devices)\n",
      "[03/14 17:18:40 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077843 s / iter per device, on 1 devices)\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.394\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.353\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.416\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.539\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.605\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 39.355 | 72.363 | 35.313 | 41.588 | 44.310 |  nan  |\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:18:40 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.175\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.282\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 17.506 | 52.625 | 1.961  | 19.100 | 15.655 |  nan  |\n",
      "[03/14 17:18:40 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:18:40 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: 39.3548,72.3631,35.3133,41.5876,44.3096,nan\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:18:40 d2.evaluation.testing]: copypaste: 17.5057,52.6248,1.9607,19.1001,15.6551,nan\n",
      "[03/14 17:18:40 d2.utils.events]:  eta: 0:17:40  iter: 1799  total_loss: 71.6  loss_ce: 0.5661  loss_mask: 0.3733  loss_dice: 1.408  loss_bbox: 0.3948  loss_giou: 0.7027  loss_ce_dn: 0.0002737  loss_mask_dn: 0.3466  loss_dice_dn: 1.414  loss_bbox_dn: 0.2874  loss_giou_dn: 0.4956  loss_ce_0: 1.01  loss_mask_0: 0.3508  loss_dice_0: 1.403  loss_bbox_0: 0.6588  loss_giou_0: 1.037  loss_ce_dn_0: 0.0693  loss_mask_dn_0: 0.637  loss_dice_dn_0: 3.044  loss_bbox_dn_0: 0.6918  loss_giou_dn_0: 0.8519  loss_ce_1: 1.04  loss_mask_1: 0.3753  loss_dice_1: 1.406  loss_bbox_1: 0.4177  loss_giou_1: 0.7272  loss_ce_dn_1: 0.001881  loss_mask_dn_1: 0.3604  loss_dice_dn_1: 1.403  loss_bbox_dn_1: 0.3804  loss_giou_dn_1: 0.5705  loss_ce_2: 0.8959  loss_mask_2: 0.3958  loss_dice_2: 1.395  loss_bbox_2: 0.438  loss_giou_2: 0.7088  loss_ce_dn_2: 0.000998  loss_mask_dn_2: 0.3576  loss_dice_dn_2: 1.405  loss_bbox_dn_2: 0.3182  loss_giou_dn_2: 0.5364  loss_ce_3: 0.7875  loss_mask_3: 0.4029  loss_dice_3: 1.371  loss_bbox_3: 0.4183  loss_giou_3: 0.6945  loss_ce_dn_3: 0.001073  loss_mask_dn_3: 0.354  loss_dice_dn_3: 1.471  loss_bbox_dn_3: 0.2892  loss_giou_dn_3: 0.5158  loss_ce_4: 0.7249  loss_mask_4: 0.3311  loss_dice_4: 1.336  loss_bbox_4: 0.3677  loss_giou_4: 0.6814  loss_ce_dn_4: 0.0008293  loss_mask_dn_4: 0.3505  loss_dice_dn_4: 1.429  loss_bbox_dn_4: 0.2908  loss_giou_dn_4: 0.5156  loss_ce_5: 0.7636  loss_mask_5: 0.3488  loss_dice_5: 1.41  loss_bbox_5: 0.3824  loss_giou_5: 0.696  loss_ce_dn_5: 0.000663  loss_mask_dn_5: 0.3564  loss_dice_dn_5: 1.437  loss_bbox_dn_5: 0.2751  loss_giou_dn_5: 0.5079  loss_ce_6: 0.7461  loss_mask_6: 0.3824  loss_dice_6: 1.384  loss_bbox_6: 0.3977  loss_giou_6: 0.716  loss_ce_dn_6: 0.000631  loss_mask_dn_6: 0.356  loss_dice_dn_6: 1.458  loss_bbox_dn_6: 0.2836  loss_giou_dn_6: 0.4973  loss_ce_7: 0.6546  loss_mask_7: 0.3736  loss_dice_7: 1.4  loss_bbox_7: 0.405  loss_giou_7: 0.6963  loss_ce_dn_7: 0.000431  loss_mask_dn_7: 0.3498  loss_dice_dn_7: 1.41  loss_bbox_dn_7: 0.281  loss_giou_dn_7: 0.4937  loss_ce_8: 0.5869  loss_mask_8: 0.3611  loss_dice_8: 1.361  loss_bbox_8: 0.3915  loss_giou_8: 0.7044  loss_ce_dn_8: 0.0003679  loss_mask_dn_8: 0.3426  loss_dice_dn_8: 1.401  loss_bbox_dn_8: 0.2891  loss_giou_dn_8: 0.4906  loss_ce_interm: 0.9996  loss_mask_interm: 0.3908  loss_dice_interm: 1.336  loss_bbox_interm: 0.3877  loss_giou_interm: 0.7355    time: 0.4897  last_time: 0.5310  data_time: 0.0035  last_data_time: 0.0045   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:18:50 d2.utils.events]:  eta: 0:17:31  iter: 1819  total_loss: 68.22  loss_ce: 0.5487  loss_mask: 0.3581  loss_dice: 1.294  loss_bbox: 0.3485  loss_giou: 0.5845  loss_ce_dn: 0.0003528  loss_mask_dn: 0.384  loss_dice_dn: 1.337  loss_bbox_dn: 0.2705  loss_giou_dn: 0.4445  loss_ce_0: 1.078  loss_mask_0: 0.3747  loss_dice_0: 1.439  loss_bbox_0: 0.6211  loss_giou_0: 0.8107  loss_ce_dn_0: 0.02316  loss_mask_dn_0: 0.7666  loss_dice_dn_0: 3.112  loss_bbox_dn_0: 0.78  loss_giou_dn_0: 0.8454  loss_ce_1: 0.9212  loss_mask_1: 0.4057  loss_dice_1: 1.481  loss_bbox_1: 0.4355  loss_giou_1: 0.6722  loss_ce_dn_1: 0.001353  loss_mask_dn_1: 0.3583  loss_dice_dn_1: 1.404  loss_bbox_dn_1: 0.3299  loss_giou_dn_1: 0.5339  loss_ce_2: 0.722  loss_mask_2: 0.3774  loss_dice_2: 1.386  loss_bbox_2: 0.4005  loss_giou_2: 0.7061  loss_ce_dn_2: 0.001001  loss_mask_dn_2: 0.3785  loss_dice_dn_2: 1.391  loss_bbox_dn_2: 0.3004  loss_giou_dn_2: 0.5031  loss_ce_3: 0.5  loss_mask_3: 0.3626  loss_dice_3: 1.415  loss_bbox_3: 0.4822  loss_giou_3: 0.7259  loss_ce_dn_3: 0.0009852  loss_mask_dn_3: 0.3762  loss_dice_dn_3: 1.371  loss_bbox_dn_3: 0.2883  loss_giou_dn_3: 0.4685  loss_ce_4: 0.4096  loss_mask_4: 0.3604  loss_dice_4: 1.372  loss_bbox_4: 0.3662  loss_giou_4: 0.6823  loss_ce_dn_4: 0.0006255  loss_mask_dn_4: 0.3829  loss_dice_dn_4: 1.366  loss_bbox_dn_4: 0.2701  loss_giou_dn_4: 0.4535  loss_ce_5: 0.4744  loss_mask_5: 0.3581  loss_dice_5: 1.401  loss_bbox_5: 0.3838  loss_giou_5: 0.6106  loss_ce_dn_5: 0.0004236  loss_mask_dn_5: 0.3848  loss_dice_dn_5: 1.348  loss_bbox_dn_5: 0.2716  loss_giou_dn_5: 0.4448  loss_ce_6: 0.5179  loss_mask_6: 0.3717  loss_dice_6: 1.449  loss_bbox_6: 0.4315  loss_giou_6: 0.6324  loss_ce_dn_6: 0.0004704  loss_mask_dn_6: 0.3799  loss_dice_dn_6: 1.31  loss_bbox_dn_6: 0.2658  loss_giou_dn_6: 0.4379  loss_ce_7: 0.5014  loss_mask_7: 0.3444  loss_dice_7: 1.366  loss_bbox_7: 0.3663  loss_giou_7: 0.5926  loss_ce_dn_7: 0.0004276  loss_mask_dn_7: 0.3813  loss_dice_dn_7: 1.326  loss_bbox_dn_7: 0.2629  loss_giou_dn_7: 0.4404  loss_ce_8: 0.5214  loss_mask_8: 0.3524  loss_dice_8: 1.346  loss_bbox_8: 0.3779  loss_giou_8: 0.5845  loss_ce_dn_8: 0.000406  loss_mask_dn_8: 0.3878  loss_dice_dn_8: 1.334  loss_bbox_dn_8: 0.2714  loss_giou_dn_8: 0.4392  loss_ce_interm: 1.284  loss_mask_interm: 0.3976  loss_dice_interm: 1.434  loss_bbox_interm: 0.4492  loss_giou_interm: 0.7143    time: 0.4896  last_time: 0.4686  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:00 d2.utils.events]:  eta: 0:17:22  iter: 1839  total_loss: 69.05  loss_ce: 0.5374  loss_mask: 0.4204  loss_dice: 1.457  loss_bbox: 0.3572  loss_giou: 0.5508  loss_ce_dn: 0.0004077  loss_mask_dn: 0.4166  loss_dice_dn: 1.412  loss_bbox_dn: 0.3012  loss_giou_dn: 0.4544  loss_ce_0: 0.9102  loss_mask_0: 0.4321  loss_dice_0: 1.34  loss_bbox_0: 0.8171  loss_giou_0: 1.027  loss_ce_dn_0: 0.06754  loss_mask_dn_0: 0.6774  loss_dice_dn_0: 2.873  loss_bbox_dn_0: 0.9199  loss_giou_dn_0: 0.8532  loss_ce_1: 0.9641  loss_mask_1: 0.4228  loss_dice_1: 1.435  loss_bbox_1: 0.3813  loss_giou_1: 0.6261  loss_ce_dn_1: 0.001296  loss_mask_dn_1: 0.4376  loss_dice_dn_1: 1.408  loss_bbox_dn_1: 0.3424  loss_giou_dn_1: 0.5099  loss_ce_2: 0.9822  loss_mask_2: 0.4545  loss_dice_2: 1.482  loss_bbox_2: 0.3468  loss_giou_2: 0.6092  loss_ce_dn_2: 0.0006172  loss_mask_dn_2: 0.4301  loss_dice_dn_2: 1.409  loss_bbox_dn_2: 0.2902  loss_giou_dn_2: 0.4775  loss_ce_3: 0.8128  loss_mask_3: 0.3968  loss_dice_3: 1.503  loss_bbox_3: 0.385  loss_giou_3: 0.5893  loss_ce_dn_3: 0.0007536  loss_mask_dn_3: 0.4109  loss_dice_dn_3: 1.382  loss_bbox_dn_3: 0.2936  loss_giou_dn_3: 0.472  loss_ce_4: 0.6361  loss_mask_4: 0.407  loss_dice_4: 1.478  loss_bbox_4: 0.3686  loss_giou_4: 0.5267  loss_ce_dn_4: 0.0005889  loss_mask_dn_4: 0.4175  loss_dice_dn_4: 1.341  loss_bbox_dn_4: 0.3016  loss_giou_dn_4: 0.4607  loss_ce_5: 0.576  loss_mask_5: 0.4118  loss_dice_5: 1.421  loss_bbox_5: 0.3756  loss_giou_5: 0.5173  loss_ce_dn_5: 0.0004136  loss_mask_dn_5: 0.4193  loss_dice_dn_5: 1.354  loss_bbox_dn_5: 0.3002  loss_giou_dn_5: 0.459  loss_ce_6: 0.564  loss_mask_6: 0.4395  loss_dice_6: 1.492  loss_bbox_6: 0.3598  loss_giou_6: 0.5462  loss_ce_dn_6: 0.0003688  loss_mask_dn_6: 0.4252  loss_dice_dn_6: 1.371  loss_bbox_dn_6: 0.3037  loss_giou_dn_6: 0.4553  loss_ce_7: 0.5413  loss_mask_7: 0.4116  loss_dice_7: 1.435  loss_bbox_7: 0.3468  loss_giou_7: 0.5102  loss_ce_dn_7: 0.0004205  loss_mask_dn_7: 0.4232  loss_dice_dn_7: 1.395  loss_bbox_dn_7: 0.3063  loss_giou_dn_7: 0.4505  loss_ce_8: 0.5193  loss_mask_8: 0.4  loss_dice_8: 1.414  loss_bbox_8: 0.3541  loss_giou_8: 0.5292  loss_ce_dn_8: 0.0003728  loss_mask_dn_8: 0.4215  loss_dice_dn_8: 1.389  loss_bbox_dn_8: 0.3003  loss_giou_dn_8: 0.4533  loss_ce_interm: 0.9845  loss_mask_interm: 0.4577  loss_dice_interm: 1.391  loss_bbox_interm: 0.5028  loss_giou_interm: 0.7096    time: 0.4896  last_time: 0.5621  data_time: 0.0035  last_data_time: 0.0033   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:09 d2.utils.events]:  eta: 0:17:13  iter: 1859  total_loss: 72.44  loss_ce: 0.5158  loss_mask: 0.4508  loss_dice: 1.438  loss_bbox: 0.3759  loss_giou: 0.4715  loss_ce_dn: 0.0006979  loss_mask_dn: 0.446  loss_dice_dn: 1.407  loss_bbox_dn: 0.2351  loss_giou_dn: 0.4331  loss_ce_0: 0.8099  loss_mask_0: 0.4781  loss_dice_0: 1.612  loss_bbox_0: 0.7347  loss_giou_0: 1.07  loss_ce_dn_0: 0.02248  loss_mask_dn_0: 0.6149  loss_dice_dn_0: 2.667  loss_bbox_dn_0: 0.7936  loss_giou_dn_0: 0.8474  loss_ce_1: 0.8932  loss_mask_1: 0.4851  loss_dice_1: 1.538  loss_bbox_1: 0.4209  loss_giou_1: 0.5545  loss_ce_dn_1: 0.001042  loss_mask_dn_1: 0.4613  loss_dice_dn_1: 1.438  loss_bbox_dn_1: 0.3496  loss_giou_dn_1: 0.5103  loss_ce_2: 0.7435  loss_mask_2: 0.4782  loss_dice_2: 1.435  loss_bbox_2: 0.3701  loss_giou_2: 0.5643  loss_ce_dn_2: 0.0007647  loss_mask_dn_2: 0.4547  loss_dice_dn_2: 1.406  loss_bbox_dn_2: 0.2698  loss_giou_dn_2: 0.4599  loss_ce_3: 0.5829  loss_mask_3: 0.4678  loss_dice_3: 1.47  loss_bbox_3: 0.403  loss_giou_3: 0.4862  loss_ce_dn_3: 0.0006839  loss_mask_dn_3: 0.4562  loss_dice_dn_3: 1.397  loss_bbox_dn_3: 0.2535  loss_giou_dn_3: 0.406  loss_ce_4: 0.5479  loss_mask_4: 0.445  loss_dice_4: 1.462  loss_bbox_4: 0.4016  loss_giou_4: 0.4864  loss_ce_dn_4: 0.0004708  loss_mask_dn_4: 0.4467  loss_dice_dn_4: 1.399  loss_bbox_dn_4: 0.2443  loss_giou_dn_4: 0.4429  loss_ce_5: 0.5217  loss_mask_5: 0.4826  loss_dice_5: 1.474  loss_bbox_5: 0.3918  loss_giou_5: 0.5026  loss_ce_dn_5: 0.0005921  loss_mask_dn_5: 0.4544  loss_dice_dn_5: 1.404  loss_bbox_dn_5: 0.2312  loss_giou_dn_5: 0.413  loss_ce_6: 0.4931  loss_mask_6: 0.4505  loss_dice_6: 1.471  loss_bbox_6: 0.3952  loss_giou_6: 0.5029  loss_ce_dn_6: 0.0005178  loss_mask_dn_6: 0.4367  loss_dice_dn_6: 1.414  loss_bbox_dn_6: 0.23  loss_giou_dn_6: 0.4373  loss_ce_7: 0.5222  loss_mask_7: 0.4484  loss_dice_7: 1.498  loss_bbox_7: 0.3841  loss_giou_7: 0.4824  loss_ce_dn_7: 0.0005819  loss_mask_dn_7: 0.4472  loss_dice_dn_7: 1.403  loss_bbox_dn_7: 0.2388  loss_giou_dn_7: 0.426  loss_ce_8: 0.5172  loss_mask_8: 0.4518  loss_dice_8: 1.454  loss_bbox_8: 0.3811  loss_giou_8: 0.4681  loss_ce_dn_8: 0.0006559  loss_mask_dn_8: 0.4437  loss_dice_dn_8: 1.415  loss_bbox_dn_8: 0.2352  loss_giou_dn_8: 0.4298  loss_ce_interm: 0.8996  loss_mask_interm: 0.4287  loss_dice_interm: 1.527  loss_bbox_interm: 0.3614  loss_giou_interm: 0.6122    time: 0.4896  last_time: 0.4957  data_time: 0.0036  last_data_time: 0.0036   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:19 d2.utils.events]:  eta: 0:17:03  iter: 1879  total_loss: 71.01  loss_ce: 0.4637  loss_mask: 0.4396  loss_dice: 1.352  loss_bbox: 0.3985  loss_giou: 0.6196  loss_ce_dn: 0.0006468  loss_mask_dn: 0.4339  loss_dice_dn: 1.382  loss_bbox_dn: 0.339  loss_giou_dn: 0.4783  loss_ce_0: 0.9354  loss_mask_0: 0.4588  loss_dice_0: 1.279  loss_bbox_0: 0.7835  loss_giou_0: 0.9853  loss_ce_dn_0: 0.05784  loss_mask_dn_0: 0.6262  loss_dice_dn_0: 2.971  loss_bbox_dn_0: 0.9109  loss_giou_dn_0: 0.8545  loss_ce_1: 0.8686  loss_mask_1: 0.4262  loss_dice_1: 1.3  loss_bbox_1: 0.4549  loss_giou_1: 0.6591  loss_ce_dn_1: 0.001114  loss_mask_dn_1: 0.4199  loss_dice_dn_1: 1.288  loss_bbox_dn_1: 0.4061  loss_giou_dn_1: 0.5437  loss_ce_2: 0.753  loss_mask_2: 0.4056  loss_dice_2: 1.281  loss_bbox_2: 0.3986  loss_giou_2: 0.5741  loss_ce_dn_2: 0.0006818  loss_mask_dn_2: 0.4191  loss_dice_dn_2: 1.291  loss_bbox_dn_2: 0.3557  loss_giou_dn_2: 0.4837  loss_ce_3: 0.6335  loss_mask_3: 0.4269  loss_dice_3: 1.277  loss_bbox_3: 0.4062  loss_giou_3: 0.6149  loss_ce_dn_3: 0.0004755  loss_mask_dn_3: 0.4335  loss_dice_dn_3: 1.301  loss_bbox_dn_3: 0.343  loss_giou_dn_3: 0.4791  loss_ce_4: 0.5146  loss_mask_4: 0.4324  loss_dice_4: 1.349  loss_bbox_4: 0.4324  loss_giou_4: 0.6109  loss_ce_dn_4: 0.0004506  loss_mask_dn_4: 0.4395  loss_dice_dn_4: 1.273  loss_bbox_dn_4: 0.3342  loss_giou_dn_4: 0.4783  loss_ce_5: 0.5738  loss_mask_5: 0.4389  loss_dice_5: 1.38  loss_bbox_5: 0.4124  loss_giou_5: 0.6237  loss_ce_dn_5: 0.0005811  loss_mask_dn_5: 0.4323  loss_dice_dn_5: 1.336  loss_bbox_dn_5: 0.332  loss_giou_dn_5: 0.4746  loss_ce_6: 0.5021  loss_mask_6: 0.4506  loss_dice_6: 1.308  loss_bbox_6: 0.4128  loss_giou_6: 0.6193  loss_ce_dn_6: 0.0006415  loss_mask_dn_6: 0.4271  loss_dice_dn_6: 1.341  loss_bbox_dn_6: 0.3288  loss_giou_dn_6: 0.4779  loss_ce_7: 0.4356  loss_mask_7: 0.447  loss_dice_7: 1.352  loss_bbox_7: 0.4099  loss_giou_7: 0.6187  loss_ce_dn_7: 0.0007284  loss_mask_dn_7: 0.4306  loss_dice_dn_7: 1.363  loss_bbox_dn_7: 0.3273  loss_giou_dn_7: 0.4773  loss_ce_8: 0.4617  loss_mask_8: 0.4451  loss_dice_8: 1.34  loss_bbox_8: 0.4037  loss_giou_8: 0.6235  loss_ce_dn_8: 0.0005707  loss_mask_dn_8: 0.4318  loss_dice_dn_8: 1.355  loss_bbox_dn_8: 0.3368  loss_giou_dn_8: 0.4725  loss_ce_interm: 0.8517  loss_mask_interm: 0.4242  loss_dice_interm: 1.239  loss_bbox_interm: 0.6159  loss_giou_interm: 0.7186    time: 0.4894  last_time: 0.4487  data_time: 0.0035  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:29 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:19:29 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:19:29 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:19:29 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:19:30 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0800 s/iter. Eval: 0.0097 s/iter. Total: 0.0906 s/iter. ETA=0:00:05\n",
      "[03/14 17:19:35 d2.evaluation.evaluator]: Total inference time: 0:00:05.544029 (0.089420 s / iter per device, on 1 devices)\n",
      "[03/14 17:19:35 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077527 s / iter per device, on 1 devices)\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.421\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.732\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.475\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.401\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.572\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.512\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.587\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.561\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.681\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 42.062 | 73.194 | 47.473 | 40.124 | 57.242 |  nan  |\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:19:35 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.223\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.652\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.204\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.298\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.341\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 22.318 | 65.181 | 3.146  | 20.378 | 29.998 |  nan  |\n",
      "[03/14 17:19:35 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:19:35 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: 42.0620,73.1937,47.4734,40.1241,57.2418,nan\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:19:35 d2.evaluation.testing]: copypaste: 22.3177,65.1809,3.1462,20.3776,29.9981,nan\n",
      "[03/14 17:19:36 d2.utils.events]:  eta: 0:16:53  iter: 1899  total_loss: 71.77  loss_ce: 0.3589  loss_mask: 0.3564  loss_dice: 1.332  loss_bbox: 0.3258  loss_giou: 0.6318  loss_ce_dn: 0.0003116  loss_mask_dn: 0.3965  loss_dice_dn: 1.32  loss_bbox_dn: 0.2714  loss_giou_dn: 0.4842  loss_ce_0: 0.8506  loss_mask_0: 0.3695  loss_dice_0: 1.344  loss_bbox_0: 0.6953  loss_giou_0: 0.9904  loss_ce_dn_0: 0.03508  loss_mask_dn_0: 0.5926  loss_dice_dn_0: 2.949  loss_bbox_dn_0: 0.8149  loss_giou_dn_0: 0.8562  loss_ce_1: 0.8937  loss_mask_1: 0.4129  loss_dice_1: 1.364  loss_bbox_1: 0.3839  loss_giou_1: 0.7148  loss_ce_dn_1: 0.0009353  loss_mask_dn_1: 0.3825  loss_dice_dn_1: 1.236  loss_bbox_dn_1: 0.3429  loss_giou_dn_1: 0.5335  loss_ce_2: 0.6272  loss_mask_2: 0.382  loss_dice_2: 1.23  loss_bbox_2: 0.3283  loss_giou_2: 0.6173  loss_ce_dn_2: 0.0009081  loss_mask_dn_2: 0.4057  loss_dice_dn_2: 1.322  loss_bbox_dn_2: 0.3002  loss_giou_dn_2: 0.4986  loss_ce_3: 0.4577  loss_mask_3: 0.439  loss_dice_3: 1.465  loss_bbox_3: 0.3334  loss_giou_3: 0.6221  loss_ce_dn_3: 0.0005535  loss_mask_dn_3: 0.3963  loss_dice_dn_3: 1.264  loss_bbox_dn_3: 0.2764  loss_giou_dn_3: 0.4729  loss_ce_4: 0.3168  loss_mask_4: 0.4079  loss_dice_4: 1.394  loss_bbox_4: 0.3177  loss_giou_4: 0.635  loss_ce_dn_4: 0.0004114  loss_mask_dn_4: 0.4001  loss_dice_dn_4: 1.278  loss_bbox_dn_4: 0.2688  loss_giou_dn_4: 0.4749  loss_ce_5: 0.2894  loss_mask_5: 0.4017  loss_dice_5: 1.399  loss_bbox_5: 0.3307  loss_giou_5: 0.6183  loss_ce_dn_5: 0.0004431  loss_mask_dn_5: 0.3938  loss_dice_dn_5: 1.304  loss_bbox_dn_5: 0.2649  loss_giou_dn_5: 0.4783  loss_ce_6: 0.2687  loss_mask_6: 0.4267  loss_dice_6: 1.337  loss_bbox_6: 0.3402  loss_giou_6: 0.6424  loss_ce_dn_6: 0.000389  loss_mask_dn_6: 0.4015  loss_dice_dn_6: 1.306  loss_bbox_dn_6: 0.2703  loss_giou_dn_6: 0.4832  loss_ce_7: 0.3531  loss_mask_7: 0.3696  loss_dice_7: 1.284  loss_bbox_7: 0.3329  loss_giou_7: 0.6323  loss_ce_dn_7: 0.0003503  loss_mask_dn_7: 0.4134  loss_dice_dn_7: 1.315  loss_bbox_dn_7: 0.2686  loss_giou_dn_7: 0.4769  loss_ce_8: 0.3571  loss_mask_8: 0.3668  loss_dice_8: 1.354  loss_bbox_8: 0.3245  loss_giou_8: 0.631  loss_ce_dn_8: 0.0002873  loss_mask_dn_8: 0.4127  loss_dice_dn_8: 1.303  loss_bbox_dn_8: 0.2729  loss_giou_dn_8: 0.4819  loss_ce_interm: 0.9199  loss_mask_interm: 0.3733  loss_dice_interm: 1.399  loss_bbox_interm: 0.4019  loss_giou_interm: 0.7836    time: 0.4893  last_time: 0.4223  data_time: 0.0035  last_data_time: 0.0042   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:45 d2.utils.events]:  eta: 0:16:43  iter: 1919  total_loss: 64.11  loss_ce: 0.1365  loss_mask: 0.478  loss_dice: 1.173  loss_bbox: 0.4136  loss_giou: 0.565  loss_ce_dn: 0.0003593  loss_mask_dn: 0.5043  loss_dice_dn: 1.197  loss_bbox_dn: 0.3242  loss_giou_dn: 0.4334  loss_ce_0: 0.7044  loss_mask_0: 0.5136  loss_dice_0: 1.17  loss_bbox_0: 0.803  loss_giou_0: 0.8901  loss_ce_dn_0: 0.05656  loss_mask_dn_0: 0.7242  loss_dice_dn_0: 2.43  loss_bbox_dn_0: 0.8439  loss_giou_dn_0: 0.8497  loss_ce_1: 0.6604  loss_mask_1: 0.4792  loss_dice_1: 1.162  loss_bbox_1: 0.417  loss_giou_1: 0.5134  loss_ce_dn_1: 0.00105  loss_mask_dn_1: 0.5018  loss_dice_dn_1: 1.202  loss_bbox_dn_1: 0.3792  loss_giou_dn_1: 0.4846  loss_ce_2: 0.4777  loss_mask_2: 0.508  loss_dice_2: 1.161  loss_bbox_2: 0.3897  loss_giou_2: 0.5758  loss_ce_dn_2: 0.0008118  loss_mask_dn_2: 0.4936  loss_dice_dn_2: 1.178  loss_bbox_dn_2: 0.3331  loss_giou_dn_2: 0.479  loss_ce_3: 0.3454  loss_mask_3: 0.4851  loss_dice_3: 1.165  loss_bbox_3: 0.4018  loss_giou_3: 0.5837  loss_ce_dn_3: 0.0009802  loss_mask_dn_3: 0.513  loss_dice_dn_3: 1.185  loss_bbox_dn_3: 0.35  loss_giou_dn_3: 0.4694  loss_ce_4: 0.2987  loss_mask_4: 0.4874  loss_dice_4: 1.162  loss_bbox_4: 0.4002  loss_giou_4: 0.5784  loss_ce_dn_4: 0.0006538  loss_mask_dn_4: 0.4974  loss_dice_dn_4: 1.18  loss_bbox_dn_4: 0.3298  loss_giou_dn_4: 0.4423  loss_ce_5: 0.1612  loss_mask_5: 0.4724  loss_dice_5: 1.212  loss_bbox_5: 0.4067  loss_giou_5: 0.5749  loss_ce_dn_5: 0.0007277  loss_mask_dn_5: 0.4931  loss_dice_dn_5: 1.206  loss_bbox_dn_5: 0.328  loss_giou_dn_5: 0.4439  loss_ce_6: 0.1301  loss_mask_6: 0.4857  loss_dice_6: 1.164  loss_bbox_6: 0.4188  loss_giou_6: 0.5781  loss_ce_dn_6: 0.0004026  loss_mask_dn_6: 0.4972  loss_dice_dn_6: 1.213  loss_bbox_dn_6: 0.3231  loss_giou_dn_6: 0.4421  loss_ce_7: 0.1387  loss_mask_7: 0.4817  loss_dice_7: 1.198  loss_bbox_7: 0.4084  loss_giou_7: 0.5671  loss_ce_dn_7: 0.0004372  loss_mask_dn_7: 0.4956  loss_dice_dn_7: 1.196  loss_bbox_dn_7: 0.3267  loss_giou_dn_7: 0.4407  loss_ce_8: 0.1322  loss_mask_8: 0.4696  loss_dice_8: 1.109  loss_bbox_8: 0.4157  loss_giou_8: 0.5634  loss_ce_dn_8: 0.0003418  loss_mask_dn_8: 0.5101  loss_dice_dn_8: 1.196  loss_bbox_dn_8: 0.3246  loss_giou_dn_8: 0.4335  loss_ce_interm: 0.7293  loss_mask_interm: 0.4842  loss_dice_interm: 1.164  loss_bbox_interm: 0.5585  loss_giou_interm: 0.6471    time: 0.4894  last_time: 0.5028  data_time: 0.0035  last_data_time: 0.0035   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:19:55 d2.utils.events]:  eta: 0:16:33  iter: 1939  total_loss: 72.71  loss_ce: 0.299  loss_mask: 0.5332  loss_dice: 1.476  loss_bbox: 0.3368  loss_giou: 0.5812  loss_ce_dn: 0.000314  loss_mask_dn: 0.5059  loss_dice_dn: 1.393  loss_bbox_dn: 0.2946  loss_giou_dn: 0.4741  loss_ce_0: 0.9533  loss_mask_0: 0.5559  loss_dice_0: 1.451  loss_bbox_0: 0.9109  loss_giou_0: 0.9525  loss_ce_dn_0: 0.06498  loss_mask_dn_0: 0.7843  loss_dice_dn_0: 2.721  loss_bbox_dn_0: 0.8292  loss_giou_dn_0: 0.848  loss_ce_1: 0.7915  loss_mask_1: 0.5651  loss_dice_1: 1.433  loss_bbox_1: 0.4677  loss_giou_1: 0.6536  loss_ce_dn_1: 0.0008254  loss_mask_dn_1: 0.4857  loss_dice_dn_1: 1.389  loss_bbox_dn_1: 0.3943  loss_giou_dn_1: 0.542  loss_ce_2: 0.6314  loss_mask_2: 0.5389  loss_dice_2: 1.459  loss_bbox_2: 0.4388  loss_giou_2: 0.6674  loss_ce_dn_2: 0.0006002  loss_mask_dn_2: 0.4978  loss_dice_dn_2: 1.368  loss_bbox_dn_2: 0.3524  loss_giou_dn_2: 0.4819  loss_ce_3: 0.475  loss_mask_3: 0.5365  loss_dice_3: 1.4  loss_bbox_3: 0.3989  loss_giou_3: 0.6346  loss_ce_dn_3: 0.0006404  loss_mask_dn_3: 0.5054  loss_dice_dn_3: 1.377  loss_bbox_dn_3: 0.3142  loss_giou_dn_3: 0.4843  loss_ce_4: 0.4707  loss_mask_4: 0.5322  loss_dice_4: 1.546  loss_bbox_4: 0.3307  loss_giou_4: 0.5436  loss_ce_dn_4: 0.0005102  loss_mask_dn_4: 0.5021  loss_dice_dn_4: 1.415  loss_bbox_dn_4: 0.305  loss_giou_dn_4: 0.4761  loss_ce_5: 0.3234  loss_mask_5: 0.5262  loss_dice_5: 1.447  loss_bbox_5: 0.3897  loss_giou_5: 0.583  loss_ce_dn_5: 0.0005263  loss_mask_dn_5: 0.5021  loss_dice_dn_5: 1.366  loss_bbox_dn_5: 0.3008  loss_giou_dn_5: 0.4752  loss_ce_6: 0.3454  loss_mask_6: 0.5518  loss_dice_6: 1.406  loss_bbox_6: 0.338  loss_giou_6: 0.5458  loss_ce_dn_6: 0.0005402  loss_mask_dn_6: 0.4998  loss_dice_dn_6: 1.388  loss_bbox_dn_6: 0.2918  loss_giou_dn_6: 0.4723  loss_ce_7: 0.3299  loss_mask_7: 0.5531  loss_dice_7: 1.445  loss_bbox_7: 0.3429  loss_giou_7: 0.5599  loss_ce_dn_7: 0.0004742  loss_mask_dn_7: 0.5096  loss_dice_dn_7: 1.366  loss_bbox_dn_7: 0.2935  loss_giou_dn_7: 0.4724  loss_ce_8: 0.3069  loss_mask_8: 0.5358  loss_dice_8: 1.455  loss_bbox_8: 0.335  loss_giou_8: 0.5819  loss_ce_dn_8: 0.0003578  loss_mask_dn_8: 0.5105  loss_dice_dn_8: 1.381  loss_bbox_dn_8: 0.2916  loss_giou_dn_8: 0.471  loss_ce_interm: 0.9864  loss_mask_interm: 0.4708  loss_dice_interm: 1.458  loss_bbox_interm: 0.533  loss_giou_interm: 0.8409    time: 0.4893  last_time: 0.4946  data_time: 0.0035  last_data_time: 0.0041   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:20:05 d2.utils.events]:  eta: 0:16:24  iter: 1959  total_loss: 78.71  loss_ce: 0.619  loss_mask: 0.5222  loss_dice: 1.4  loss_bbox: 0.3375  loss_giou: 0.6673  loss_ce_dn: 0.0003677  loss_mask_dn: 0.4793  loss_dice_dn: 1.426  loss_bbox_dn: 0.3101  loss_giou_dn: 0.47  loss_ce_0: 0.927  loss_mask_0: 0.5475  loss_dice_0: 1.502  loss_bbox_0: 0.7396  loss_giou_0: 0.9286  loss_ce_dn_0: 0.02114  loss_mask_dn_0: 0.8674  loss_dice_dn_0: 2.981  loss_bbox_dn_0: 0.7962  loss_giou_dn_0: 0.8479  loss_ce_1: 0.9263  loss_mask_1: 0.5528  loss_dice_1: 1.463  loss_bbox_1: 0.4905  loss_giou_1: 0.6012  loss_ce_dn_1: 0.0006446  loss_mask_dn_1: 0.4998  loss_dice_dn_1: 1.475  loss_bbox_dn_1: 0.4138  loss_giou_dn_1: 0.5291  loss_ce_2: 0.7164  loss_mask_2: 0.5476  loss_dice_2: 1.459  loss_bbox_2: 0.4655  loss_giou_2: 0.6437  loss_ce_dn_2: 0.000791  loss_mask_dn_2: 0.4952  loss_dice_dn_2: 1.423  loss_bbox_dn_2: 0.3305  loss_giou_dn_2: 0.504  loss_ce_3: 0.5206  loss_mask_3: 0.5425  loss_dice_3: 1.483  loss_bbox_3: 0.4626  loss_giou_3: 0.6722  loss_ce_dn_3: 0.0007623  loss_mask_dn_3: 0.4974  loss_dice_dn_3: 1.435  loss_bbox_dn_3: 0.3092  loss_giou_dn_3: 0.4967  loss_ce_4: 0.539  loss_mask_4: 0.5114  loss_dice_4: 1.41  loss_bbox_4: 0.433  loss_giou_4: 0.6862  loss_ce_dn_4: 0.000517  loss_mask_dn_4: 0.4836  loss_dice_dn_4: 1.396  loss_bbox_dn_4: 0.3236  loss_giou_dn_4: 0.4924  loss_ce_5: 0.6094  loss_mask_5: 0.5194  loss_dice_5: 1.45  loss_bbox_5: 0.3748  loss_giou_5: 0.6606  loss_ce_dn_5: 0.0005468  loss_mask_dn_5: 0.4701  loss_dice_dn_5: 1.418  loss_bbox_dn_5: 0.3152  loss_giou_dn_5: 0.4691  loss_ce_6: 0.6627  loss_mask_6: 0.5256  loss_dice_6: 1.475  loss_bbox_6: 0.34  loss_giou_6: 0.6522  loss_ce_dn_6: 0.0004777  loss_mask_dn_6: 0.4745  loss_dice_dn_6: 1.424  loss_bbox_dn_6: 0.3128  loss_giou_dn_6: 0.4624  loss_ce_7: 0.6428  loss_mask_7: 0.5335  loss_dice_7: 1.453  loss_bbox_7: 0.3389  loss_giou_7: 0.6571  loss_ce_dn_7: 0.0003834  loss_mask_dn_7: 0.4965  loss_dice_dn_7: 1.42  loss_bbox_dn_7: 0.3128  loss_giou_dn_7: 0.4642  loss_ce_8: 0.6309  loss_mask_8: 0.5256  loss_dice_8: 1.417  loss_bbox_8: 0.3412  loss_giou_8: 0.6646  loss_ce_dn_8: 0.0003446  loss_mask_dn_8: 0.4843  loss_dice_dn_8: 1.407  loss_bbox_dn_8: 0.3137  loss_giou_dn_8: 0.4701  loss_ce_interm: 0.9726  loss_mask_interm: 0.5559  loss_dice_interm: 1.463  loss_bbox_interm: 0.5271  loss_giou_interm: 0.6778    time: 0.4894  last_time: 0.4756  data_time: 0.0034  last_data_time: 0.0032   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:20:15 d2.utils.events]:  eta: 0:16:15  iter: 1979  total_loss: 72.35  loss_ce: 0.4003  loss_mask: 0.3941  loss_dice: 1.352  loss_bbox: 0.3517  loss_giou: 0.6012  loss_ce_dn: 0.0008958  loss_mask_dn: 0.3884  loss_dice_dn: 1.379  loss_bbox_dn: 0.2618  loss_giou_dn: 0.4616  loss_ce_0: 0.705  loss_mask_0: 0.3629  loss_dice_0: 1.314  loss_bbox_0: 0.6991  loss_giou_0: 0.9445  loss_ce_dn_0: 0.06324  loss_mask_dn_0: 0.7941  loss_dice_dn_0: 2.891  loss_bbox_dn_0: 0.7943  loss_giou_dn_0: 0.8525  loss_ce_1: 0.7844  loss_mask_1: 0.3799  loss_dice_1: 1.269  loss_bbox_1: 0.3522  loss_giou_1: 0.6186  loss_ce_dn_1: 0.00107  loss_mask_dn_1: 0.4045  loss_dice_dn_1: 1.422  loss_bbox_dn_1: 0.3299  loss_giou_dn_1: 0.5526  loss_ce_2: 0.5374  loss_mask_2: 0.3923  loss_dice_2: 1.363  loss_bbox_2: 0.3336  loss_giou_2: 0.6289  loss_ce_dn_2: 0.0009557  loss_mask_dn_2: 0.3845  loss_dice_dn_2: 1.349  loss_bbox_dn_2: 0.3049  loss_giou_dn_2: 0.4944  loss_ce_3: 0.518  loss_mask_3: 0.371  loss_dice_3: 1.341  loss_bbox_3: 0.3571  loss_giou_3: 0.6554  loss_ce_dn_3: 0.0007963  loss_mask_dn_3: 0.4022  loss_dice_dn_3: 1.357  loss_bbox_dn_3: 0.2748  loss_giou_dn_3: 0.4752  loss_ce_4: 0.3602  loss_mask_4: 0.3862  loss_dice_4: 1.338  loss_bbox_4: 0.3288  loss_giou_4: 0.598  loss_ce_dn_4: 0.0007665  loss_mask_dn_4: 0.3988  loss_dice_dn_4: 1.325  loss_bbox_dn_4: 0.2747  loss_giou_dn_4: 0.4638  loss_ce_5: 0.341  loss_mask_5: 0.3898  loss_dice_5: 1.337  loss_bbox_5: 0.3258  loss_giou_5: 0.6085  loss_ce_dn_5: 0.0008905  loss_mask_dn_5: 0.3958  loss_dice_dn_5: 1.332  loss_bbox_dn_5: 0.2693  loss_giou_dn_5: 0.465  loss_ce_6: 0.3763  loss_mask_6: 0.4138  loss_dice_6: 1.333  loss_bbox_6: 0.3254  loss_giou_6: 0.6005  loss_ce_dn_6: 0.0009692  loss_mask_dn_6: 0.3991  loss_dice_dn_6: 1.361  loss_bbox_dn_6: 0.2633  loss_giou_dn_6: 0.4643  loss_ce_7: 0.3876  loss_mask_7: 0.3915  loss_dice_7: 1.36  loss_bbox_7: 0.3471  loss_giou_7: 0.5641  loss_ce_dn_7: 0.0007618  loss_mask_dn_7: 0.3969  loss_dice_dn_7: 1.359  loss_bbox_dn_7: 0.2653  loss_giou_dn_7: 0.4602  loss_ce_8: 0.3652  loss_mask_8: 0.3877  loss_dice_8: 1.385  loss_bbox_8: 0.3466  loss_giou_8: 0.5985  loss_ce_dn_8: 0.0007524  loss_mask_dn_8: 0.3932  loss_dice_dn_8: 1.373  loss_bbox_dn_8: 0.2639  loss_giou_dn_8: 0.4629  loss_ce_interm: 0.7488  loss_mask_interm: 0.3594  loss_dice_interm: 1.384  loss_bbox_interm: 0.4698  loss_giou_interm: 0.7859    time: 0.4893  last_time: 0.4284  data_time: 0.0036  last_data_time: 0.0038   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:20:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:20:26 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:20:26 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:20:26 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:20:27 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0744 s/iter. Eval: 0.0094 s/iter. Total: 0.0847 s/iter. ETA=0:00:04\n",
      "[03/14 17:20:32 d2.evaluation.evaluator]: Total inference time: 0:00:05.498018 (0.088678 s / iter per device, on 1 devices)\n",
      "[03/14 17:20:32 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076747 s / iter per device, on 1 devices)\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.367\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.372\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.394\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.289\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.477\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.553\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.643\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 36.746 | 68.914 | 37.198 | 39.414 | 42.423 |  nan  |\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:20:32 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.668\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.207\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.346\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 20.183 | 66.817 | 2.024  | 20.664 | 22.441 |  nan  |\n",
      "[03/14 17:20:32 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:20:32 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: 36.7459,68.9135,37.1985,39.4144,42.4226,nan\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:20:32 d2.evaluation.testing]: copypaste: 20.1833,66.8167,2.0239,20.6640,22.4412,nan\n",
      "[03/14 17:20:32 d2.utils.events]:  eta: 0:16:05  iter: 1999  total_loss: 63.19  loss_ce: 0.1754  loss_mask: 0.5052  loss_dice: 1.09  loss_bbox: 0.3515  loss_giou: 0.5108  loss_ce_dn: 0.0004262  loss_mask_dn: 0.4283  loss_dice_dn: 1.038  loss_bbox_dn: 0.3112  loss_giou_dn: 0.4525  loss_ce_0: 0.8319  loss_mask_0: 0.462  loss_dice_0: 1.108  loss_bbox_0: 0.6843  loss_giou_0: 0.7795  loss_ce_dn_0: 0.0217  loss_mask_dn_0: 0.8765  loss_dice_dn_0: 2.372  loss_bbox_dn_0: 1.028  loss_giou_dn_0: 0.8525  loss_ce_1: 0.7251  loss_mask_1: 0.5071  loss_dice_1: 1.029  loss_bbox_1: 0.3852  loss_giou_1: 0.5356  loss_ce_dn_1: 0.00065  loss_mask_dn_1: 0.4482  loss_dice_dn_1: 1.08  loss_bbox_dn_1: 0.4534  loss_giou_dn_1: 0.5356  loss_ce_2: 0.5001  loss_mask_2: 0.5111  loss_dice_2: 1.104  loss_bbox_2: 0.3936  loss_giou_2: 0.5195  loss_ce_dn_2: 0.0006164  loss_mask_dn_2: 0.4236  loss_dice_dn_2: 1.085  loss_bbox_dn_2: 0.4082  loss_giou_dn_2: 0.4836  loss_ce_3: 0.3323  loss_mask_3: 0.5027  loss_dice_3: 1.127  loss_bbox_3: 0.3801  loss_giou_3: 0.486  loss_ce_dn_3: 0.001201  loss_mask_dn_3: 0.4245  loss_dice_dn_3: 1.059  loss_bbox_dn_3: 0.3584  loss_giou_dn_3: 0.4549  loss_ce_4: 0.2657  loss_mask_4: 0.4986  loss_dice_4: 1.114  loss_bbox_4: 0.362  loss_giou_4: 0.5087  loss_ce_dn_4: 0.0005399  loss_mask_dn_4: 0.4248  loss_dice_dn_4: 1.079  loss_bbox_dn_4: 0.3182  loss_giou_dn_4: 0.4552  loss_ce_5: 0.2133  loss_mask_5: 0.4876  loss_dice_5: 1.104  loss_bbox_5: 0.3484  loss_giou_5: 0.4966  loss_ce_dn_5: 0.0005819  loss_mask_dn_5: 0.4157  loss_dice_dn_5: 1.059  loss_bbox_dn_5: 0.3213  loss_giou_dn_5: 0.4571  loss_ce_6: 0.1981  loss_mask_6: 0.498  loss_dice_6: 1.09  loss_bbox_6: 0.353  loss_giou_6: 0.4998  loss_ce_dn_6: 0.0005886  loss_mask_dn_6: 0.4207  loss_dice_dn_6: 1.057  loss_bbox_dn_6: 0.3126  loss_giou_dn_6: 0.4596  loss_ce_7: 0.1862  loss_mask_7: 0.4957  loss_dice_7: 1.062  loss_bbox_7: 0.3663  loss_giou_7: 0.5052  loss_ce_dn_7: 0.0004568  loss_mask_dn_7: 0.4238  loss_dice_dn_7: 1.04  loss_bbox_dn_7: 0.3193  loss_giou_dn_7: 0.4569  loss_ce_8: 0.174  loss_mask_8: 0.4974  loss_dice_8: 1.092  loss_bbox_8: 0.3394  loss_giou_8: 0.506  loss_ce_dn_8: 0.0004371  loss_mask_dn_8: 0.4307  loss_dice_dn_8: 1.035  loss_bbox_dn_8: 0.3089  loss_giou_dn_8: 0.4496  loss_ce_interm: 0.8329  loss_mask_interm: 0.47  loss_dice_interm: 1.044  loss_bbox_interm: 0.5025  loss_giou_interm: 0.5738    time: 0.4892  last_time: 0.4936  data_time: 0.0035  last_data_time: 0.0028   lr: 0.0001  max_mem: 2505M\n",
      "[03/14 17:20:42 d2.utils.events]:  eta: 0:15:56  iter: 2019  total_loss: 73.48  loss_ce: 0.3464  loss_mask: 0.4375  loss_dice: 1.322  loss_bbox: 0.369  loss_giou: 0.6771  loss_ce_dn: 0.001157  loss_mask_dn: 0.4458  loss_dice_dn: 1.338  loss_bbox_dn: 0.2661  loss_giou_dn: 0.4629  loss_ce_0: 0.7652  loss_mask_0: 0.4516  loss_dice_0: 1.34  loss_bbox_0: 0.7662  loss_giou_0: 0.9075  loss_ce_dn_0: 0.02155  loss_mask_dn_0: 0.7657  loss_dice_dn_0: 2.818  loss_bbox_dn_0: 0.8587  loss_giou_dn_0: 0.8552  loss_ce_1: 0.8904  loss_mask_1: 0.4896  loss_dice_1: 1.37  loss_bbox_1: 0.3911  loss_giou_1: 0.6221  loss_ce_dn_1: 0.0004433  loss_mask_dn_1: 0.4519  loss_dice_dn_1: 1.398  loss_bbox_dn_1: 0.3424  loss_giou_dn_1: 0.5198  loss_ce_2: 0.5304  loss_mask_2: 0.4796  loss_dice_2: 1.412  loss_bbox_2: 0.426  loss_giou_2: 0.5837  loss_ce_dn_2: 0.0005794  loss_mask_dn_2: 0.4604  loss_dice_dn_2: 1.346  loss_bbox_dn_2: 0.3141  loss_giou_dn_2: 0.4981  loss_ce_3: 0.3298  loss_mask_3: 0.4781  loss_dice_3: 1.409  loss_bbox_3: 0.39  loss_giou_3: 0.5862  loss_ce_dn_3: 0.001609  loss_mask_dn_3: 0.4461  loss_dice_dn_3: 1.359  loss_bbox_dn_3: 0.2683  loss_giou_dn_3: 0.4879  loss_ce_4: 0.2999  loss_mask_4: 0.4541  loss_dice_4: 1.336  loss_bbox_4: 0.4421  loss_giou_4: 0.6356  loss_ce_dn_4: 0.0009871  loss_mask_dn_4: 0.4446  loss_dice_dn_4: 1.329  loss_bbox_dn_4: 0.2618  loss_giou_dn_4: 0.4903  loss_ce_5: 0.2654  loss_mask_5: 0.4463  loss_dice_5: 1.366  loss_bbox_5: 0.3853  loss_giou_5: 0.6556  loss_ce_dn_5: 0.001189  loss_mask_dn_5: 0.445  loss_dice_dn_5: 1.327  loss_bbox_dn_5: 0.2611  loss_giou_dn_5: 0.4711  loss_ce_6: 0.2979  loss_mask_6: 0.4447  loss_dice_6: 1.378  loss_bbox_6: 0.3925  loss_giou_6: 0.6725  loss_ce_dn_6: 0.001398  loss_mask_dn_6: 0.4429  loss_dice_dn_6: 1.355  loss_bbox_dn_6: 0.2562  loss_giou_dn_6: 0.4702  loss_ce_7: 0.3034  loss_mask_7: 0.4645  loss_dice_7: 1.298  loss_bbox_7: 0.3585  loss_giou_7: 0.6782  loss_ce_dn_7: 0.001325  loss_mask_dn_7: 0.446  loss_dice_dn_7: 1.34  loss_bbox_dn_7: 0.2556  loss_giou_dn_7: 0.4676  loss_ce_8: 0.3033  loss_mask_8: 0.4674  loss_dice_8: 1.343  loss_bbox_8: 0.3573  loss_giou_8: 0.6799  loss_ce_dn_8: 0.001158  loss_mask_dn_8: 0.4498  loss_dice_dn_8: 1.349  loss_bbox_dn_8: 0.2628  loss_giou_dn_8: 0.4635  loss_ce_interm: 0.7652  loss_mask_interm: 0.4776  loss_dice_interm: 1.394  loss_bbox_interm: 0.4886  loss_giou_interm: 0.7924    time: 0.4891  last_time: 0.4723  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:20:51 d2.utils.events]:  eta: 0:15:46  iter: 2039  total_loss: 71.69  loss_ce: 0.4898  loss_mask: 0.3872  loss_dice: 1.344  loss_bbox: 0.3717  loss_giou: 0.6768  loss_ce_dn: 0.0009713  loss_mask_dn: 0.393  loss_dice_dn: 1.4  loss_bbox_dn: 0.2591  loss_giou_dn: 0.5036  loss_ce_0: 0.7484  loss_mask_0: 0.4426  loss_dice_0: 1.391  loss_bbox_0: 0.6923  loss_giou_0: 0.9981  loss_ce_dn_0: 0.02151  loss_mask_dn_0: 0.7117  loss_dice_dn_0: 2.657  loss_bbox_dn_0: 0.8856  loss_giou_dn_0: 0.8523  loss_ce_1: 0.8406  loss_mask_1: 0.3951  loss_dice_1: 1.369  loss_bbox_1: 0.3747  loss_giou_1: 0.6797  loss_ce_dn_1: 0.0004646  loss_mask_dn_1: 0.3933  loss_dice_dn_1: 1.463  loss_bbox_dn_1: 0.3233  loss_giou_dn_1: 0.5287  loss_ce_2: 0.6584  loss_mask_2: 0.377  loss_dice_2: 1.376  loss_bbox_2: 0.365  loss_giou_2: 0.6449  loss_ce_dn_2: 0.0006165  loss_mask_dn_2: 0.3979  loss_dice_dn_2: 1.457  loss_bbox_dn_2: 0.2851  loss_giou_dn_2: 0.5238  loss_ce_3: 0.5802  loss_mask_3: 0.3692  loss_dice_3: 1.334  loss_bbox_3: 0.3456  loss_giou_3: 0.6956  loss_ce_dn_3: 0.001598  loss_mask_dn_3: 0.397  loss_dice_dn_3: 1.463  loss_bbox_dn_3: 0.2835  loss_giou_dn_3: 0.5157  loss_ce_4: 0.4299  loss_mask_4: 0.3884  loss_dice_4: 1.429  loss_bbox_4: 0.3798  loss_giou_4: 0.694  loss_ce_dn_4: 0.0006895  loss_mask_dn_4: 0.3908  loss_dice_dn_4: 1.486  loss_bbox_dn_4: 0.2846  loss_giou_dn_4: 0.4992  loss_ce_5: 0.4816  loss_mask_5: 0.3825  loss_dice_5: 1.38  loss_bbox_5: 0.3654  loss_giou_5: 0.6882  loss_ce_dn_5: 0.001153  loss_mask_dn_5: 0.3867  loss_dice_dn_5: 1.464  loss_bbox_dn_5: 0.2536  loss_giou_dn_5: 0.5022  loss_ce_6: 0.5072  loss_mask_6: 0.3821  loss_dice_6: 1.36  loss_bbox_6: 0.3546  loss_giou_6: 0.6875  loss_ce_dn_6: 0.001011  loss_mask_dn_6: 0.3937  loss_dice_dn_6: 1.421  loss_bbox_dn_6: 0.2618  loss_giou_dn_6: 0.4996  loss_ce_7: 0.5197  loss_mask_7: 0.3936  loss_dice_7: 1.288  loss_bbox_7: 0.3717  loss_giou_7: 0.6787  loss_ce_dn_7: 0.001074  loss_mask_dn_7: 0.3963  loss_dice_dn_7: 1.398  loss_bbox_dn_7: 0.2511  loss_giou_dn_7: 0.5069  loss_ce_8: 0.4928  loss_mask_8: 0.3734  loss_dice_8: 1.354  loss_bbox_8: 0.363  loss_giou_8: 0.6771  loss_ce_dn_8: 0.001014  loss_mask_dn_8: 0.3939  loss_dice_dn_8: 1.395  loss_bbox_dn_8: 0.2565  loss_giou_dn_8: 0.5007  loss_ce_interm: 0.7494  loss_mask_interm: 0.4413  loss_dice_interm: 1.477  loss_bbox_interm: 0.4333  loss_giou_interm: 0.7549    time: 0.4890  last_time: 0.4920  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:01 d2.utils.events]:  eta: 0:15:38  iter: 2059  total_loss: 73.61  loss_ce: 0.6174  loss_mask: 0.3688  loss_dice: 1.531  loss_bbox: 0.3226  loss_giou: 0.6576  loss_ce_dn: 0.0009732  loss_mask_dn: 0.3752  loss_dice_dn: 1.481  loss_bbox_dn: 0.2516  loss_giou_dn: 0.4809  loss_ce_0: 0.8748  loss_mask_0: 0.362  loss_dice_0: 1.452  loss_bbox_0: 0.6587  loss_giou_0: 1.16  loss_ce_dn_0: 0.02136  loss_mask_dn_0: 0.5269  loss_dice_dn_0: 2.825  loss_bbox_dn_0: 0.777  loss_giou_dn_0: 0.8473  loss_ce_1: 0.8979  loss_mask_1: 0.3825  loss_dice_1: 1.452  loss_bbox_1: 0.373  loss_giou_1: 0.8132  loss_ce_dn_1: 0.0004629  loss_mask_dn_1: 0.3467  loss_dice_dn_1: 1.464  loss_bbox_dn_1: 0.3076  loss_giou_dn_1: 0.5598  loss_ce_2: 0.5526  loss_mask_2: 0.3982  loss_dice_2: 1.753  loss_bbox_2: 0.3883  loss_giou_2: 0.79  loss_ce_dn_2: 0.0004812  loss_mask_dn_2: 0.3609  loss_dice_dn_2: 1.464  loss_bbox_dn_2: 0.2787  loss_giou_dn_2: 0.5107  loss_ce_3: 0.4634  loss_mask_3: 0.3973  loss_dice_3: 1.777  loss_bbox_3: 0.3305  loss_giou_3: 0.7061  loss_ce_dn_3: 0.00115  loss_mask_dn_3: 0.3797  loss_dice_dn_3: 1.444  loss_bbox_dn_3: 0.286  loss_giou_dn_3: 0.4961  loss_ce_4: 0.434  loss_mask_4: 0.3735  loss_dice_4: 1.697  loss_bbox_4: 0.3324  loss_giou_4: 0.6784  loss_ce_dn_4: 0.0006658  loss_mask_dn_4: 0.3651  loss_dice_dn_4: 1.46  loss_bbox_dn_4: 0.2464  loss_giou_dn_4: 0.4838  loss_ce_5: 0.431  loss_mask_5: 0.3284  loss_dice_5: 1.581  loss_bbox_5: 0.3252  loss_giou_5: 0.6864  loss_ce_dn_5: 0.00107  loss_mask_dn_5: 0.3667  loss_dice_dn_5: 1.474  loss_bbox_dn_5: 0.2389  loss_giou_dn_5: 0.4786  loss_ce_6: 0.4183  loss_mask_6: 0.361  loss_dice_6: 1.554  loss_bbox_6: 0.3002  loss_giou_6: 0.6996  loss_ce_dn_6: 0.001136  loss_mask_dn_6: 0.3716  loss_dice_dn_6: 1.478  loss_bbox_dn_6: 0.2436  loss_giou_dn_6: 0.4749  loss_ce_7: 0.5543  loss_mask_7: 0.3635  loss_dice_7: 1.57  loss_bbox_7: 0.2955  loss_giou_7: 0.6686  loss_ce_dn_7: 0.001161  loss_mask_dn_7: 0.3696  loss_dice_dn_7: 1.492  loss_bbox_dn_7: 0.2497  loss_giou_dn_7: 0.4782  loss_ce_8: 0.5735  loss_mask_8: 0.3638  loss_dice_8: 1.454  loss_bbox_8: 0.3099  loss_giou_8: 0.6592  loss_ce_dn_8: 0.0008825  loss_mask_dn_8: 0.377  loss_dice_dn_8: 1.487  loss_bbox_dn_8: 0.2495  loss_giou_dn_8: 0.4771  loss_ce_interm: 0.8748  loss_mask_interm: 0.3467  loss_dice_interm: 1.52  loss_bbox_interm: 0.3862  loss_giou_interm: 0.8452    time: 0.4890  last_time: 0.5252  data_time: 0.0034  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:11 d2.utils.events]:  eta: 0:15:28  iter: 2079  total_loss: 67.02  loss_ce: 0.3338  loss_mask: 0.3452  loss_dice: 1.48  loss_bbox: 0.3509  loss_giou: 0.5882  loss_ce_dn: 0.0005633  loss_mask_dn: 0.3444  loss_dice_dn: 1.348  loss_bbox_dn: 0.2524  loss_giou_dn: 0.413  loss_ce_0: 0.6332  loss_mask_0: 0.3779  loss_dice_0: 1.413  loss_bbox_0: 0.6901  loss_giou_0: 1.012  loss_ce_dn_0: 0.06208  loss_mask_dn_0: 0.6421  loss_dice_dn_0: 3.015  loss_bbox_dn_0: 0.728  loss_giou_dn_0: 0.8521  loss_ce_1: 0.7649  loss_mask_1: 0.3459  loss_dice_1: 1.395  loss_bbox_1: 0.3432  loss_giou_1: 0.5372  loss_ce_dn_1: 0.0006012  loss_mask_dn_1: 0.3452  loss_dice_dn_1: 1.308  loss_bbox_dn_1: 0.3178  loss_giou_dn_1: 0.4921  loss_ce_2: 0.5923  loss_mask_2: 0.3469  loss_dice_2: 1.439  loss_bbox_2: 0.3271  loss_giou_2: 0.6511  loss_ce_dn_2: 0.0005756  loss_mask_dn_2: 0.3492  loss_dice_dn_2: 1.372  loss_bbox_dn_2: 0.2572  loss_giou_dn_2: 0.4333  loss_ce_3: 0.3903  loss_mask_3: 0.3217  loss_dice_3: 1.424  loss_bbox_3: 0.3099  loss_giou_3: 0.6609  loss_ce_dn_3: 0.001244  loss_mask_dn_3: 0.3433  loss_dice_dn_3: 1.332  loss_bbox_dn_3: 0.2599  loss_giou_dn_3: 0.4414  loss_ce_4: 0.3373  loss_mask_4: 0.3057  loss_dice_4: 1.403  loss_bbox_4: 0.3322  loss_giou_4: 0.6118  loss_ce_dn_4: 0.0007036  loss_mask_dn_4: 0.3439  loss_dice_dn_4: 1.352  loss_bbox_dn_4: 0.2389  loss_giou_dn_4: 0.4434  loss_ce_5: 0.3192  loss_mask_5: 0.346  loss_dice_5: 1.436  loss_bbox_5: 0.3701  loss_giou_5: 0.6074  loss_ce_dn_5: 0.001032  loss_mask_dn_5: 0.3404  loss_dice_dn_5: 1.336  loss_bbox_dn_5: 0.2398  loss_giou_dn_5: 0.4194  loss_ce_6: 0.3154  loss_mask_6: 0.3176  loss_dice_6: 1.468  loss_bbox_6: 0.3809  loss_giou_6: 0.5764  loss_ce_dn_6: 0.001105  loss_mask_dn_6: 0.3389  loss_dice_dn_6: 1.347  loss_bbox_dn_6: 0.2419  loss_giou_dn_6: 0.4138  loss_ce_7: 0.314  loss_mask_7: 0.3398  loss_dice_7: 1.337  loss_bbox_7: 0.3701  loss_giou_7: 0.5931  loss_ce_dn_7: 0.0007134  loss_mask_dn_7: 0.3443  loss_dice_dn_7: 1.351  loss_bbox_dn_7: 0.2511  loss_giou_dn_7: 0.4201  loss_ce_8: 0.3173  loss_mask_8: 0.3537  loss_dice_8: 1.361  loss_bbox_8: 0.3458  loss_giou_8: 0.5878  loss_ce_dn_8: 0.0005108  loss_mask_dn_8: 0.3453  loss_dice_dn_8: 1.354  loss_bbox_dn_8: 0.2435  loss_giou_dn_8: 0.4147  loss_ce_interm: 0.6908  loss_mask_interm: 0.3567  loss_dice_interm: 1.376  loss_bbox_interm: 0.4513  loss_giou_interm: 0.6986    time: 0.4889  last_time: 0.4425  data_time: 0.0036  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:21 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:21:21 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:21:21 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:21:21 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:21:22 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0834 s/iter. Eval: 0.0102 s/iter. Total: 0.0945 s/iter. ETA=0:00:05\n",
      "[03/14 17:21:27 d2.evaluation.evaluator]: Total inference time: 0:00:05.548158 (0.089486 s / iter per device, on 1 devices)\n",
      "[03/14 17:21:27 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077179 s / iter per device, on 1 devices)\n",
      "[03/14 17:21:27 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:21:27 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:21:27 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:21:27 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.776\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.537\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.587\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.547\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.733\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:21:28 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 45.600 | 77.593 | 49.032 | 44.562 | 58.826 |  nan  |\n",
      "[03/14 17:21:28 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:21:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.223\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.313\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.326\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:21:28 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 23.876 | 70.034 | 5.400  | 22.305 | 31.335 |  nan  |\n",
      "[03/14 17:21:28 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:21:28 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: 45.6000,77.5931,49.0320,44.5616,58.8258,nan\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:21:28 d2.evaluation.testing]: copypaste: 23.8758,70.0337,5.4004,22.3048,31.3348,nan\n",
      "[03/14 17:21:28 d2.utils.events]:  eta: 0:15:19  iter: 2099  total_loss: 67.27  loss_ce: 0.3304  loss_mask: 0.3374  loss_dice: 1.29  loss_bbox: 0.3471  loss_giou: 0.5574  loss_ce_dn: 0.0004902  loss_mask_dn: 0.3251  loss_dice_dn: 1.347  loss_bbox_dn: 0.2428  loss_giou_dn: 0.4361  loss_ce_0: 0.8565  loss_mask_0: 0.3234  loss_dice_0: 1.174  loss_bbox_0: 0.5671  loss_giou_0: 0.8219  loss_ce_dn_0: 0.04574  loss_mask_dn_0: 0.6549  loss_dice_dn_0: 2.951  loss_bbox_dn_0: 0.735  loss_giou_dn_0: 0.8549  loss_ce_1: 0.7103  loss_mask_1: 0.3185  loss_dice_1: 1.329  loss_bbox_1: 0.3453  loss_giou_1: 0.6063  loss_ce_dn_1: 0.0006528  loss_mask_dn_1: 0.3112  loss_dice_dn_1: 1.383  loss_bbox_dn_1: 0.3151  loss_giou_dn_1: 0.5341  loss_ce_2: 0.5664  loss_mask_2: 0.3202  loss_dice_2: 1.302  loss_bbox_2: 0.3516  loss_giou_2: 0.6332  loss_ce_dn_2: 0.000644  loss_mask_dn_2: 0.3269  loss_dice_dn_2: 1.404  loss_bbox_dn_2: 0.2629  loss_giou_dn_2: 0.4753  loss_ce_3: 0.4152  loss_mask_3: 0.3053  loss_dice_3: 1.45  loss_bbox_3: 0.3505  loss_giou_3: 0.6011  loss_ce_dn_3: 0.001057  loss_mask_dn_3: 0.3224  loss_dice_dn_3: 1.376  loss_bbox_dn_3: 0.2463  loss_giou_dn_3: 0.4395  loss_ce_4: 0.3125  loss_mask_4: 0.316  loss_dice_4: 1.257  loss_bbox_4: 0.3489  loss_giou_4: 0.5508  loss_ce_dn_4: 0.0005866  loss_mask_dn_4: 0.317  loss_dice_dn_4: 1.376  loss_bbox_dn_4: 0.25  loss_giou_dn_4: 0.4402  loss_ce_5: 0.3353  loss_mask_5: 0.3086  loss_dice_5: 1.31  loss_bbox_5: 0.3412  loss_giou_5: 0.541  loss_ce_dn_5: 0.0006791  loss_mask_dn_5: 0.3165  loss_dice_dn_5: 1.371  loss_bbox_dn_5: 0.2479  loss_giou_dn_5: 0.4305  loss_ce_6: 0.3547  loss_mask_6: 0.3081  loss_dice_6: 1.306  loss_bbox_6: 0.3378  loss_giou_6: 0.5568  loss_ce_dn_6: 0.0006447  loss_mask_dn_6: 0.3223  loss_dice_dn_6: 1.35  loss_bbox_dn_6: 0.2428  loss_giou_dn_6: 0.4314  loss_ce_7: 0.3118  loss_mask_7: 0.3136  loss_dice_7: 1.442  loss_bbox_7: 0.3432  loss_giou_7: 0.5548  loss_ce_dn_7: 0.0005007  loss_mask_dn_7: 0.3222  loss_dice_dn_7: 1.356  loss_bbox_dn_7: 0.2446  loss_giou_dn_7: 0.4336  loss_ce_8: 0.308  loss_mask_8: 0.3447  loss_dice_8: 1.468  loss_bbox_8: 0.345  loss_giou_8: 0.5509  loss_ce_dn_8: 0.0004747  loss_mask_dn_8: 0.3294  loss_dice_dn_8: 1.335  loss_bbox_dn_8: 0.2422  loss_giou_dn_8: 0.4316  loss_ce_interm: 0.8278  loss_mask_interm: 0.321  loss_dice_interm: 1.352  loss_bbox_interm: 0.378  loss_giou_interm: 0.6447    time: 0.4889  last_time: 0.4818  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:38 d2.utils.events]:  eta: 0:15:09  iter: 2119  total_loss: 64.57  loss_ce: 0.5087  loss_mask: 0.3711  loss_dice: 1.262  loss_bbox: 0.3401  loss_giou: 0.6267  loss_ce_dn: 0.0006073  loss_mask_dn: 0.3572  loss_dice_dn: 1.278  loss_bbox_dn: 0.2625  loss_giou_dn: 0.4631  loss_ce_0: 0.8326  loss_mask_0: 0.3699  loss_dice_0: 1.47  loss_bbox_0: 0.7166  loss_giou_0: 0.8626  loss_ce_dn_0: 0.06216  loss_mask_dn_0: 0.5649  loss_dice_dn_0: 2.701  loss_bbox_dn_0: 0.8224  loss_giou_dn_0: 0.8518  loss_ce_1: 0.8886  loss_mask_1: 0.3664  loss_dice_1: 1.196  loss_bbox_1: 0.3903  loss_giou_1: 0.6605  loss_ce_dn_1: 0.0007385  loss_mask_dn_1: 0.3362  loss_dice_dn_1: 1.256  loss_bbox_dn_1: 0.3227  loss_giou_dn_1: 0.5622  loss_ce_2: 0.7838  loss_mask_2: 0.354  loss_dice_2: 1.408  loss_bbox_2: 0.3358  loss_giou_2: 0.6086  loss_ce_dn_2: 0.0005208  loss_mask_dn_2: 0.3406  loss_dice_dn_2: 1.266  loss_bbox_dn_2: 0.2693  loss_giou_dn_2: 0.4983  loss_ce_3: 0.6111  loss_mask_3: 0.3573  loss_dice_3: 1.322  loss_bbox_3: 0.4023  loss_giou_3: 0.6475  loss_ce_dn_3: 0.0006742  loss_mask_dn_3: 0.3428  loss_dice_dn_3: 1.285  loss_bbox_dn_3: 0.2593  loss_giou_dn_3: 0.4711  loss_ce_4: 0.492  loss_mask_4: 0.3359  loss_dice_4: 1.402  loss_bbox_4: 0.3656  loss_giou_4: 0.644  loss_ce_dn_4: 0.000454  loss_mask_dn_4: 0.344  loss_dice_dn_4: 1.284  loss_bbox_dn_4: 0.2609  loss_giou_dn_4: 0.478  loss_ce_5: 0.5736  loss_mask_5: 0.3498  loss_dice_5: 1.287  loss_bbox_5: 0.3525  loss_giou_5: 0.6165  loss_ce_dn_5: 0.0006417  loss_mask_dn_5: 0.3428  loss_dice_dn_5: 1.262  loss_bbox_dn_5: 0.2779  loss_giou_dn_5: 0.4701  loss_ce_6: 0.5662  loss_mask_6: 0.3457  loss_dice_6: 1.317  loss_bbox_6: 0.345  loss_giou_6: 0.6419  loss_ce_dn_6: 0.0007079  loss_mask_dn_6: 0.3512  loss_dice_dn_6: 1.263  loss_bbox_dn_6: 0.2728  loss_giou_dn_6: 0.4654  loss_ce_7: 0.5106  loss_mask_7: 0.3701  loss_dice_7: 1.298  loss_bbox_7: 0.3422  loss_giou_7: 0.6383  loss_ce_dn_7: 0.0005544  loss_mask_dn_7: 0.3461  loss_dice_dn_7: 1.273  loss_bbox_dn_7: 0.2671  loss_giou_dn_7: 0.4654  loss_ce_8: 0.502  loss_mask_8: 0.384  loss_dice_8: 1.308  loss_bbox_8: 0.3381  loss_giou_8: 0.628  loss_ce_dn_8: 0.0005312  loss_mask_dn_8: 0.3553  loss_dice_dn_8: 1.271  loss_bbox_dn_8: 0.2666  loss_giou_dn_8: 0.4662  loss_ce_interm: 0.8326  loss_mask_interm: 0.3798  loss_dice_interm: 1.383  loss_bbox_interm: 0.4867  loss_giou_interm: 0.7743    time: 0.4889  last_time: 0.4668  data_time: 0.0034  last_data_time: 0.0028   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:48 d2.utils.events]:  eta: 0:15:01  iter: 2139  total_loss: 66.31  loss_ce: 0.2746  loss_mask: 0.283  loss_dice: 1.465  loss_bbox: 0.2914  loss_giou: 0.6086  loss_ce_dn: 0.0004754  loss_mask_dn: 0.2992  loss_dice_dn: 1.357  loss_bbox_dn: 0.208  loss_giou_dn: 0.481  loss_ce_0: 0.7773  loss_mask_0: 0.2872  loss_dice_0: 1.464  loss_bbox_0: 0.7075  loss_giou_0: 0.9984  loss_ce_dn_0: 0.04149  loss_mask_dn_0: 0.3858  loss_dice_dn_0: 2.895  loss_bbox_dn_0: 0.6935  loss_giou_dn_0: 0.8472  loss_ce_1: 0.8011  loss_mask_1: 0.2987  loss_dice_1: 1.402  loss_bbox_1: 0.3648  loss_giou_1: 0.5727  loss_ce_dn_1: 0.0004587  loss_mask_dn_1: 0.2944  loss_dice_dn_1: 1.386  loss_bbox_dn_1: 0.3235  loss_giou_dn_1: 0.5598  loss_ce_2: 0.6449  loss_mask_2: 0.2947  loss_dice_2: 1.422  loss_bbox_2: 0.3351  loss_giou_2: 0.572  loss_ce_dn_2: 0.0005361  loss_mask_dn_2: 0.2959  loss_dice_dn_2: 1.392  loss_bbox_dn_2: 0.2651  loss_giou_dn_2: 0.4926  loss_ce_3: 0.3172  loss_mask_3: 0.3199  loss_dice_3: 1.37  loss_bbox_3: 0.3618  loss_giou_3: 0.5764  loss_ce_dn_3: 0.0007902  loss_mask_dn_3: 0.2933  loss_dice_dn_3: 1.376  loss_bbox_dn_3: 0.2419  loss_giou_dn_3: 0.5021  loss_ce_4: 0.2102  loss_mask_4: 0.3105  loss_dice_4: 1.408  loss_bbox_4: 0.3546  loss_giou_4: 0.5956  loss_ce_dn_4: 0.0004626  loss_mask_dn_4: 0.2934  loss_dice_dn_4: 1.385  loss_bbox_dn_4: 0.235  loss_giou_dn_4: 0.4881  loss_ce_5: 0.2582  loss_mask_5: 0.2997  loss_dice_5: 1.345  loss_bbox_5: 0.3037  loss_giou_5: 0.5936  loss_ce_dn_5: 0.000501  loss_mask_dn_5: 0.2949  loss_dice_dn_5: 1.36  loss_bbox_dn_5: 0.2319  loss_giou_dn_5: 0.4925  loss_ce_6: 0.2529  loss_mask_6: 0.3024  loss_dice_6: 1.416  loss_bbox_6: 0.298  loss_giou_6: 0.6152  loss_ce_dn_6: 0.0005947  loss_mask_dn_6: 0.3006  loss_dice_dn_6: 1.357  loss_bbox_dn_6: 0.2173  loss_giou_dn_6: 0.4815  loss_ce_7: 0.2605  loss_mask_7: 0.2961  loss_dice_7: 1.44  loss_bbox_7: 0.284  loss_giou_7: 0.6056  loss_ce_dn_7: 0.000484  loss_mask_dn_7: 0.3012  loss_dice_dn_7: 1.355  loss_bbox_dn_7: 0.2002  loss_giou_dn_7: 0.4787  loss_ce_8: 0.287  loss_mask_8: 0.2929  loss_dice_8: 1.444  loss_bbox_8: 0.294  loss_giou_8: 0.6121  loss_ce_dn_8: 0.0004413  loss_mask_dn_8: 0.3024  loss_dice_dn_8: 1.346  loss_bbox_dn_8: 0.2114  loss_giou_dn_8: 0.4791  loss_ce_interm: 0.8156  loss_mask_interm: 0.3001  loss_dice_interm: 1.424  loss_bbox_interm: 0.424  loss_giou_interm: 0.7211    time: 0.4890  last_time: 0.4496  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:21:58 d2.utils.events]:  eta: 0:14:51  iter: 2159  total_loss: 69.93  loss_ce: 0.5711  loss_mask: 0.3493  loss_dice: 1.471  loss_bbox: 0.365  loss_giou: 0.7035  loss_ce_dn: 0.001077  loss_mask_dn: 0.3259  loss_dice_dn: 1.334  loss_bbox_dn: 0.2469  loss_giou_dn: 0.5105  loss_ce_0: 0.8168  loss_mask_0: 0.3452  loss_dice_0: 1.425  loss_bbox_0: 0.6647  loss_giou_0: 1.148  loss_ce_dn_0: 0.04143  loss_mask_dn_0: 0.4664  loss_dice_dn_0: 2.823  loss_bbox_dn_0: 0.7168  loss_giou_dn_0: 0.8514  loss_ce_1: 1.108  loss_mask_1: 0.3244  loss_dice_1: 1.439  loss_bbox_1: 0.3557  loss_giou_1: 0.7787  loss_ce_dn_1: 0.0004025  loss_mask_dn_1: 0.3216  loss_dice_dn_1: 1.433  loss_bbox_dn_1: 0.299  loss_giou_dn_1: 0.5583  loss_ce_2: 0.5704  loss_mask_2: 0.3235  loss_dice_2: 1.479  loss_bbox_2: 0.3768  loss_giou_2: 0.724  loss_ce_dn_2: 0.000452  loss_mask_dn_2: 0.3246  loss_dice_dn_2: 1.363  loss_bbox_dn_2: 0.269  loss_giou_dn_2: 0.5468  loss_ce_3: 0.5397  loss_mask_3: 0.3155  loss_dice_3: 1.427  loss_bbox_3: 0.3514  loss_giou_3: 0.6901  loss_ce_dn_3: 0.0006535  loss_mask_dn_3: 0.3209  loss_dice_dn_3: 1.345  loss_bbox_dn_3: 0.2694  loss_giou_dn_3: 0.5365  loss_ce_4: 0.4728  loss_mask_4: 0.343  loss_dice_4: 1.463  loss_bbox_4: 0.3691  loss_giou_4: 0.7259  loss_ce_dn_4: 0.0003994  loss_mask_dn_4: 0.3243  loss_dice_dn_4: 1.345  loss_bbox_dn_4: 0.2501  loss_giou_dn_4: 0.5182  loss_ce_5: 0.5354  loss_mask_5: 0.3267  loss_dice_5: 1.418  loss_bbox_5: 0.3645  loss_giou_5: 0.6466  loss_ce_dn_5: 0.0008108  loss_mask_dn_5: 0.3224  loss_dice_dn_5: 1.341  loss_bbox_dn_5: 0.2507  loss_giou_dn_5: 0.5212  loss_ce_6: 0.5366  loss_mask_6: 0.3567  loss_dice_6: 1.445  loss_bbox_6: 0.3793  loss_giou_6: 0.6974  loss_ce_dn_6: 0.0009966  loss_mask_dn_6: 0.3299  loss_dice_dn_6: 1.332  loss_bbox_dn_6: 0.2502  loss_giou_dn_6: 0.5137  loss_ce_7: 0.5526  loss_mask_7: 0.3382  loss_dice_7: 1.413  loss_bbox_7: 0.3674  loss_giou_7: 0.6959  loss_ce_dn_7: 0.0008077  loss_mask_dn_7: 0.3249  loss_dice_dn_7: 1.333  loss_bbox_dn_7: 0.2487  loss_giou_dn_7: 0.5091  loss_ce_8: 0.5527  loss_mask_8: 0.3455  loss_dice_8: 1.44  loss_bbox_8: 0.3635  loss_giou_8: 0.7016  loss_ce_dn_8: 0.0008149  loss_mask_dn_8: 0.3295  loss_dice_dn_8: 1.333  loss_bbox_dn_8: 0.2526  loss_giou_dn_8: 0.5133  loss_ce_interm: 0.8611  loss_mask_interm: 0.3357  loss_dice_interm: 1.399  loss_bbox_interm: 0.401  loss_giou_interm: 0.7363    time: 0.4892  last_time: 0.5803  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:22:08 d2.utils.events]:  eta: 0:14:43  iter: 2179  total_loss: 72.95  loss_ce: 0.5514  loss_mask: 0.3851  loss_dice: 1.597  loss_bbox: 0.3312  loss_giou: 0.6762  loss_ce_dn: 0.001093  loss_mask_dn: 0.3267  loss_dice_dn: 1.505  loss_bbox_dn: 0.2438  loss_giou_dn: 0.4855  loss_ce_0: 0.7493  loss_mask_0: 0.4294  loss_dice_0: 1.728  loss_bbox_0: 0.7705  loss_giou_0: 1.126  loss_ce_dn_0: 0.0619  loss_mask_dn_0: 0.6518  loss_dice_dn_0: 2.957  loss_bbox_dn_0: 0.6996  loss_giou_dn_0: 0.8479  loss_ce_1: 0.8257  loss_mask_1: 0.382  loss_dice_1: 1.621  loss_bbox_1: 0.4012  loss_giou_1: 0.6538  loss_ce_dn_1: 0.0006817  loss_mask_dn_1: 0.3394  loss_dice_dn_1: 1.506  loss_bbox_dn_1: 0.3227  loss_giou_dn_1: 0.5693  loss_ce_2: 0.6432  loss_mask_2: 0.3869  loss_dice_2: 1.673  loss_bbox_2: 0.3649  loss_giou_2: 0.6118  loss_ce_dn_2: 0.0007023  loss_mask_dn_2: 0.3322  loss_dice_dn_2: 1.493  loss_bbox_dn_2: 0.2836  loss_giou_dn_2: 0.4943  loss_ce_3: 0.4935  loss_mask_3: 0.3671  loss_dice_3: 1.687  loss_bbox_3: 0.3866  loss_giou_3: 0.6488  loss_ce_dn_3: 0.0009986  loss_mask_dn_3: 0.3251  loss_dice_dn_3: 1.45  loss_bbox_dn_3: 0.2746  loss_giou_dn_3: 0.4827  loss_ce_4: 0.5132  loss_mask_4: 0.3626  loss_dice_4: 1.805  loss_bbox_4: 0.3695  loss_giou_4: 0.6686  loss_ce_dn_4: 0.0007557  loss_mask_dn_4: 0.34  loss_dice_dn_4: 1.438  loss_bbox_dn_4: 0.2445  loss_giou_dn_4: 0.4896  loss_ce_5: 0.6015  loss_mask_5: 0.4021  loss_dice_5: 1.59  loss_bbox_5: 0.4027  loss_giou_5: 0.6951  loss_ce_dn_5: 0.001134  loss_mask_dn_5: 0.3307  loss_dice_dn_5: 1.477  loss_bbox_dn_5: 0.2428  loss_giou_dn_5: 0.4812  loss_ce_6: 0.6041  loss_mask_6: 0.3763  loss_dice_6: 1.745  loss_bbox_6: 0.3812  loss_giou_6: 0.6838  loss_ce_dn_6: 0.001399  loss_mask_dn_6: 0.3453  loss_dice_dn_6: 1.462  loss_bbox_dn_6: 0.2525  loss_giou_dn_6: 0.4857  loss_ce_7: 0.5481  loss_mask_7: 0.3819  loss_dice_7: 1.62  loss_bbox_7: 0.3587  loss_giou_7: 0.6844  loss_ce_dn_7: 0.0008738  loss_mask_dn_7: 0.3277  loss_dice_dn_7: 1.487  loss_bbox_dn_7: 0.2498  loss_giou_dn_7: 0.4864  loss_ce_8: 0.5554  loss_mask_8: 0.3787  loss_dice_8: 1.566  loss_bbox_8: 0.358  loss_giou_8: 0.6753  loss_ce_dn_8: 0.0009916  loss_mask_dn_8: 0.3303  loss_dice_dn_8: 1.487  loss_bbox_dn_8: 0.2437  loss_giou_dn_8: 0.4869  loss_ce_interm: 0.7708  loss_mask_interm: 0.4103  loss_dice_interm: 1.764  loss_bbox_interm: 0.478  loss_giou_interm: 0.7977    time: 0.4892  last_time: 0.4929  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:22:18 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:22:18 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:22:18 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:22:18 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:22:20 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0010 s/iter. Inference: 0.0783 s/iter. Eval: 0.0087 s/iter. Total: 0.0880 s/iter. ETA=0:00:04\n",
      "[03/14 17:22:25 d2.evaluation.evaluator]: Total inference time: 0:00:05.494697 (0.088624 s / iter per device, on 1 devices)\n",
      "[03/14 17:22:25 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076321 s / iter per device, on 1 devices)\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.784\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.464\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.615\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.560\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.616\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.581\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.748\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 47.368 | 78.428 | 57.377 | 46.416 | 61.463 |  nan  |\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:22:25 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.676\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.222\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.357\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 24.383 | 67.636 | 4.884  | 22.244 | 33.488 |  nan  |\n",
      "[03/14 17:22:25 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:22:25 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: 47.3682,78.4280,57.3774,46.4161,61.4634,nan\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:22:25 d2.evaluation.testing]: copypaste: 24.3830,67.6360,4.8836,22.2440,33.4882,nan\n",
      "[03/14 17:22:25 d2.utils.events]:  eta: 0:14:34  iter: 2199  total_loss: 60.85  loss_ce: 0.3662  loss_mask: 0.316  loss_dice: 1.191  loss_bbox: 0.3338  loss_giou: 0.4669  loss_ce_dn: 0.0004385  loss_mask_dn: 0.3275  loss_dice_dn: 1.211  loss_bbox_dn: 0.2287  loss_giou_dn: 0.371  loss_ce_0: 0.8486  loss_mask_0: 0.2759  loss_dice_0: 1.324  loss_bbox_0: 0.6417  loss_giou_0: 0.7804  loss_ce_dn_0: 0.06192  loss_mask_dn_0: 0.6537  loss_dice_dn_0: 2.947  loss_bbox_dn_0: 0.7481  loss_giou_dn_0: 0.8514  loss_ce_1: 0.7016  loss_mask_1: 0.2957  loss_dice_1: 1.253  loss_bbox_1: 0.3874  loss_giou_1: 0.5668  loss_ce_dn_1: 0.0005562  loss_mask_dn_1: 0.322  loss_dice_dn_1: 1.29  loss_bbox_dn_1: 0.3109  loss_giou_dn_1: 0.5019  loss_ce_2: 0.6159  loss_mask_2: 0.3159  loss_dice_2: 1.317  loss_bbox_2: 0.321  loss_giou_2: 0.4765  loss_ce_dn_2: 0.0006347  loss_mask_dn_2: 0.3386  loss_dice_dn_2: 1.215  loss_bbox_dn_2: 0.2697  loss_giou_dn_2: 0.4303  loss_ce_3: 0.5504  loss_mask_3: 0.3419  loss_dice_3: 1.346  loss_bbox_3: 0.2638  loss_giou_3: 0.4965  loss_ce_dn_3: 0.0007664  loss_mask_dn_3: 0.3519  loss_dice_dn_3: 1.213  loss_bbox_dn_3: 0.2572  loss_giou_dn_3: 0.4271  loss_ce_4: 0.3931  loss_mask_4: 0.2946  loss_dice_4: 1.213  loss_bbox_4: 0.3109  loss_giou_4: 0.4623  loss_ce_dn_4: 0.0004119  loss_mask_dn_4: 0.332  loss_dice_dn_4: 1.221  loss_bbox_dn_4: 0.2521  loss_giou_dn_4: 0.3868  loss_ce_5: 0.3631  loss_mask_5: 0.3013  loss_dice_5: 1.244  loss_bbox_5: 0.2994  loss_giou_5: 0.459  loss_ce_dn_5: 0.0005505  loss_mask_dn_5: 0.3167  loss_dice_dn_5: 1.21  loss_bbox_dn_5: 0.2429  loss_giou_dn_5: 0.3787  loss_ce_6: 0.3461  loss_mask_6: 0.3377  loss_dice_6: 1.254  loss_bbox_6: 0.3209  loss_giou_6: 0.4704  loss_ce_dn_6: 0.0006475  loss_mask_dn_6: 0.3214  loss_dice_dn_6: 1.21  loss_bbox_dn_6: 0.237  loss_giou_dn_6: 0.381  loss_ce_7: 0.3437  loss_mask_7: 0.3  loss_dice_7: 1.238  loss_bbox_7: 0.3314  loss_giou_7: 0.4719  loss_ce_dn_7: 0.0005093  loss_mask_dn_7: 0.3236  loss_dice_dn_7: 1.194  loss_bbox_dn_7: 0.2381  loss_giou_dn_7: 0.3712  loss_ce_8: 0.3458  loss_mask_8: 0.3211  loss_dice_8: 1.2  loss_bbox_8: 0.3286  loss_giou_8: 0.4715  loss_ce_dn_8: 0.0003835  loss_mask_dn_8: 0.3411  loss_dice_dn_8: 1.212  loss_bbox_dn_8: 0.2309  loss_giou_dn_8: 0.3724  loss_ce_interm: 0.8506  loss_mask_interm: 0.277  loss_dice_interm: 1.231  loss_bbox_interm: 0.4162  loss_giou_interm: 0.65    time: 0.4893  last_time: 0.4817  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:22:35 d2.utils.events]:  eta: 0:14:25  iter: 2219  total_loss: 64.03  loss_ce: 0.4354  loss_mask: 0.4222  loss_dice: 1.424  loss_bbox: 0.2966  loss_giou: 0.5105  loss_ce_dn: 0.0002929  loss_mask_dn: 0.4095  loss_dice_dn: 1.315  loss_bbox_dn: 0.2062  loss_giou_dn: 0.4341  loss_ce_0: 0.5948  loss_mask_0: 0.3841  loss_dice_0: 1.36  loss_bbox_0: 0.8028  loss_giou_0: 1.007  loss_ce_dn_0: 0.0207  loss_mask_dn_0: 0.4976  loss_dice_dn_0: 2.229  loss_bbox_dn_0: 0.701  loss_giou_dn_0: 0.8535  loss_ce_1: 0.6736  loss_mask_1: 0.4031  loss_dice_1: 1.349  loss_bbox_1: 0.4083  loss_giou_1: 0.521  loss_ce_dn_1: 0.0005505  loss_mask_dn_1: 0.4196  loss_dice_dn_1: 1.342  loss_bbox_dn_1: 0.2898  loss_giou_dn_1: 0.4996  loss_ce_2: 0.4185  loss_mask_2: 0.4466  loss_dice_2: 1.36  loss_bbox_2: 0.3294  loss_giou_2: 0.504  loss_ce_dn_2: 0.0003444  loss_mask_dn_2: 0.4411  loss_dice_dn_2: 1.347  loss_bbox_dn_2: 0.2258  loss_giou_dn_2: 0.4502  loss_ce_3: 0.35  loss_mask_3: 0.4058  loss_dice_3: 1.383  loss_bbox_3: 0.3261  loss_giou_3: 0.5309  loss_ce_dn_3: 0.0004314  loss_mask_dn_3: 0.4231  loss_dice_dn_3: 1.342  loss_bbox_dn_3: 0.2172  loss_giou_dn_3: 0.4432  loss_ce_4: 0.3211  loss_mask_4: 0.4071  loss_dice_4: 1.434  loss_bbox_4: 0.323  loss_giou_4: 0.5751  loss_ce_dn_4: 0.000424  loss_mask_dn_4: 0.4208  loss_dice_dn_4: 1.344  loss_bbox_dn_4: 0.2112  loss_giou_dn_4: 0.4525  loss_ce_5: 0.3213  loss_mask_5: 0.4362  loss_dice_5: 1.408  loss_bbox_5: 0.3205  loss_giou_5: 0.564  loss_ce_dn_5: 0.0005983  loss_mask_dn_5: 0.429  loss_dice_dn_5: 1.316  loss_bbox_dn_5: 0.2094  loss_giou_dn_5: 0.4278  loss_ce_6: 0.3427  loss_mask_6: 0.4187  loss_dice_6: 1.414  loss_bbox_6: 0.3228  loss_giou_6: 0.5257  loss_ce_dn_6: 0.0006248  loss_mask_dn_6: 0.4242  loss_dice_dn_6: 1.333  loss_bbox_dn_6: 0.21  loss_giou_dn_6: 0.4383  loss_ce_7: 0.3954  loss_mask_7: 0.4322  loss_dice_7: 1.424  loss_bbox_7: 0.335  loss_giou_7: 0.5221  loss_ce_dn_7: 0.0003834  loss_mask_dn_7: 0.4212  loss_dice_dn_7: 1.323  loss_bbox_dn_7: 0.2063  loss_giou_dn_7: 0.4335  loss_ce_8: 0.4216  loss_mask_8: 0.4366  loss_dice_8: 1.393  loss_bbox_8: 0.3104  loss_giou_8: 0.5105  loss_ce_dn_8: 0.0003128  loss_mask_dn_8: 0.4093  loss_dice_dn_8: 1.32  loss_bbox_dn_8: 0.205  loss_giou_dn_8: 0.4377  loss_ce_interm: 0.651  loss_mask_interm: 0.3791  loss_dice_interm: 1.452  loss_bbox_interm: 0.426  loss_giou_interm: 0.6706    time: 0.4894  last_time: 0.5114  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:22:45 d2.utils.events]:  eta: 0:14:16  iter: 2239  total_loss: 58.94  loss_ce: 0.2664  loss_mask: 0.4008  loss_dice: 1.082  loss_bbox: 0.2854  loss_giou: 0.4784  loss_ce_dn: 0.0003047  loss_mask_dn: 0.3879  loss_dice_dn: 1.052  loss_bbox_dn: 0.2477  loss_giou_dn: 0.4249  loss_ce_0: 0.6517  loss_mask_0: 0.421  loss_dice_0: 1.146  loss_bbox_0: 0.6807  loss_giou_0: 0.9404  loss_ce_dn_0: 0.06186  loss_mask_dn_0: 0.5921  loss_dice_dn_0: 2.546  loss_bbox_dn_0: 0.7403  loss_giou_dn_0: 0.8437  loss_ce_1: 0.7724  loss_mask_1: 0.4615  loss_dice_1: 1.094  loss_bbox_1: 0.3433  loss_giou_1: 0.5136  loss_ce_dn_1: 0.0006798  loss_mask_dn_1: 0.4075  loss_dice_dn_1: 1.115  loss_bbox_dn_1: 0.3608  loss_giou_dn_1: 0.495  loss_ce_2: 0.4336  loss_mask_2: 0.4592  loss_dice_2: 1.152  loss_bbox_2: 0.3605  loss_giou_2: 0.5796  loss_ce_dn_2: 0.0004741  loss_mask_dn_2: 0.4127  loss_dice_dn_2: 1.069  loss_bbox_dn_2: 0.2737  loss_giou_dn_2: 0.447  loss_ce_3: 0.3741  loss_mask_3: 0.4453  loss_dice_3: 1.085  loss_bbox_3: 0.33  loss_giou_3: 0.5185  loss_ce_dn_3: 0.0005895  loss_mask_dn_3: 0.4045  loss_dice_dn_3: 1.066  loss_bbox_dn_3: 0.2912  loss_giou_dn_3: 0.4203  loss_ce_4: 0.295  loss_mask_4: 0.3774  loss_dice_4: 1.078  loss_bbox_4: 0.3217  loss_giou_4: 0.4921  loss_ce_dn_4: 0.0003033  loss_mask_dn_4: 0.3862  loss_dice_dn_4: 1.065  loss_bbox_dn_4: 0.2438  loss_giou_dn_4: 0.4243  loss_ce_5: 0.2498  loss_mask_5: 0.3804  loss_dice_5: 1.116  loss_bbox_5: 0.3303  loss_giou_5: 0.4788  loss_ce_dn_5: 0.0005034  loss_mask_dn_5: 0.3841  loss_dice_dn_5: 1.054  loss_bbox_dn_5: 0.2526  loss_giou_dn_5: 0.4289  loss_ce_6: 0.2524  loss_mask_6: 0.3893  loss_dice_6: 1.095  loss_bbox_6: 0.3057  loss_giou_6: 0.4897  loss_ce_dn_6: 0.000597  loss_mask_dn_6: 0.3904  loss_dice_dn_6: 1.045  loss_bbox_dn_6: 0.2452  loss_giou_dn_6: 0.4249  loss_ce_7: 0.2507  loss_mask_7: 0.3693  loss_dice_7: 1.112  loss_bbox_7: 0.295  loss_giou_7: 0.4782  loss_ce_dn_7: 0.0003225  loss_mask_dn_7: 0.3856  loss_dice_dn_7: 1.048  loss_bbox_dn_7: 0.255  loss_giou_dn_7: 0.4236  loss_ce_8: 0.2566  loss_mask_8: 0.3801  loss_dice_8: 1.086  loss_bbox_8: 0.2907  loss_giou_8: 0.479  loss_ce_dn_8: 0.0003046  loss_mask_dn_8: 0.3882  loss_dice_dn_8: 1.054  loss_bbox_dn_8: 0.2441  loss_giou_dn_8: 0.4235  loss_ce_interm: 0.643  loss_mask_interm: 0.46  loss_dice_interm: 1.148  loss_bbox_interm: 0.3859  loss_giou_interm: 0.6262    time: 0.4895  last_time: 0.5671  data_time: 0.0036  last_data_time: 0.0042   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:22:55 d2.utils.events]:  eta: 0:14:06  iter: 2259  total_loss: 70.78  loss_ce: 0.5341  loss_mask: 0.2961  loss_dice: 1.281  loss_bbox: 0.2966  loss_giou: 0.5735  loss_ce_dn: 0.0007786  loss_mask_dn: 0.3335  loss_dice_dn: 1.41  loss_bbox_dn: 0.231  loss_giou_dn: 0.4918  loss_ce_0: 0.7154  loss_mask_0: 0.3847  loss_dice_0: 1.458  loss_bbox_0: 0.7614  loss_giou_0: 1.201  loss_ce_dn_0: 0.06178  loss_mask_dn_0: 0.6426  loss_dice_dn_0: 2.971  loss_bbox_dn_0: 0.6829  loss_giou_dn_0: 0.8525  loss_ce_1: 0.7977  loss_mask_1: 0.3556  loss_dice_1: 1.354  loss_bbox_1: 0.3795  loss_giou_1: 0.7056  loss_ce_dn_1: 0.0007036  loss_mask_dn_1: 0.3328  loss_dice_dn_1: 1.404  loss_bbox_dn_1: 0.3432  loss_giou_dn_1: 0.572  loss_ce_2: 0.6159  loss_mask_2: 0.371  loss_dice_2: 1.395  loss_bbox_2: 0.3455  loss_giou_2: 0.6219  loss_ce_dn_2: 0.0005229  loss_mask_dn_2: 0.3497  loss_dice_dn_2: 1.436  loss_bbox_dn_2: 0.2741  loss_giou_dn_2: 0.5385  loss_ce_3: 0.5335  loss_mask_3: 0.3222  loss_dice_3: 1.331  loss_bbox_3: 0.3425  loss_giou_3: 0.6402  loss_ce_dn_3: 0.0007048  loss_mask_dn_3: 0.3424  loss_dice_dn_3: 1.38  loss_bbox_dn_3: 0.2455  loss_giou_dn_3: 0.5136  loss_ce_4: 0.3648  loss_mask_4: 0.3273  loss_dice_4: 1.213  loss_bbox_4: 0.3186  loss_giou_4: 0.5803  loss_ce_dn_4: 0.000574  loss_mask_dn_4: 0.3351  loss_dice_dn_4: 1.422  loss_bbox_dn_4: 0.229  loss_giou_dn_4: 0.5021  loss_ce_5: 0.3368  loss_mask_5: 0.3339  loss_dice_5: 1.249  loss_bbox_5: 0.298  loss_giou_5: 0.6045  loss_ce_dn_5: 0.0007676  loss_mask_dn_5: 0.3293  loss_dice_dn_5: 1.403  loss_bbox_dn_5: 0.2329  loss_giou_dn_5: 0.4965  loss_ce_6: 0.5352  loss_mask_6: 0.3009  loss_dice_6: 1.324  loss_bbox_6: 0.3188  loss_giou_6: 0.5939  loss_ce_dn_6: 0.0007266  loss_mask_dn_6: 0.3311  loss_dice_dn_6: 1.41  loss_bbox_dn_6: 0.2241  loss_giou_dn_6: 0.491  loss_ce_7: 0.5304  loss_mask_7: 0.3026  loss_dice_7: 1.282  loss_bbox_7: 0.3025  loss_giou_7: 0.5733  loss_ce_dn_7: 0.0006376  loss_mask_dn_7: 0.3386  loss_dice_dn_7: 1.399  loss_bbox_dn_7: 0.2217  loss_giou_dn_7: 0.4967  loss_ce_8: 0.5014  loss_mask_8: 0.3027  loss_dice_8: 1.306  loss_bbox_8: 0.3094  loss_giou_8: 0.5744  loss_ce_dn_8: 0.000674  loss_mask_dn_8: 0.3346  loss_dice_dn_8: 1.391  loss_bbox_dn_8: 0.2263  loss_giou_dn_8: 0.4913  loss_ce_interm: 0.8088  loss_mask_interm: 0.3642  loss_dice_interm: 1.499  loss_bbox_interm: 0.3452  loss_giou_interm: 0.7086    time: 0.4894  last_time: 0.5248  data_time: 0.0036  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:23:04 d2.utils.events]:  eta: 0:13:56  iter: 2279  total_loss: 55.06  loss_ce: 0.3362  loss_mask: 0.3362  loss_dice: 1.192  loss_bbox: 0.2977  loss_giou: 0.6327  loss_ce_dn: 0.000303  loss_mask_dn: 0.3354  loss_dice_dn: 1.144  loss_bbox_dn: 0.2736  loss_giou_dn: 0.4646  loss_ce_0: 0.6897  loss_mask_0: 0.324  loss_dice_0: 1.246  loss_bbox_0: 0.7098  loss_giou_0: 1.008  loss_ce_dn_0: 0.04528  loss_mask_dn_0: 0.4662  loss_dice_dn_0: 2.482  loss_bbox_dn_0: 0.7006  loss_giou_dn_0: 0.8544  loss_ce_1: 0.6636  loss_mask_1: 0.2757  loss_dice_1: 1.218  loss_bbox_1: 0.3909  loss_giou_1: 0.6836  loss_ce_dn_1: 0.0007111  loss_mask_dn_1: 0.292  loss_dice_dn_1: 1.189  loss_bbox_dn_1: 0.3246  loss_giou_dn_1: 0.498  loss_ce_2: 0.4863  loss_mask_2: 0.2867  loss_dice_2: 1.256  loss_bbox_2: 0.345  loss_giou_2: 0.6335  loss_ce_dn_2: 0.0005356  loss_mask_dn_2: 0.328  loss_dice_dn_2: 1.179  loss_bbox_dn_2: 0.2922  loss_giou_dn_2: 0.4719  loss_ce_3: 0.433  loss_mask_3: 0.294  loss_dice_3: 1.241  loss_bbox_3: 0.3437  loss_giou_3: 0.6559  loss_ce_dn_3: 0.0006385  loss_mask_dn_3: 0.3262  loss_dice_dn_3: 1.167  loss_bbox_dn_3: 0.2909  loss_giou_dn_3: 0.49  loss_ce_4: 0.3275  loss_mask_4: 0.3146  loss_dice_4: 1.231  loss_bbox_4: 0.3148  loss_giou_4: 0.5761  loss_ce_dn_4: 0.0003651  loss_mask_dn_4: 0.3303  loss_dice_dn_4: 1.155  loss_bbox_dn_4: 0.2823  loss_giou_dn_4: 0.4806  loss_ce_5: 0.2501  loss_mask_5: 0.3324  loss_dice_5: 1.238  loss_bbox_5: 0.3181  loss_giou_5: 0.5496  loss_ce_dn_5: 0.0004936  loss_mask_dn_5: 0.3288  loss_dice_dn_5: 1.144  loss_bbox_dn_5: 0.2757  loss_giou_dn_5: 0.4722  loss_ce_6: 0.2895  loss_mask_6: 0.3252  loss_dice_6: 1.244  loss_bbox_6: 0.3171  loss_giou_6: 0.6536  loss_ce_dn_6: 0.0004281  loss_mask_dn_6: 0.34  loss_dice_dn_6: 1.15  loss_bbox_dn_6: 0.2773  loss_giou_dn_6: 0.476  loss_ce_7: 0.3607  loss_mask_7: 0.3257  loss_dice_7: 1.177  loss_bbox_7: 0.3122  loss_giou_7: 0.6395  loss_ce_dn_7: 0.0003041  loss_mask_dn_7: 0.3318  loss_dice_dn_7: 1.146  loss_bbox_dn_7: 0.274  loss_giou_dn_7: 0.4658  loss_ce_8: 0.3167  loss_mask_8: 0.322  loss_dice_8: 1.223  loss_bbox_8: 0.2988  loss_giou_8: 0.6413  loss_ce_dn_8: 0.0002759  loss_mask_dn_8: 0.3399  loss_dice_dn_8: 1.152  loss_bbox_dn_8: 0.2761  loss_giou_dn_8: 0.4685  loss_ce_interm: 0.683  loss_mask_interm: 0.3446  loss_dice_interm: 1.263  loss_bbox_interm: 0.4051  loss_giou_interm: 0.6464    time: 0.4892  last_time: 0.4605  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:23:14 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:23:14 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:23:14 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:23:14 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:23:16 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0828 s/iter. Eval: 0.0095 s/iter. Total: 0.0932 s/iter. ETA=0:00:05\n",
      "[03/14 17:23:21 d2.evaluation.evaluator]: Total inference time: 0:00:05.396109 (0.087034 s / iter per device, on 1 devices)\n",
      "[03/14 17:23:21 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.074705 s / iter per device, on 1 devices)\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.804\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.537\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.476\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.606\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.570\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.619\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.752\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.208 | 80.449 | 53.659 | 47.585 | 60.615 |  nan  |\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:23:21 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.246\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.676\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.344\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 24.646 | 67.607 | 6.162  | 21.285 | 35.466 |  nan  |\n",
      "[03/14 17:23:21 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:23:21 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: 48.2080,80.4492,53.6590,47.5852,60.6150,nan\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:23:21 d2.evaluation.testing]: copypaste: 24.6463,67.6073,6.1624,21.2849,35.4661,nan\n",
      "[03/14 17:23:21 d2.utils.events]:  eta: 0:13:46  iter: 2299  total_loss: 69.72  loss_ce: 0.3085  loss_mask: 0.3915  loss_dice: 1.59  loss_bbox: 0.3437  loss_giou: 0.691  loss_ce_dn: 0.000422  loss_mask_dn: 0.378  loss_dice_dn: 1.4  loss_bbox_dn: 0.2743  loss_giou_dn: 0.5024  loss_ce_0: 0.7359  loss_mask_0: 0.3883  loss_dice_0: 1.586  loss_bbox_0: 0.6463  loss_giou_0: 1.144  loss_ce_dn_0: 0.02058  loss_mask_dn_0: 0.4921  loss_dice_dn_0: 2.67  loss_bbox_dn_0: 0.7683  loss_giou_dn_0: 0.8473  loss_ce_1: 0.6758  loss_mask_1: 0.389  loss_dice_1: 1.409  loss_bbox_1: 0.337  loss_giou_1: 0.6475  loss_ce_dn_1: 0.0005227  loss_mask_dn_1: 0.3913  loss_dice_dn_1: 1.403  loss_bbox_dn_1: 0.2969  loss_giou_dn_1: 0.5334  loss_ce_2: 0.5639  loss_mask_2: 0.3943  loss_dice_2: 1.489  loss_bbox_2: 0.3857  loss_giou_2: 0.6897  loss_ce_dn_2: 0.0004901  loss_mask_dn_2: 0.3784  loss_dice_dn_2: 1.415  loss_bbox_dn_2: 0.278  loss_giou_dn_2: 0.5111  loss_ce_3: 0.3825  loss_mask_3: 0.3986  loss_dice_3: 1.243  loss_bbox_3: 0.3401  loss_giou_3: 0.634  loss_ce_dn_3: 0.0006093  loss_mask_dn_3: 0.3702  loss_dice_dn_3: 1.406  loss_bbox_dn_3: 0.2746  loss_giou_dn_3: 0.5016  loss_ce_4: 0.2714  loss_mask_4: 0.3813  loss_dice_4: 1.594  loss_bbox_4: 0.3199  loss_giou_4: 0.6408  loss_ce_dn_4: 0.0003731  loss_mask_dn_4: 0.3722  loss_dice_dn_4: 1.383  loss_bbox_dn_4: 0.2735  loss_giou_dn_4: 0.4943  loss_ce_5: 0.2318  loss_mask_5: 0.3815  loss_dice_5: 1.48  loss_bbox_5: 0.349  loss_giou_5: 0.6607  loss_ce_dn_5: 0.00052  loss_mask_dn_5: 0.3667  loss_dice_dn_5: 1.387  loss_bbox_dn_5: 0.2735  loss_giou_dn_5: 0.4968  loss_ce_6: 0.2748  loss_mask_6: 0.3897  loss_dice_6: 1.563  loss_bbox_6: 0.3361  loss_giou_6: 0.6574  loss_ce_dn_6: 0.0005621  loss_mask_dn_6: 0.3779  loss_dice_dn_6: 1.395  loss_bbox_dn_6: 0.2808  loss_giou_dn_6: 0.4999  loss_ce_7: 0.2716  loss_mask_7: 0.3768  loss_dice_7: 1.579  loss_bbox_7: 0.3661  loss_giou_7: 0.6635  loss_ce_dn_7: 0.0003697  loss_mask_dn_7: 0.3814  loss_dice_dn_7: 1.385  loss_bbox_dn_7: 0.2823  loss_giou_dn_7: 0.505  loss_ce_8: 0.2679  loss_mask_8: 0.3876  loss_dice_8: 1.597  loss_bbox_8: 0.3566  loss_giou_8: 0.6944  loss_ce_dn_8: 0.0003875  loss_mask_dn_8: 0.3781  loss_dice_dn_8: 1.405  loss_bbox_dn_8: 0.2756  loss_giou_dn_8: 0.5022  loss_ce_interm: 0.7674  loss_mask_interm: 0.3959  loss_dice_interm: 1.49  loss_bbox_interm: 0.4136  loss_giou_interm: 0.7761    time: 0.4892  last_time: 0.4231  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:23:31 d2.utils.events]:  eta: 0:13:37  iter: 2319  total_loss: 59.18  loss_ce: 0.1513  loss_mask: 0.3461  loss_dice: 1.091  loss_bbox: 0.3827  loss_giou: 0.5146  loss_ce_dn: 0.0002478  loss_mask_dn: 0.3513  loss_dice_dn: 1.1  loss_bbox_dn: 0.2989  loss_giou_dn: 0.4457  loss_ce_0: 0.6776  loss_mask_0: 0.3705  loss_dice_0: 1.199  loss_bbox_0: 0.7698  loss_giou_0: 1.006  loss_ce_dn_0: 0.02052  loss_mask_dn_0: 0.54  loss_dice_dn_0: 2.609  loss_bbox_dn_0: 0.7664  loss_giou_dn_0: 0.8523  loss_ce_1: 0.7125  loss_mask_1: 0.39  loss_dice_1: 1.218  loss_bbox_1: 0.4164  loss_giou_1: 0.622  loss_ce_dn_1: 0.0005588  loss_mask_dn_1: 0.3337  loss_dice_dn_1: 1.112  loss_bbox_dn_1: 0.3343  loss_giou_dn_1: 0.4887  loss_ce_2: 0.5957  loss_mask_2: 0.3389  loss_dice_2: 1.138  loss_bbox_2: 0.4122  loss_giou_2: 0.6372  loss_ce_dn_2: 0.0004109  loss_mask_dn_2: 0.3533  loss_dice_dn_2: 1.096  loss_bbox_dn_2: 0.3229  loss_giou_dn_2: 0.449  loss_ce_3: 0.2094  loss_mask_3: 0.3727  loss_dice_3: 1.208  loss_bbox_3: 0.3768  loss_giou_3: 0.6033  loss_ce_dn_3: 0.0005652  loss_mask_dn_3: 0.3457  loss_dice_dn_3: 1.104  loss_bbox_dn_3: 0.3029  loss_giou_dn_3: 0.4485  loss_ce_4: 0.1495  loss_mask_4: 0.3549  loss_dice_4: 1.209  loss_bbox_4: 0.3804  loss_giou_4: 0.5941  loss_ce_dn_4: 0.000302  loss_mask_dn_4: 0.3376  loss_dice_dn_4: 1.089  loss_bbox_dn_4: 0.324  loss_giou_dn_4: 0.4462  loss_ce_5: 0.2385  loss_mask_5: 0.3575  loss_dice_5: 1.187  loss_bbox_5: 0.3811  loss_giou_5: 0.537  loss_ce_dn_5: 0.0005399  loss_mask_dn_5: 0.3457  loss_dice_dn_5: 1.105  loss_bbox_dn_5: 0.3075  loss_giou_dn_5: 0.4455  loss_ce_6: 0.1336  loss_mask_6: 0.3594  loss_dice_6: 1.176  loss_bbox_6: 0.3808  loss_giou_6: 0.5174  loss_ce_dn_6: 0.0005216  loss_mask_dn_6: 0.3549  loss_dice_dn_6: 1.094  loss_bbox_dn_6: 0.3027  loss_giou_dn_6: 0.4475  loss_ce_7: 0.1361  loss_mask_7: 0.3348  loss_dice_7: 1.117  loss_bbox_7: 0.398  loss_giou_7: 0.5226  loss_ce_dn_7: 0.000289  loss_mask_dn_7: 0.3682  loss_dice_dn_7: 1.1  loss_bbox_dn_7: 0.3006  loss_giou_dn_7: 0.4456  loss_ce_8: 0.1359  loss_mask_8: 0.4054  loss_dice_8: 1.111  loss_bbox_8: 0.3876  loss_giou_8: 0.5178  loss_ce_dn_8: 0.0002145  loss_mask_dn_8: 0.3497  loss_dice_dn_8: 1.091  loss_bbox_dn_8: 0.2982  loss_giou_dn_8: 0.4488  loss_ce_interm: 0.6801  loss_mask_interm: 0.3565  loss_dice_interm: 1.135  loss_bbox_interm: 0.4706  loss_giou_interm: 0.6603    time: 0.4893  last_time: 0.4661  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:23:41 d2.utils.events]:  eta: 0:13:27  iter: 2339  total_loss: 68.77  loss_ce: 0.2938  loss_mask: 0.3292  loss_dice: 1.57  loss_bbox: 0.2758  loss_giou: 0.6431  loss_ce_dn: 0.0002902  loss_mask_dn: 0.3252  loss_dice_dn: 1.434  loss_bbox_dn: 0.2148  loss_giou_dn: 0.4585  loss_ce_0: 0.8764  loss_mask_0: 0.323  loss_dice_0: 1.45  loss_bbox_0: 0.5934  loss_giou_0: 1.147  loss_ce_dn_0: 0.0616  loss_mask_dn_0: 0.6516  loss_dice_dn_0: 2.948  loss_bbox_dn_0: 0.7323  loss_giou_dn_0: 0.8496  loss_ce_1: 0.7134  loss_mask_1: 0.3311  loss_dice_1: 1.512  loss_bbox_1: 0.3638  loss_giou_1: 0.7479  loss_ce_dn_1: 0.0006499  loss_mask_dn_1: 0.3363  loss_dice_dn_1: 1.458  loss_bbox_dn_1: 0.3251  loss_giou_dn_1: 0.5163  loss_ce_2: 0.436  loss_mask_2: 0.3334  loss_dice_2: 1.472  loss_bbox_2: 0.3147  loss_giou_2: 0.6563  loss_ce_dn_2: 0.0004664  loss_mask_dn_2: 0.327  loss_dice_dn_2: 1.444  loss_bbox_dn_2: 0.2818  loss_giou_dn_2: 0.4908  loss_ce_3: 0.3068  loss_mask_3: 0.3325  loss_dice_3: 1.457  loss_bbox_3: 0.295  loss_giou_3: 0.6181  loss_ce_dn_3: 0.0006701  loss_mask_dn_3: 0.3322  loss_dice_dn_3: 1.432  loss_bbox_dn_3: 0.2305  loss_giou_dn_3: 0.4775  loss_ce_4: 0.2565  loss_mask_4: 0.3181  loss_dice_4: 1.47  loss_bbox_4: 0.289  loss_giou_4: 0.6345  loss_ce_dn_4: 0.0004272  loss_mask_dn_4: 0.3209  loss_dice_dn_4: 1.438  loss_bbox_dn_4: 0.2366  loss_giou_dn_4: 0.4692  loss_ce_5: 0.2546  loss_mask_5: 0.3106  loss_dice_5: 1.455  loss_bbox_5: 0.2861  loss_giou_5: 0.6384  loss_ce_dn_5: 0.0006011  loss_mask_dn_5: 0.3155  loss_dice_dn_5: 1.435  loss_bbox_dn_5: 0.2304  loss_giou_dn_5: 0.4598  loss_ce_6: 0.2773  loss_mask_6: 0.3179  loss_dice_6: 1.495  loss_bbox_6: 0.2969  loss_giou_6: 0.6257  loss_ce_dn_6: 0.0005944  loss_mask_dn_6: 0.3246  loss_dice_dn_6: 1.421  loss_bbox_dn_6: 0.2246  loss_giou_dn_6: 0.4575  loss_ce_7: 0.2851  loss_mask_7: 0.3235  loss_dice_7: 1.479  loss_bbox_7: 0.2851  loss_giou_7: 0.6544  loss_ce_dn_7: 0.000341  loss_mask_dn_7: 0.324  loss_dice_dn_7: 1.419  loss_bbox_dn_7: 0.219  loss_giou_dn_7: 0.4644  loss_ce_8: 0.2825  loss_mask_8: 0.3219  loss_dice_8: 1.485  loss_bbox_8: 0.2798  loss_giou_8: 0.6495  loss_ce_dn_8: 0.000281  loss_mask_dn_8: 0.325  loss_dice_dn_8: 1.431  loss_bbox_dn_8: 0.219  loss_giou_dn_8: 0.4616  loss_ce_interm: 0.8518  loss_mask_interm: 0.3262  loss_dice_interm: 1.495  loss_bbox_interm: 0.4038  loss_giou_interm: 0.7062    time: 0.4892  last_time: 0.4921  data_time: 0.0036  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:23:51 d2.utils.events]:  eta: 0:13:17  iter: 2359  total_loss: 67.31  loss_ce: 0.6624  loss_mask: 0.3049  loss_dice: 1.637  loss_bbox: 0.3034  loss_giou: 0.6671  loss_ce_dn: 0.0006108  loss_mask_dn: 0.2832  loss_dice_dn: 1.443  loss_bbox_dn: 0.217  loss_giou_dn: 0.4599  loss_ce_0: 0.8101  loss_mask_0: 0.2972  loss_dice_0: 1.646  loss_bbox_0: 0.6152  loss_giou_0: 0.9146  loss_ce_dn_0: 0.02041  loss_mask_dn_0: 0.5532  loss_dice_dn_0: 3.359  loss_bbox_dn_0: 0.6047  loss_giou_dn_0: 0.856  loss_ce_1: 0.9793  loss_mask_1: 0.2949  loss_dice_1: 1.6  loss_bbox_1: 0.372  loss_giou_1: 0.7051  loss_ce_dn_1: 0.0005276  loss_mask_dn_1: 0.3177  loss_dice_dn_1: 1.515  loss_bbox_dn_1: 0.2501  loss_giou_dn_1: 0.5423  loss_ce_2: 0.8126  loss_mask_2: 0.2929  loss_dice_2: 1.643  loss_bbox_2: 0.2693  loss_giou_2: 0.6675  loss_ce_dn_2: 0.000517  loss_mask_dn_2: 0.2838  loss_dice_dn_2: 1.475  loss_bbox_dn_2: 0.223  loss_giou_dn_2: 0.4944  loss_ce_3: 0.6532  loss_mask_3: 0.2897  loss_dice_3: 1.651  loss_bbox_3: 0.2848  loss_giou_3: 0.6921  loss_ce_dn_3: 0.0008979  loss_mask_dn_3: 0.2904  loss_dice_dn_3: 1.467  loss_bbox_dn_3: 0.2253  loss_giou_dn_3: 0.4805  loss_ce_4: 0.8183  loss_mask_4: 0.3047  loss_dice_4: 1.55  loss_bbox_4: 0.2818  loss_giou_4: 0.6532  loss_ce_dn_4: 0.0005225  loss_mask_dn_4: 0.2824  loss_dice_dn_4: 1.453  loss_bbox_dn_4: 0.2188  loss_giou_dn_4: 0.4599  loss_ce_5: 0.7518  loss_mask_5: 0.3073  loss_dice_5: 1.594  loss_bbox_5: 0.2783  loss_giou_5: 0.6562  loss_ce_dn_5: 0.0006244  loss_mask_dn_5: 0.2878  loss_dice_dn_5: 1.472  loss_bbox_dn_5: 0.2199  loss_giou_dn_5: 0.4607  loss_ce_6: 0.7501  loss_mask_6: 0.3091  loss_dice_6: 1.503  loss_bbox_6: 0.2989  loss_giou_6: 0.6519  loss_ce_dn_6: 0.0006586  loss_mask_dn_6: 0.2869  loss_dice_dn_6: 1.453  loss_bbox_dn_6: 0.2214  loss_giou_dn_6: 0.4582  loss_ce_7: 0.6941  loss_mask_7: 0.3158  loss_dice_7: 1.578  loss_bbox_7: 0.2952  loss_giou_7: 0.6737  loss_ce_dn_7: 0.0005512  loss_mask_dn_7: 0.2912  loss_dice_dn_7: 1.453  loss_bbox_dn_7: 0.2108  loss_giou_dn_7: 0.4577  loss_ce_8: 0.7442  loss_mask_8: 0.2944  loss_dice_8: 1.566  loss_bbox_8: 0.3058  loss_giou_8: 0.6645  loss_ce_dn_8: 0.0006093  loss_mask_dn_8: 0.2875  loss_dice_dn_8: 1.441  loss_bbox_dn_8: 0.2167  loss_giou_dn_8: 0.4584  loss_ce_interm: 0.9263  loss_mask_interm: 0.2928  loss_dice_interm: 1.531  loss_bbox_interm: 0.3747  loss_giou_interm: 0.7726    time: 0.4892  last_time: 0.5298  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:00 d2.utils.events]:  eta: 0:13:07  iter: 2379  total_loss: 61.97  loss_ce: 0.3202  loss_mask: 0.3491  loss_dice: 1.143  loss_bbox: 0.2706  loss_giou: 0.4349  loss_ce_dn: 0.0002522  loss_mask_dn: 0.3358  loss_dice_dn: 1.199  loss_bbox_dn: 0.2252  loss_giou_dn: 0.4144  loss_ce_0: 0.6044  loss_mask_0: 0.3713  loss_dice_0: 1.265  loss_bbox_0: 0.7327  loss_giou_0: 0.8821  loss_ce_dn_0: 0.0533  loss_mask_dn_0: 0.5822  loss_dice_dn_0: 2.758  loss_bbox_dn_0: 0.8342  loss_giou_dn_0: 0.8457  loss_ce_1: 0.6717  loss_mask_1: 0.3423  loss_dice_1: 1.178  loss_bbox_1: 0.3496  loss_giou_1: 0.4792  loss_ce_dn_1: 0.0005915  loss_mask_dn_1: 0.3379  loss_dice_dn_1: 1.233  loss_bbox_dn_1: 0.353  loss_giou_dn_1: 0.493  loss_ce_2: 0.491  loss_mask_2: 0.3431  loss_dice_2: 1.187  loss_bbox_2: 0.3295  loss_giou_2: 0.4411  loss_ce_dn_2: 0.0005046  loss_mask_dn_2: 0.3513  loss_dice_dn_2: 1.221  loss_bbox_dn_2: 0.2673  loss_giou_dn_2: 0.4504  loss_ce_3: 0.3016  loss_mask_3: 0.3457  loss_dice_3: 1.219  loss_bbox_3: 0.2937  loss_giou_3: 0.4316  loss_ce_dn_3: 0.0005165  loss_mask_dn_3: 0.356  loss_dice_dn_3: 1.221  loss_bbox_dn_3: 0.2544  loss_giou_dn_3: 0.4366  loss_ce_4: 0.2584  loss_mask_4: 0.3443  loss_dice_4: 1.213  loss_bbox_4: 0.2824  loss_giou_4: 0.4114  loss_ce_dn_4: 0.0003417  loss_mask_dn_4: 0.3552  loss_dice_dn_4: 1.214  loss_bbox_dn_4: 0.2476  loss_giou_dn_4: 0.4187  loss_ce_5: 0.2772  loss_mask_5: 0.3332  loss_dice_5: 1.166  loss_bbox_5: 0.2812  loss_giou_5: 0.4469  loss_ce_dn_5: 0.0004466  loss_mask_dn_5: 0.3467  loss_dice_dn_5: 1.209  loss_bbox_dn_5: 0.2524  loss_giou_dn_5: 0.4124  loss_ce_6: 0.3064  loss_mask_6: 0.3376  loss_dice_6: 1.151  loss_bbox_6: 0.2952  loss_giou_6: 0.4355  loss_ce_dn_6: 0.0004585  loss_mask_dn_6: 0.3439  loss_dice_dn_6: 1.194  loss_bbox_dn_6: 0.2518  loss_giou_dn_6: 0.4119  loss_ce_7: 0.3114  loss_mask_7: 0.3284  loss_dice_7: 1.199  loss_bbox_7: 0.2727  loss_giou_7: 0.4416  loss_ce_dn_7: 0.0003487  loss_mask_dn_7: 0.3417  loss_dice_dn_7: 1.194  loss_bbox_dn_7: 0.2297  loss_giou_dn_7: 0.4188  loss_ce_8: 0.3168  loss_mask_8: 0.3273  loss_dice_8: 1.171  loss_bbox_8: 0.2659  loss_giou_8: 0.437  loss_ce_dn_8: 0.0002641  loss_mask_dn_8: 0.3379  loss_dice_dn_8: 1.185  loss_bbox_dn_8: 0.23  loss_giou_dn_8: 0.4144  loss_ce_interm: 0.6598  loss_mask_interm: 0.3903  loss_dice_interm: 1.242  loss_bbox_interm: 0.3786  loss_giou_interm: 0.5981    time: 0.4891  last_time: 0.5366  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:10 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:24:10 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:24:10 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:24:10 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:24:11 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0007 s/iter. Inference: 0.0782 s/iter. Eval: 0.0086 s/iter. Total: 0.0875 s/iter. ETA=0:00:04\n",
      "[03/14 17:24:16 d2.evaluation.evaluator]: Total inference time: 0:00:05.368068 (0.086582 s / iter per device, on 1 devices)\n",
      "[03/14 17:24:16 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.074232 s / iter per device, on 1 devices)\n",
      "[03/14 17:24:16 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:24:16 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:24:16 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:24:16 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:24:16 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:24:16 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:24:16 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.800\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.556\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.488\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.573\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:24:16 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.420 | 79.984 | 55.596 | 48.755 | 59.405 |  nan  |\n",
      "[03/14 17:24:16 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:24:17 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:24:17 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:24:17 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:24:17 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.256\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.733\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.232\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.350\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:24:17 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 25.599 | 73.279 | 5.309  | 23.154 | 34.994 |  nan  |\n",
      "[03/14 17:24:17 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:24:17 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: 48.4198,79.9842,55.5963,48.7554,59.4047,nan\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:24:17 d2.evaluation.testing]: copypaste: 25.5990,73.2792,5.3090,23.1541,34.9941,nan\n",
      "[03/14 17:24:17 d2.utils.events]:  eta: 0:12:57  iter: 2399  total_loss: 66.2  loss_ce: 0.3877  loss_mask: 0.2672  loss_dice: 1.548  loss_bbox: 0.2785  loss_giou: 0.6552  loss_ce_dn: 0.0005031  loss_mask_dn: 0.2631  loss_dice_dn: 1.471  loss_bbox_dn: 0.2216  loss_giou_dn: 0.4709  loss_ce_0: 0.7977  loss_mask_0: 0.3314  loss_dice_0: 1.576  loss_bbox_0: 0.6251  loss_giou_0: 1.076  loss_ce_dn_0: 0.06151  loss_mask_dn_0: 0.6102  loss_dice_dn_0: 2.932  loss_bbox_dn_0: 0.6212  loss_giou_dn_0: 0.85  loss_ce_1: 0.6495  loss_mask_1: 0.2736  loss_dice_1: 1.606  loss_bbox_1: 0.3174  loss_giou_1: 0.6569  loss_ce_dn_1: 0.0007362  loss_mask_dn_1: 0.2847  loss_dice_dn_1: 1.467  loss_bbox_dn_1: 0.3118  loss_giou_dn_1: 0.5304  loss_ce_2: 0.5126  loss_mask_2: 0.2858  loss_dice_2: 1.596  loss_bbox_2: 0.2844  loss_giou_2: 0.6201  loss_ce_dn_2: 0.0003964  loss_mask_dn_2: 0.288  loss_dice_dn_2: 1.495  loss_bbox_dn_2: 0.2576  loss_giou_dn_2: 0.505  loss_ce_3: 0.4465  loss_mask_3: 0.2818  loss_dice_3: 1.475  loss_bbox_3: 0.2825  loss_giou_3: 0.6696  loss_ce_dn_3: 0.0005365  loss_mask_dn_3: 0.286  loss_dice_dn_3: 1.513  loss_bbox_dn_3: 0.2278  loss_giou_dn_3: 0.4625  loss_ce_4: 0.4564  loss_mask_4: 0.2742  loss_dice_4: 1.519  loss_bbox_4: 0.2688  loss_giou_4: 0.6388  loss_ce_dn_4: 0.0004094  loss_mask_dn_4: 0.2829  loss_dice_dn_4: 1.504  loss_bbox_dn_4: 0.2185  loss_giou_dn_4: 0.4727  loss_ce_5: 0.4027  loss_mask_5: 0.292  loss_dice_5: 1.596  loss_bbox_5: 0.2631  loss_giou_5: 0.6646  loss_ce_dn_5: 0.0007071  loss_mask_dn_5: 0.2752  loss_dice_dn_5: 1.474  loss_bbox_dn_5: 0.2232  loss_giou_dn_5: 0.4648  loss_ce_6: 0.3941  loss_mask_6: 0.2949  loss_dice_6: 1.552  loss_bbox_6: 0.266  loss_giou_6: 0.6526  loss_ce_dn_6: 0.0005972  loss_mask_dn_6: 0.2647  loss_dice_dn_6: 1.486  loss_bbox_dn_6: 0.2172  loss_giou_dn_6: 0.4633  loss_ce_7: 0.3621  loss_mask_7: 0.2907  loss_dice_7: 1.527  loss_bbox_7: 0.272  loss_giou_7: 0.6481  loss_ce_dn_7: 0.0003412  loss_mask_dn_7: 0.2664  loss_dice_dn_7: 1.489  loss_bbox_dn_7: 0.2232  loss_giou_dn_7: 0.4696  loss_ce_8: 0.3624  loss_mask_8: 0.2916  loss_dice_8: 1.564  loss_bbox_8: 0.2648  loss_giou_8: 0.6425  loss_ce_dn_8: 0.0003748  loss_mask_dn_8: 0.271  loss_dice_dn_8: 1.469  loss_bbox_dn_8: 0.2212  loss_giou_dn_8: 0.4702  loss_ce_interm: 0.8881  loss_mask_interm: 0.3158  loss_dice_interm: 1.63  loss_bbox_interm: 0.3698  loss_giou_interm: 0.7696    time: 0.4890  last_time: 0.5224  data_time: 0.0034  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:26 d2.utils.events]:  eta: 0:12:48  iter: 2419  total_loss: 64  loss_ce: 0.356  loss_mask: 0.3554  loss_dice: 1.391  loss_bbox: 0.368  loss_giou: 0.5739  loss_ce_dn: 0.0003236  loss_mask_dn: 0.3673  loss_dice_dn: 1.39  loss_bbox_dn: 0.2593  loss_giou_dn: 0.4811  loss_ce_0: 0.8478  loss_mask_0: 0.334  loss_dice_0: 1.421  loss_bbox_0: 0.6713  loss_giou_0: 0.9743  loss_ce_dn_0: 0.04088  loss_mask_dn_0: 0.5404  loss_dice_dn_0: 2.468  loss_bbox_dn_0: 0.7917  loss_giou_dn_0: 0.8607  loss_ce_1: 0.8549  loss_mask_1: 0.3599  loss_dice_1: 1.466  loss_bbox_1: 0.4119  loss_giou_1: 0.7416  loss_ce_dn_1: 0.0007906  loss_mask_dn_1: 0.3501  loss_dice_dn_1: 1.447  loss_bbox_dn_1: 0.3217  loss_giou_dn_1: 0.5223  loss_ce_2: 0.4947  loss_mask_2: 0.3627  loss_dice_2: 1.399  loss_bbox_2: 0.3545  loss_giou_2: 0.6297  loss_ce_dn_2: 0.0004068  loss_mask_dn_2: 0.3646  loss_dice_dn_2: 1.467  loss_bbox_dn_2: 0.3084  loss_giou_dn_2: 0.4745  loss_ce_3: 0.4039  loss_mask_3: 0.3491  loss_dice_3: 1.396  loss_bbox_3: 0.3374  loss_giou_3: 0.5696  loss_ce_dn_3: 0.0004085  loss_mask_dn_3: 0.3714  loss_dice_dn_3: 1.462  loss_bbox_dn_3: 0.2756  loss_giou_dn_3: 0.4855  loss_ce_4: 0.3578  loss_mask_4: 0.3557  loss_dice_4: 1.421  loss_bbox_4: 0.3242  loss_giou_4: 0.554  loss_ce_dn_4: 0.0003083  loss_mask_dn_4: 0.3691  loss_dice_dn_4: 1.379  loss_bbox_dn_4: 0.2489  loss_giou_dn_4: 0.4715  loss_ce_5: 0.317  loss_mask_5: 0.3585  loss_dice_5: 1.429  loss_bbox_5: 0.3334  loss_giou_5: 0.5724  loss_ce_dn_5: 0.0005157  loss_mask_dn_5: 0.374  loss_dice_dn_5: 1.371  loss_bbox_dn_5: 0.2455  loss_giou_dn_5: 0.48  loss_ce_6: 0.3304  loss_mask_6: 0.3525  loss_dice_6: 1.474  loss_bbox_6: 0.3521  loss_giou_6: 0.584  loss_ce_dn_6: 0.0004935  loss_mask_dn_6: 0.368  loss_dice_dn_6: 1.384  loss_bbox_dn_6: 0.2518  loss_giou_dn_6: 0.4719  loss_ce_7: 0.3384  loss_mask_7: 0.3405  loss_dice_7: 1.399  loss_bbox_7: 0.363  loss_giou_7: 0.574  loss_ce_dn_7: 0.0003622  loss_mask_dn_7: 0.3651  loss_dice_dn_7: 1.374  loss_bbox_dn_7: 0.2464  loss_giou_dn_7: 0.4757  loss_ce_8: 0.3359  loss_mask_8: 0.344  loss_dice_8: 1.428  loss_bbox_8: 0.3653  loss_giou_8: 0.5728  loss_ce_dn_8: 0.0002884  loss_mask_dn_8: 0.3698  loss_dice_dn_8: 1.39  loss_bbox_dn_8: 0.2623  loss_giou_dn_8: 0.4755  loss_ce_interm: 0.8188  loss_mask_interm: 0.3487  loss_dice_interm: 1.509  loss_bbox_interm: 0.3752  loss_giou_interm: 0.6732    time: 0.4890  last_time: 0.5064  data_time: 0.0035  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:37 d2.utils.events]:  eta: 0:12:39  iter: 2439  total_loss: 67.66  loss_ce: 0.3848  loss_mask: 0.3942  loss_dice: 1.334  loss_bbox: 0.4129  loss_giou: 0.5823  loss_ce_dn: 0.000567  loss_mask_dn: 0.3979  loss_dice_dn: 1.342  loss_bbox_dn: 0.3191  loss_giou_dn: 0.4649  loss_ce_0: 0.7428  loss_mask_0: 0.3936  loss_dice_0: 1.281  loss_bbox_0: 0.9151  loss_giou_0: 0.9733  loss_ce_dn_0: 0.02028  loss_mask_dn_0: 0.59  loss_dice_dn_0: 2.852  loss_bbox_dn_0: 0.7427  loss_giou_dn_0: 0.8433  loss_ce_1: 0.6966  loss_mask_1: 0.3864  loss_dice_1: 1.372  loss_bbox_1: 0.4025  loss_giou_1: 0.6513  loss_ce_dn_1: 0.0004809  loss_mask_dn_1: 0.3997  loss_dice_dn_1: 1.407  loss_bbox_dn_1: 0.3286  loss_giou_dn_1: 0.5401  loss_ce_2: 0.5638  loss_mask_2: 0.4185  loss_dice_2: 1.157  loss_bbox_2: 0.4021  loss_giou_2: 0.6828  loss_ce_dn_2: 0.0004793  loss_mask_dn_2: 0.3925  loss_dice_dn_2: 1.36  loss_bbox_dn_2: 0.3022  loss_giou_dn_2: 0.52  loss_ce_3: 0.4228  loss_mask_3: 0.4356  loss_dice_3: 1.485  loss_bbox_3: 0.375  loss_giou_3: 0.5623  loss_ce_dn_3: 0.000568  loss_mask_dn_3: 0.3983  loss_dice_dn_3: 1.402  loss_bbox_dn_3: 0.3143  loss_giou_dn_3: 0.5012  loss_ce_4: 0.4457  loss_mask_4: 0.3954  loss_dice_4: 1.289  loss_bbox_4: 0.4126  loss_giou_4: 0.5475  loss_ce_dn_4: 0.0004637  loss_mask_dn_4: 0.392  loss_dice_dn_4: 1.355  loss_bbox_dn_4: 0.3223  loss_giou_dn_4: 0.489  loss_ce_5: 0.4463  loss_mask_5: 0.4161  loss_dice_5: 1.363  loss_bbox_5: 0.4161  loss_giou_5: 0.5294  loss_ce_dn_5: 0.0005438  loss_mask_dn_5: 0.4048  loss_dice_dn_5: 1.347  loss_bbox_dn_5: 0.3158  loss_giou_dn_5: 0.467  loss_ce_6: 0.3774  loss_mask_6: 0.3902  loss_dice_6: 1.333  loss_bbox_6: 0.4395  loss_giou_6: 0.5387  loss_ce_dn_6: 0.000608  loss_mask_dn_6: 0.4024  loss_dice_dn_6: 1.319  loss_bbox_dn_6: 0.3196  loss_giou_dn_6: 0.4558  loss_ce_7: 0.3513  loss_mask_7: 0.4067  loss_dice_7: 1.277  loss_bbox_7: 0.4116  loss_giou_7: 0.5854  loss_ce_dn_7: 0.000568  loss_mask_dn_7: 0.3972  loss_dice_dn_7: 1.316  loss_bbox_dn_7: 0.314  loss_giou_dn_7: 0.4616  loss_ce_8: 0.3679  loss_mask_8: 0.3968  loss_dice_8: 1.18  loss_bbox_8: 0.4078  loss_giou_8: 0.5816  loss_ce_dn_8: 0.0005557  loss_mask_dn_8: 0.3997  loss_dice_dn_8: 1.32  loss_bbox_dn_8: 0.3139  loss_giou_dn_8: 0.4601  loss_ce_interm: 0.7888  loss_mask_interm: 0.3938  loss_dice_interm: 1.485  loss_bbox_interm: 0.4369  loss_giou_interm: 0.6948    time: 0.4892  last_time: 0.5493  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:47 d2.utils.events]:  eta: 0:12:30  iter: 2459  total_loss: 63.17  loss_ce: 0.4777  loss_mask: 0.3336  loss_dice: 1.521  loss_bbox: 0.2867  loss_giou: 0.5093  loss_ce_dn: 0.000322  loss_mask_dn: 0.3662  loss_dice_dn: 1.364  loss_bbox_dn: 0.2595  loss_giou_dn: 0.4497  loss_ce_0: 0.8037  loss_mask_0: 0.3391  loss_dice_0: 1.423  loss_bbox_0: 0.8154  loss_giou_0: 0.9919  loss_ce_dn_0: 0.02023  loss_mask_dn_0: 0.5719  loss_dice_dn_0: 2.463  loss_bbox_dn_0: 0.783  loss_giou_dn_0: 0.8504  loss_ce_1: 0.7232  loss_mask_1: 0.3572  loss_dice_1: 1.419  loss_bbox_1: 0.3551  loss_giou_1: 0.5684  loss_ce_dn_1: 0.0003776  loss_mask_dn_1: 0.374  loss_dice_dn_1: 1.373  loss_bbox_dn_1: 0.3132  loss_giou_dn_1: 0.522  loss_ce_2: 0.7223  loss_mask_2: 0.361  loss_dice_2: 1.429  loss_bbox_2: 0.3359  loss_giou_2: 0.5229  loss_ce_dn_2: 0.0004245  loss_mask_dn_2: 0.3646  loss_dice_dn_2: 1.381  loss_bbox_dn_2: 0.2816  loss_giou_dn_2: 0.4937  loss_ce_3: 0.4557  loss_mask_3: 0.3176  loss_dice_3: 1.55  loss_bbox_3: 0.3973  loss_giou_3: 0.516  loss_ce_dn_3: 0.0004493  loss_mask_dn_3: 0.3614  loss_dice_dn_3: 1.378  loss_bbox_dn_3: 0.2692  loss_giou_dn_3: 0.4862  loss_ce_4: 0.4655  loss_mask_4: 0.3301  loss_dice_4: 1.448  loss_bbox_4: 0.3587  loss_giou_4: 0.5152  loss_ce_dn_4: 0.0004539  loss_mask_dn_4: 0.3612  loss_dice_dn_4: 1.354  loss_bbox_dn_4: 0.2615  loss_giou_dn_4: 0.4569  loss_ce_5: 0.4654  loss_mask_5: 0.3284  loss_dice_5: 1.383  loss_bbox_5: 0.3281  loss_giou_5: 0.5072  loss_ce_dn_5: 0.0005234  loss_mask_dn_5: 0.3704  loss_dice_dn_5: 1.381  loss_bbox_dn_5: 0.2594  loss_giou_dn_5: 0.456  loss_ce_6: 0.4995  loss_mask_6: 0.3469  loss_dice_6: 1.386  loss_bbox_6: 0.2928  loss_giou_6: 0.5097  loss_ce_dn_6: 0.0005146  loss_mask_dn_6: 0.3662  loss_dice_dn_6: 1.365  loss_bbox_dn_6: 0.2639  loss_giou_dn_6: 0.4474  loss_ce_7: 0.4761  loss_mask_7: 0.3329  loss_dice_7: 1.395  loss_bbox_7: 0.2994  loss_giou_7: 0.5184  loss_ce_dn_7: 0.0003432  loss_mask_dn_7: 0.3675  loss_dice_dn_7: 1.353  loss_bbox_dn_7: 0.2617  loss_giou_dn_7: 0.4497  loss_ce_8: 0.4888  loss_mask_8: 0.3446  loss_dice_8: 1.31  loss_bbox_8: 0.2947  loss_giou_8: 0.5051  loss_ce_dn_8: 0.000327  loss_mask_dn_8: 0.366  loss_dice_dn_8: 1.364  loss_bbox_dn_8: 0.2622  loss_giou_dn_8: 0.4499  loss_ce_interm: 0.8445  loss_mask_interm: 0.3534  loss_dice_interm: 1.424  loss_bbox_interm: 0.3954  loss_giou_interm: 0.7097    time: 0.4893  last_time: 0.5284  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:24:57 d2.utils.events]:  eta: 0:12:21  iter: 2479  total_loss: 63.14  loss_ce: 0.3236  loss_mask: 0.291  loss_dice: 1.334  loss_bbox: 0.3048  loss_giou: 0.5749  loss_ce_dn: 0.0003199  loss_mask_dn: 0.2795  loss_dice_dn: 1.259  loss_bbox_dn: 0.2556  loss_giou_dn: 0.4508  loss_ce_0: 0.5727  loss_mask_0: 0.287  loss_dice_0: 1.312  loss_bbox_0: 0.7701  loss_giou_0: 1.018  loss_ce_dn_0: 0.02016  loss_mask_dn_0: 0.4797  loss_dice_dn_0: 2.518  loss_bbox_dn_0: 0.6956  loss_giou_dn_0: 0.8537  loss_ce_1: 0.7018  loss_mask_1: 0.3264  loss_dice_1: 1.322  loss_bbox_1: 0.3624  loss_giou_1: 0.6181  loss_ce_dn_1: 0.0005549  loss_mask_dn_1: 0.2839  loss_dice_dn_1: 1.276  loss_bbox_dn_1: 0.2996  loss_giou_dn_1: 0.4852  loss_ce_2: 0.4218  loss_mask_2: 0.3188  loss_dice_2: 1.314  loss_bbox_2: 0.3061  loss_giou_2: 0.6505  loss_ce_dn_2: 0.0005047  loss_mask_dn_2: 0.2871  loss_dice_dn_2: 1.27  loss_bbox_dn_2: 0.2723  loss_giou_dn_2: 0.4517  loss_ce_3: 0.314  loss_mask_3: 0.2939  loss_dice_3: 1.303  loss_bbox_3: 0.3111  loss_giou_3: 0.5965  loss_ce_dn_3: 0.0007209  loss_mask_dn_3: 0.2842  loss_dice_dn_3: 1.268  loss_bbox_dn_3: 0.2605  loss_giou_dn_3: 0.4509  loss_ce_4: 0.3383  loss_mask_4: 0.2831  loss_dice_4: 1.307  loss_bbox_4: 0.3137  loss_giou_4: 0.5606  loss_ce_dn_4: 0.0004541  loss_mask_dn_4: 0.2861  loss_dice_dn_4: 1.261  loss_bbox_dn_4: 0.2503  loss_giou_dn_4: 0.4412  loss_ce_5: 0.3534  loss_mask_5: 0.2857  loss_dice_5: 1.338  loss_bbox_5: 0.3105  loss_giou_5: 0.5433  loss_ce_dn_5: 0.0006447  loss_mask_dn_5: 0.2829  loss_dice_dn_5: 1.267  loss_bbox_dn_5: 0.2567  loss_giou_dn_5: 0.4468  loss_ce_6: 0.3512  loss_mask_6: 0.3001  loss_dice_6: 1.294  loss_bbox_6: 0.334  loss_giou_6: 0.558  loss_ce_dn_6: 0.0005831  loss_mask_dn_6: 0.2843  loss_dice_dn_6: 1.275  loss_bbox_dn_6: 0.2596  loss_giou_dn_6: 0.4547  loss_ce_7: 0.3103  loss_mask_7: 0.3034  loss_dice_7: 1.332  loss_bbox_7: 0.313  loss_giou_7: 0.5735  loss_ce_dn_7: 0.0003109  loss_mask_dn_7: 0.2802  loss_dice_dn_7: 1.268  loss_bbox_dn_7: 0.2533  loss_giou_dn_7: 0.4541  loss_ce_8: 0.3  loss_mask_8: 0.3084  loss_dice_8: 1.344  loss_bbox_8: 0.323  loss_giou_8: 0.577  loss_ce_dn_8: 0.0002746  loss_mask_dn_8: 0.2793  loss_dice_dn_8: 1.259  loss_bbox_dn_8: 0.2552  loss_giou_dn_8: 0.453  loss_ce_interm: 0.7112  loss_mask_interm: 0.3004  loss_dice_interm: 1.264  loss_bbox_interm: 0.3937  loss_giou_interm: 0.653    time: 0.4895  last_time: 0.5035  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:25:09 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:25:09 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:25:09 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:25:09 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:25:10 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0010 s/iter. Inference: 0.0812 s/iter. Eval: 0.0099 s/iter. Total: 0.0921 s/iter. ETA=0:00:05\n",
      "[03/14 17:25:15 d2.evaluation.evaluator]: Total inference time: 0:00:05.397434 (0.087055 s / iter per device, on 1 devices)\n",
      "[03/14 17:25:15 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075281 s / iter per device, on 1 devices)\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.797\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.538\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.477\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.586\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.364\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.748\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.022 | 79.717 | 53.799 | 47.745 | 58.607 |  nan  |\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:25:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 25.165 | 68.973 | 5.968  | 23.052 | 34.027 |  nan  |\n",
      "[03/14 17:25:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:25:15 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: 48.0218,79.7169,53.7995,47.7454,58.6067,nan\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:25:15 d2.evaluation.testing]: copypaste: 25.1647,68.9728,5.9684,23.0520,34.0270,nan\n",
      "[03/14 17:25:15 d2.utils.events]:  eta: 0:12:12  iter: 2499  total_loss: 58.68  loss_ce: 0.3347  loss_mask: 0.3376  loss_dice: 1.184  loss_bbox: 0.3151  loss_giou: 0.6225  loss_ce_dn: 0.0001987  loss_mask_dn: 0.3301  loss_dice_dn: 1.196  loss_bbox_dn: 0.2252  loss_giou_dn: 0.436  loss_ce_0: 0.5727  loss_mask_0: 0.3431  loss_dice_0: 1.237  loss_bbox_0: 0.7288  loss_giou_0: 1.095  loss_ce_dn_0: 0.02005  loss_mask_dn_0: 0.4191  loss_dice_dn_0: 2.335  loss_bbox_dn_0: 0.8119  loss_giou_dn_0: 0.849  loss_ce_1: 0.6838  loss_mask_1: 0.3222  loss_dice_1: 1.228  loss_bbox_1: 0.3947  loss_giou_1: 0.6115  loss_ce_dn_1: 0.000315  loss_mask_dn_1: 0.3413  loss_dice_dn_1: 1.225  loss_bbox_dn_1: 0.2844  loss_giou_dn_1: 0.4896  loss_ce_2: 0.4926  loss_mask_2: 0.3157  loss_dice_2: 1.224  loss_bbox_2: 0.3398  loss_giou_2: 0.6099  loss_ce_dn_2: 0.0003571  loss_mask_dn_2: 0.3391  loss_dice_dn_2: 1.187  loss_bbox_dn_2: 0.2553  loss_giou_dn_2: 0.4652  loss_ce_3: 0.3766  loss_mask_3: 0.3544  loss_dice_3: 1.267  loss_bbox_3: 0.3161  loss_giou_3: 0.6344  loss_ce_dn_3: 0.0003766  loss_mask_dn_3: 0.3285  loss_dice_dn_3: 1.2  loss_bbox_dn_3: 0.2403  loss_giou_dn_3: 0.4578  loss_ce_4: 0.3418  loss_mask_4: 0.3296  loss_dice_4: 1.261  loss_bbox_4: 0.3209  loss_giou_4: 0.6102  loss_ce_dn_4: 0.0002936  loss_mask_dn_4: 0.3359  loss_dice_dn_4: 1.193  loss_bbox_dn_4: 0.2382  loss_giou_dn_4: 0.4429  loss_ce_5: 0.3307  loss_mask_5: 0.3344  loss_dice_5: 1.231  loss_bbox_5: 0.3214  loss_giou_5: 0.6017  loss_ce_dn_5: 0.0003421  loss_mask_dn_5: 0.3346  loss_dice_dn_5: 1.194  loss_bbox_dn_5: 0.2325  loss_giou_dn_5: 0.4418  loss_ce_6: 0.3276  loss_mask_6: 0.3216  loss_dice_6: 1.25  loss_bbox_6: 0.3203  loss_giou_6: 0.6224  loss_ce_dn_6: 0.0003643  loss_mask_dn_6: 0.3301  loss_dice_dn_6: 1.182  loss_bbox_dn_6: 0.237  loss_giou_dn_6: 0.436  loss_ce_7: 0.3229  loss_mask_7: 0.3263  loss_dice_7: 1.271  loss_bbox_7: 0.318  loss_giou_7: 0.6146  loss_ce_dn_7: 0.0002302  loss_mask_dn_7: 0.3351  loss_dice_dn_7: 1.195  loss_bbox_dn_7: 0.2294  loss_giou_dn_7: 0.4364  loss_ce_8: 0.3195  loss_mask_8: 0.317  loss_dice_8: 1.28  loss_bbox_8: 0.3139  loss_giou_8: 0.609  loss_ce_dn_8: 0.0002176  loss_mask_dn_8: 0.3271  loss_dice_dn_8: 1.2  loss_bbox_dn_8: 0.2325  loss_giou_dn_8: 0.4364  loss_ce_interm: 0.5558  loss_mask_interm: 0.3548  loss_dice_interm: 1.244  loss_bbox_interm: 0.3823  loss_giou_interm: 0.6462    time: 0.4897  last_time: 0.5075  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:25:25 d2.utils.events]:  eta: 0:12:02  iter: 2519  total_loss: 55.75  loss_ce: 0.2032  loss_mask: 0.2819  loss_dice: 1.202  loss_bbox: 0.2837  loss_giou: 0.5115  loss_ce_dn: 0.000212  loss_mask_dn: 0.2824  loss_dice_dn: 1.142  loss_bbox_dn: 0.242  loss_giou_dn: 0.4548  loss_ce_0: 0.5846  loss_mask_0: 0.2812  loss_dice_0: 1.223  loss_bbox_0: 0.7254  loss_giou_0: 1.063  loss_ce_dn_0: 0.06115  loss_mask_dn_0: 0.6042  loss_dice_dn_0: 2.859  loss_bbox_dn_0: 0.7005  loss_giou_dn_0: 0.8499  loss_ce_1: 0.5699  loss_mask_1: 0.2808  loss_dice_1: 1.243  loss_bbox_1: 0.3199  loss_giou_1: 0.5295  loss_ce_dn_1: 0.0006057  loss_mask_dn_1: 0.2954  loss_dice_dn_1: 1.18  loss_bbox_dn_1: 0.2757  loss_giou_dn_1: 0.4918  loss_ce_2: 0.3446  loss_mask_2: 0.2794  loss_dice_2: 1.229  loss_bbox_2: 0.3049  loss_giou_2: 0.5316  loss_ce_dn_2: 0.0005302  loss_mask_dn_2: 0.293  loss_dice_dn_2: 1.152  loss_bbox_dn_2: 0.2511  loss_giou_dn_2: 0.4813  loss_ce_3: 0.2632  loss_mask_3: 0.2757  loss_dice_3: 1.192  loss_bbox_3: 0.2954  loss_giou_3: 0.484  loss_ce_dn_3: 0.0005083  loss_mask_dn_3: 0.2922  loss_dice_dn_3: 1.144  loss_bbox_dn_3: 0.2592  loss_giou_dn_3: 0.4611  loss_ce_4: 0.2423  loss_mask_4: 0.2665  loss_dice_4: 1.23  loss_bbox_4: 0.2915  loss_giou_4: 0.5044  loss_ce_dn_4: 0.0003475  loss_mask_dn_4: 0.2864  loss_dice_dn_4: 1.151  loss_bbox_dn_4: 0.2467  loss_giou_dn_4: 0.4329  loss_ce_5: 0.2083  loss_mask_5: 0.2833  loss_dice_5: 1.232  loss_bbox_5: 0.2718  loss_giou_5: 0.5081  loss_ce_dn_5: 0.0003957  loss_mask_dn_5: 0.2887  loss_dice_dn_5: 1.163  loss_bbox_dn_5: 0.2502  loss_giou_dn_5: 0.4505  loss_ce_6: 0.2028  loss_mask_6: 0.29  loss_dice_6: 1.245  loss_bbox_6: 0.2881  loss_giou_6: 0.5018  loss_ce_dn_6: 0.0004251  loss_mask_dn_6: 0.2875  loss_dice_dn_6: 1.139  loss_bbox_dn_6: 0.2523  loss_giou_dn_6: 0.4514  loss_ce_7: 0.2058  loss_mask_7: 0.293  loss_dice_7: 1.175  loss_bbox_7: 0.2886  loss_giou_7: 0.523  loss_ce_dn_7: 0.0002256  loss_mask_dn_7: 0.2917  loss_dice_dn_7: 1.144  loss_bbox_dn_7: 0.2492  loss_giou_dn_7: 0.4555  loss_ce_8: 0.2015  loss_mask_8: 0.2865  loss_dice_8: 1.227  loss_bbox_8: 0.2999  loss_giou_8: 0.5174  loss_ce_dn_8: 0.0002134  loss_mask_dn_8: 0.2832  loss_dice_dn_8: 1.148  loss_bbox_dn_8: 0.2465  loss_giou_dn_8: 0.4576  loss_ce_interm: 0.6104  loss_mask_interm: 0.2774  loss_dice_interm: 1.227  loss_bbox_interm: 0.3387  loss_giou_interm: 0.6065    time: 0.4896  last_time: 0.4711  data_time: 0.0042  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:25:35 d2.utils.events]:  eta: 0:11:53  iter: 2539  total_loss: 73.48  loss_ce: 0.3957  loss_mask: 0.3604  loss_dice: 1.523  loss_bbox: 0.3857  loss_giou: 0.7024  loss_ce_dn: 0.0005514  loss_mask_dn: 0.335  loss_dice_dn: 1.435  loss_bbox_dn: 0.2598  loss_giou_dn: 0.4698  loss_ce_0: 0.6727  loss_mask_0: 0.3835  loss_dice_0: 1.581  loss_bbox_0: 0.7358  loss_giou_0: 1.077  loss_ce_dn_0: 0.02004  loss_mask_dn_0: 0.5022  loss_dice_dn_0: 2.731  loss_bbox_dn_0: 0.7533  loss_giou_dn_0: 0.8539  loss_ce_1: 0.7433  loss_mask_1: 0.3441  loss_dice_1: 1.504  loss_bbox_1: 0.4205  loss_giou_1: 0.756  loss_ce_dn_1: 0.0004593  loss_mask_dn_1: 0.3247  loss_dice_dn_1: 1.462  loss_bbox_dn_1: 0.3034  loss_giou_dn_1: 0.5656  loss_ce_2: 0.4448  loss_mask_2: 0.3991  loss_dice_2: 1.569  loss_bbox_2: 0.4067  loss_giou_2: 0.688  loss_ce_dn_2: 0.0004386  loss_mask_dn_2: 0.3269  loss_dice_dn_2: 1.423  loss_bbox_dn_2: 0.2683  loss_giou_dn_2: 0.519  loss_ce_3: 0.3435  loss_mask_3: 0.3705  loss_dice_3: 1.556  loss_bbox_3: 0.3891  loss_giou_3: 0.7114  loss_ce_dn_3: 0.000656  loss_mask_dn_3: 0.3292  loss_dice_dn_3: 1.433  loss_bbox_dn_3: 0.2686  loss_giou_dn_3: 0.4994  loss_ce_4: 0.2701  loss_mask_4: 0.3712  loss_dice_4: 1.551  loss_bbox_4: 0.3916  loss_giou_4: 0.6842  loss_ce_dn_4: 0.0004352  loss_mask_dn_4: 0.3395  loss_dice_dn_4: 1.439  loss_bbox_dn_4: 0.2697  loss_giou_dn_4: 0.497  loss_ce_5: 0.2983  loss_mask_5: 0.3712  loss_dice_5: 1.523  loss_bbox_5: 0.3851  loss_giou_5: 0.6745  loss_ce_dn_5: 0.0006369  loss_mask_dn_5: 0.3339  loss_dice_dn_5: 1.424  loss_bbox_dn_5: 0.2705  loss_giou_dn_5: 0.4809  loss_ce_6: 0.4657  loss_mask_6: 0.3762  loss_dice_6: 1.541  loss_bbox_6: 0.3757  loss_giou_6: 0.6792  loss_ce_dn_6: 0.0006215  loss_mask_dn_6: 0.3383  loss_dice_dn_6: 1.442  loss_bbox_dn_6: 0.2673  loss_giou_dn_6: 0.4824  loss_ce_7: 0.396  loss_mask_7: 0.3888  loss_dice_7: 1.474  loss_bbox_7: 0.3813  loss_giou_7: 0.6801  loss_ce_dn_7: 0.0005693  loss_mask_dn_7: 0.3374  loss_dice_dn_7: 1.436  loss_bbox_dn_7: 0.2636  loss_giou_dn_7: 0.4779  loss_ce_8: 0.4384  loss_mask_8: 0.3906  loss_dice_8: 1.534  loss_bbox_8: 0.3768  loss_giou_8: 0.6883  loss_ce_dn_8: 0.0004997  loss_mask_dn_8: 0.3346  loss_dice_dn_8: 1.428  loss_bbox_dn_8: 0.2632  loss_giou_dn_8: 0.4745  loss_ce_interm: 0.758  loss_mask_interm: 0.38  loss_dice_interm: 1.574  loss_bbox_interm: 0.4824  loss_giou_interm: 0.8322    time: 0.4898  last_time: 0.5560  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:25:45 d2.utils.events]:  eta: 0:11:42  iter: 2559  total_loss: 71.52  loss_ce: 0.4908  loss_mask: 0.3985  loss_dice: 1.576  loss_bbox: 0.336  loss_giou: 0.5831  loss_ce_dn: 0.0004168  loss_mask_dn: 0.4043  loss_dice_dn: 1.457  loss_bbox_dn: 0.2547  loss_giou_dn: 0.4617  loss_ce_0: 0.7547  loss_mask_0: 0.4077  loss_dice_0: 1.536  loss_bbox_0: 0.6039  loss_giou_0: 1.054  loss_ce_dn_0: 0.06099  loss_mask_dn_0: 0.7813  loss_dice_dn_0: 2.912  loss_bbox_dn_0: 0.6846  loss_giou_dn_0: 0.8497  loss_ce_1: 0.69  loss_mask_1: 0.4083  loss_dice_1: 1.533  loss_bbox_1: 0.4063  loss_giou_1: 0.5958  loss_ce_dn_1: 0.0008794  loss_mask_dn_1: 0.4032  loss_dice_dn_1: 1.477  loss_bbox_dn_1: 0.3215  loss_giou_dn_1: 0.5302  loss_ce_2: 0.6644  loss_mask_2: 0.3904  loss_dice_2: 1.49  loss_bbox_2: 0.3492  loss_giou_2: 0.536  loss_ce_dn_2: 0.0004549  loss_mask_dn_2: 0.3975  loss_dice_dn_2: 1.435  loss_bbox_dn_2: 0.2813  loss_giou_dn_2: 0.4839  loss_ce_3: 0.4703  loss_mask_3: 0.4123  loss_dice_3: 1.587  loss_bbox_3: 0.3557  loss_giou_3: 0.5641  loss_ce_dn_3: 0.0004517  loss_mask_dn_3: 0.4049  loss_dice_dn_3: 1.429  loss_bbox_dn_3: 0.2559  loss_giou_dn_3: 0.4623  loss_ce_4: 0.6017  loss_mask_4: 0.3838  loss_dice_4: 1.563  loss_bbox_4: 0.3432  loss_giou_4: 0.5483  loss_ce_dn_4: 0.0003698  loss_mask_dn_4: 0.4082  loss_dice_dn_4: 1.46  loss_bbox_dn_4: 0.2598  loss_giou_dn_4: 0.4635  loss_ce_5: 0.4927  loss_mask_5: 0.3923  loss_dice_5: 1.551  loss_bbox_5: 0.353  loss_giou_5: 0.565  loss_ce_dn_5: 0.0005097  loss_mask_dn_5: 0.4072  loss_dice_dn_5: 1.437  loss_bbox_dn_5: 0.2503  loss_giou_dn_5: 0.4559  loss_ce_6: 0.5362  loss_mask_6: 0.3846  loss_dice_6: 1.582  loss_bbox_6: 0.3284  loss_giou_6: 0.5703  loss_ce_dn_6: 0.0005085  loss_mask_dn_6: 0.4027  loss_dice_dn_6: 1.445  loss_bbox_dn_6: 0.2499  loss_giou_dn_6: 0.4546  loss_ce_7: 0.5946  loss_mask_7: 0.3978  loss_dice_7: 1.472  loss_bbox_7: 0.3446  loss_giou_7: 0.5774  loss_ce_dn_7: 0.000362  loss_mask_dn_7: 0.4032  loss_dice_dn_7: 1.454  loss_bbox_dn_7: 0.2504  loss_giou_dn_7: 0.4609  loss_ce_8: 0.6158  loss_mask_8: 0.3855  loss_dice_8: 1.544  loss_bbox_8: 0.3278  loss_giou_8: 0.5867  loss_ce_dn_8: 0.0003943  loss_mask_dn_8: 0.4022  loss_dice_dn_8: 1.472  loss_bbox_dn_8: 0.2546  loss_giou_dn_8: 0.4615  loss_ce_interm: 0.902  loss_mask_interm: 0.4064  loss_dice_interm: 1.512  loss_bbox_interm: 0.3919  loss_giou_interm: 0.5942    time: 0.4897  last_time: 0.5012  data_time: 0.0036  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:25:55 d2.utils.events]:  eta: 0:11:33  iter: 2579  total_loss: 58.06  loss_ce: 0.179  loss_mask: 0.2663  loss_dice: 1.244  loss_bbox: 0.2259  loss_giou: 0.4706  loss_ce_dn: 0.0001847  loss_mask_dn: 0.2469  loss_dice_dn: 1.139  loss_bbox_dn: 0.2143  loss_giou_dn: 0.4117  loss_ce_0: 0.6406  loss_mask_0: 0.2384  loss_dice_0: 1.232  loss_bbox_0: 0.7029  loss_giou_0: 1.109  loss_ce_dn_0: 0.0199  loss_mask_dn_0: 0.3965  loss_dice_dn_0: 2.634  loss_bbox_dn_0: 0.7097  loss_giou_dn_0: 0.8479  loss_ce_1: 0.64  loss_mask_1: 0.2555  loss_dice_1: 1.263  loss_bbox_1: 0.2907  loss_giou_1: 0.5962  loss_ce_dn_1: 0.0003535  loss_mask_dn_1: 0.2602  loss_dice_dn_1: 1.224  loss_bbox_dn_1: 0.2553  loss_giou_dn_1: 0.481  loss_ce_2: 0.3941  loss_mask_2: 0.2509  loss_dice_2: 1.234  loss_bbox_2: 0.2455  loss_giou_2: 0.5896  loss_ce_dn_2: 0.0002913  loss_mask_dn_2: 0.2569  loss_dice_dn_2: 1.207  loss_bbox_dn_2: 0.2203  loss_giou_dn_2: 0.4309  loss_ce_3: 0.3781  loss_mask_3: 0.2602  loss_dice_3: 1.247  loss_bbox_3: 0.2458  loss_giou_3: 0.4973  loss_ce_dn_3: 0.0004677  loss_mask_dn_3: 0.2453  loss_dice_dn_3: 1.15  loss_bbox_dn_3: 0.2201  loss_giou_dn_3: 0.412  loss_ce_4: 0.1832  loss_mask_4: 0.258  loss_dice_4: 1.257  loss_bbox_4: 0.2235  loss_giou_4: 0.4791  loss_ce_dn_4: 0.0002305  loss_mask_dn_4: 0.2412  loss_dice_dn_4: 1.141  loss_bbox_dn_4: 0.199  loss_giou_dn_4: 0.4083  loss_ce_5: 0.1546  loss_mask_5: 0.2566  loss_dice_5: 1.275  loss_bbox_5: 0.226  loss_giou_5: 0.466  loss_ce_dn_5: 0.0003481  loss_mask_dn_5: 0.2413  loss_dice_dn_5: 1.138  loss_bbox_dn_5: 0.2063  loss_giou_dn_5: 0.3996  loss_ce_6: 0.1675  loss_mask_6: 0.2539  loss_dice_6: 1.311  loss_bbox_6: 0.2315  loss_giou_6: 0.4654  loss_ce_dn_6: 0.0003412  loss_mask_dn_6: 0.2459  loss_dice_dn_6: 1.13  loss_bbox_dn_6: 0.2079  loss_giou_dn_6: 0.407  loss_ce_7: 0.1552  loss_mask_7: 0.2524  loss_dice_7: 1.227  loss_bbox_7: 0.2309  loss_giou_7: 0.4696  loss_ce_dn_7: 0.0002296  loss_mask_dn_7: 0.2471  loss_dice_dn_7: 1.135  loss_bbox_dn_7: 0.2111  loss_giou_dn_7: 0.4094  loss_ce_8: 0.1709  loss_mask_8: 0.2553  loss_dice_8: 1.269  loss_bbox_8: 0.2281  loss_giou_8: 0.4702  loss_ce_dn_8: 0.0001964  loss_mask_dn_8: 0.2477  loss_dice_dn_8: 1.126  loss_bbox_dn_8: 0.2033  loss_giou_dn_8: 0.4038  loss_ce_interm: 0.5867  loss_mask_interm: 0.2523  loss_dice_interm: 1.201  loss_bbox_interm: 0.3315  loss_giou_interm: 0.6496    time: 0.4898  last_time: 0.4980  data_time: 0.0035  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:26:05 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:26:05 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:26:05 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:26:05 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:26:07 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0807 s/iter. Eval: 0.0095 s/iter. Total: 0.0910 s/iter. ETA=0:00:05\n",
      "[03/14 17:26:12 d2.evaluation.evaluator]: Total inference time: 0:00:05.555009 (0.089597 s / iter per device, on 1 devices)\n",
      "[03/14 17:26:12 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077525 s / iter per device, on 1 devices)\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.814\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.558\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.494\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.576\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.605\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.752\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 49.756 | 81.399 | 55.820 | 49.392 | 60.944 |  nan  |\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:26:12 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.223\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.335\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 24.955 | 71.527 | 5.100  | 22.307 | 35.085 |  nan  |\n",
      "[03/14 17:26:12 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:26:12 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: 49.7556,81.3994,55.8196,49.3918,60.9436,nan\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:26:12 d2.evaluation.testing]: copypaste: 24.9545,71.5269,5.0996,22.3071,35.0848,nan\n",
      "[03/14 17:26:12 d2.utils.events]:  eta: 0:11:24  iter: 2599  total_loss: 66.33  loss_ce: 0.252  loss_mask: 0.4059  loss_dice: 1.287  loss_bbox: 0.3446  loss_giou: 0.6269  loss_ce_dn: 0.0002324  loss_mask_dn: 0.3484  loss_dice_dn: 1.374  loss_bbox_dn: 0.2737  loss_giou_dn: 0.505  loss_ce_0: 0.7753  loss_mask_0: 0.3785  loss_dice_0: 1.117  loss_bbox_0: 0.7754  loss_giou_0: 0.9481  loss_ce_dn_0: 0.01989  loss_mask_dn_0: 0.4854  loss_dice_dn_0: 2.215  loss_bbox_dn_0: 0.8734  loss_giou_dn_0: 0.8499  loss_ce_1: 0.6682  loss_mask_1: 0.4084  loss_dice_1: 1.237  loss_bbox_1: 0.4113  loss_giou_1: 0.6514  loss_ce_dn_1: 0.000525  loss_mask_dn_1: 0.3502  loss_dice_dn_1: 1.393  loss_bbox_dn_1: 0.3378  loss_giou_dn_1: 0.5231  loss_ce_2: 0.4024  loss_mask_2: 0.4056  loss_dice_2: 1.177  loss_bbox_2: 0.3816  loss_giou_2: 0.6588  loss_ce_dn_2: 0.0005814  loss_mask_dn_2: 0.3384  loss_dice_dn_2: 1.366  loss_bbox_dn_2: 0.283  loss_giou_dn_2: 0.5125  loss_ce_3: 0.2182  loss_mask_3: 0.4222  loss_dice_3: 1.356  loss_bbox_3: 0.3507  loss_giou_3: 0.6517  loss_ce_dn_3: 0.0006917  loss_mask_dn_3: 0.335  loss_dice_dn_3: 1.391  loss_bbox_dn_3: 0.2673  loss_giou_dn_3: 0.5095  loss_ce_4: 0.2157  loss_mask_4: 0.3931  loss_dice_4: 1.274  loss_bbox_4: 0.3384  loss_giou_4: 0.6367  loss_ce_dn_4: 0.0003781  loss_mask_dn_4: 0.338  loss_dice_dn_4: 1.386  loss_bbox_dn_4: 0.2775  loss_giou_dn_4: 0.5004  loss_ce_5: 0.2767  loss_mask_5: 0.3715  loss_dice_5: 1.291  loss_bbox_5: 0.3339  loss_giou_5: 0.65  loss_ce_dn_5: 0.0004731  loss_mask_dn_5: 0.352  loss_dice_dn_5: 1.393  loss_bbox_dn_5: 0.2777  loss_giou_dn_5: 0.5075  loss_ce_6: 0.2453  loss_mask_6: 0.3728  loss_dice_6: 1.305  loss_bbox_6: 0.339  loss_giou_6: 0.6421  loss_ce_dn_6: 0.0005285  loss_mask_dn_6: 0.3496  loss_dice_dn_6: 1.396  loss_bbox_dn_6: 0.2788  loss_giou_dn_6: 0.5053  loss_ce_7: 0.2085  loss_mask_7: 0.4161  loss_dice_7: 1.201  loss_bbox_7: 0.3442  loss_giou_7: 0.6353  loss_ce_dn_7: 0.000267  loss_mask_dn_7: 0.3453  loss_dice_dn_7: 1.383  loss_bbox_dn_7: 0.2738  loss_giou_dn_7: 0.5043  loss_ce_8: 0.2329  loss_mask_8: 0.4126  loss_dice_8: 1.212  loss_bbox_8: 0.3479  loss_giou_8: 0.6312  loss_ce_dn_8: 0.0002742  loss_mask_dn_8: 0.3551  loss_dice_dn_8: 1.372  loss_bbox_dn_8: 0.2741  loss_giou_dn_8: 0.5068  loss_ce_interm: 0.8545  loss_mask_interm: 0.3826  loss_dice_interm: 1.243  loss_bbox_interm: 0.5011  loss_giou_interm: 0.6557    time: 0.4898  last_time: 0.4903  data_time: 0.0037  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:26:22 d2.utils.events]:  eta: 0:11:14  iter: 2619  total_loss: 63.05  loss_ce: 0.2722  loss_mask: 0.4002  loss_dice: 1.184  loss_bbox: 0.3327  loss_giou: 0.5472  loss_ce_dn: 0.0003409  loss_mask_dn: 0.4072  loss_dice_dn: 1.145  loss_bbox_dn: 0.2438  loss_giou_dn: 0.4515  loss_ce_0: 0.7051  loss_mask_0: 0.3992  loss_dice_0: 1.221  loss_bbox_0: 0.651  loss_giou_0: 0.8237  loss_ce_dn_0: 0.0321  loss_mask_dn_0: 0.6193  loss_dice_dn_0: 2.682  loss_bbox_dn_0: 0.8278  loss_giou_dn_0: 0.8517  loss_ce_1: 0.6735  loss_mask_1: 0.4323  loss_dice_1: 1.187  loss_bbox_1: 0.377  loss_giou_1: 0.5875  loss_ce_dn_1: 0.000499  loss_mask_dn_1: 0.417  loss_dice_dn_1: 1.193  loss_bbox_dn_1: 0.3367  loss_giou_dn_1: 0.5033  loss_ce_2: 0.4221  loss_mask_2: 0.4684  loss_dice_2: 1.227  loss_bbox_2: 0.3307  loss_giou_2: 0.6024  loss_ce_dn_2: 0.0005285  loss_mask_dn_2: 0.4231  loss_dice_dn_2: 1.146  loss_bbox_dn_2: 0.2779  loss_giou_dn_2: 0.4844  loss_ce_3: 0.3317  loss_mask_3: 0.4703  loss_dice_3: 1.206  loss_bbox_3: 0.3064  loss_giou_3: 0.6069  loss_ce_dn_3: 0.0005822  loss_mask_dn_3: 0.4141  loss_dice_dn_3: 1.163  loss_bbox_dn_3: 0.2613  loss_giou_dn_3: 0.4561  loss_ce_4: 0.3705  loss_mask_4: 0.4394  loss_dice_4: 1.19  loss_bbox_4: 0.3269  loss_giou_4: 0.5681  loss_ce_dn_4: 0.000414  loss_mask_dn_4: 0.4112  loss_dice_dn_4: 1.161  loss_bbox_dn_4: 0.2483  loss_giou_dn_4: 0.4474  loss_ce_5: 0.3155  loss_mask_5: 0.4089  loss_dice_5: 1.178  loss_bbox_5: 0.3135  loss_giou_5: 0.5649  loss_ce_dn_5: 0.0006604  loss_mask_dn_5: 0.4155  loss_dice_dn_5: 1.154  loss_bbox_dn_5: 0.25  loss_giou_dn_5: 0.4493  loss_ce_6: 0.2845  loss_mask_6: 0.4071  loss_dice_6: 1.162  loss_bbox_6: 0.3195  loss_giou_6: 0.58  loss_ce_dn_6: 0.0005676  loss_mask_dn_6: 0.4049  loss_dice_dn_6: 1.128  loss_bbox_dn_6: 0.245  loss_giou_dn_6: 0.4513  loss_ce_7: 0.2701  loss_mask_7: 0.4268  loss_dice_7: 1.229  loss_bbox_7: 0.3177  loss_giou_7: 0.5599  loss_ce_dn_7: 0.0003786  loss_mask_dn_7: 0.4044  loss_dice_dn_7: 1.134  loss_bbox_dn_7: 0.2418  loss_giou_dn_7: 0.455  loss_ce_8: 0.2656  loss_mask_8: 0.4188  loss_dice_8: 1.199  loss_bbox_8: 0.3253  loss_giou_8: 0.546  loss_ce_dn_8: 0.0003481  loss_mask_dn_8: 0.4082  loss_dice_dn_8: 1.148  loss_bbox_dn_8: 0.2424  loss_giou_dn_8: 0.4513  loss_ce_interm: 0.742  loss_mask_interm: 0.442  loss_dice_interm: 1.198  loss_bbox_interm: 0.4416  loss_giou_interm: 0.697    time: 0.4898  last_time: 0.4563  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:26:31 d2.utils.events]:  eta: 0:11:04  iter: 2639  total_loss: 68.81  loss_ce: 0.4111  loss_mask: 0.3138  loss_dice: 1.701  loss_bbox: 0.2629  loss_giou: 0.5516  loss_ce_dn: 0.0004227  loss_mask_dn: 0.3415  loss_dice_dn: 1.452  loss_bbox_dn: 0.2158  loss_giou_dn: 0.4661  loss_ce_0: 0.7748  loss_mask_0: 0.3555  loss_dice_0: 1.488  loss_bbox_0: 0.7683  loss_giou_0: 1.207  loss_ce_dn_0: 0.06071  loss_mask_dn_0: 0.5732  loss_dice_dn_0: 2.941  loss_bbox_dn_0: 0.6927  loss_giou_dn_0: 0.8489  loss_ce_1: 0.8391  loss_mask_1: 0.3494  loss_dice_1: 1.736  loss_bbox_1: 0.3134  loss_giou_1: 0.6304  loss_ce_dn_1: 0.000554  loss_mask_dn_1: 0.3355  loss_dice_dn_1: 1.52  loss_bbox_dn_1: 0.2881  loss_giou_dn_1: 0.5334  loss_ce_2: 0.6794  loss_mask_2: 0.3118  loss_dice_2: 1.604  loss_bbox_2: 0.2805  loss_giou_2: 0.5972  loss_ce_dn_2: 0.0004439  loss_mask_dn_2: 0.3319  loss_dice_dn_2: 1.487  loss_bbox_dn_2: 0.2536  loss_giou_dn_2: 0.4743  loss_ce_3: 0.5282  loss_mask_3: 0.3134  loss_dice_3: 1.556  loss_bbox_3: 0.286  loss_giou_3: 0.5793  loss_ce_dn_3: 0.000644  loss_mask_dn_3: 0.343  loss_dice_dn_3: 1.464  loss_bbox_dn_3: 0.2376  loss_giou_dn_3: 0.4634  loss_ce_4: 0.4513  loss_mask_4: 0.3511  loss_dice_4: 1.654  loss_bbox_4: 0.2604  loss_giou_4: 0.582  loss_ce_dn_4: 0.0004555  loss_mask_dn_4: 0.3359  loss_dice_dn_4: 1.439  loss_bbox_dn_4: 0.2228  loss_giou_dn_4: 0.4704  loss_ce_5: 0.3981  loss_mask_5: 0.3333  loss_dice_5: 1.551  loss_bbox_5: 0.2486  loss_giou_5: 0.5465  loss_ce_dn_5: 0.0006094  loss_mask_dn_5: 0.3331  loss_dice_dn_5: 1.462  loss_bbox_dn_5: 0.2181  loss_giou_dn_5: 0.4654  loss_ce_6: 0.413  loss_mask_6: 0.3264  loss_dice_6: 1.6  loss_bbox_6: 0.2592  loss_giou_6: 0.5534  loss_ce_dn_6: 0.0005735  loss_mask_dn_6: 0.3329  loss_dice_dn_6: 1.451  loss_bbox_dn_6: 0.2169  loss_giou_dn_6: 0.4707  loss_ce_7: 0.4133  loss_mask_7: 0.3211  loss_dice_7: 1.632  loss_bbox_7: 0.2611  loss_giou_7: 0.545  loss_ce_dn_7: 0.0004653  loss_mask_dn_7: 0.3437  loss_dice_dn_7: 1.461  loss_bbox_dn_7: 0.2128  loss_giou_dn_7: 0.473  loss_ce_8: 0.4086  loss_mask_8: 0.3377  loss_dice_8: 1.71  loss_bbox_8: 0.261  loss_giou_8: 0.5557  loss_ce_dn_8: 0.000439  loss_mask_dn_8: 0.3466  loss_dice_dn_8: 1.468  loss_bbox_dn_8: 0.2169  loss_giou_dn_8: 0.4675  loss_ce_interm: 0.7748  loss_mask_interm: 0.33  loss_dice_interm: 1.468  loss_bbox_interm: 0.3625  loss_giou_interm: 0.6601    time: 0.4897  last_time: 0.4839  data_time: 0.0036  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:26:41 d2.utils.events]:  eta: 0:10:55  iter: 2659  total_loss: 59.26  loss_ce: 0.3556  loss_mask: 0.3686  loss_dice: 1.192  loss_bbox: 0.2645  loss_giou: 0.4385  loss_ce_dn: 0.0002318  loss_mask_dn: 0.4291  loss_dice_dn: 1.256  loss_bbox_dn: 0.2427  loss_giou_dn: 0.3786  loss_ce_0: 0.7284  loss_mask_0: 0.4525  loss_dice_0: 1.172  loss_bbox_0: 0.7194  loss_giou_0: 0.9287  loss_ce_dn_0: 0.05252  loss_mask_dn_0: 0.7938  loss_dice_dn_0: 2.371  loss_bbox_dn_0: 0.884  loss_giou_dn_0: 0.8518  loss_ce_1: 0.6959  loss_mask_1: 0.433  loss_dice_1: 1.248  loss_bbox_1: 0.3384  loss_giou_1: 0.4688  loss_ce_dn_1: 0.0006114  loss_mask_dn_1: 0.4181  loss_dice_dn_1: 1.303  loss_bbox_dn_1: 0.3114  loss_giou_dn_1: 0.4697  loss_ce_2: 0.5514  loss_mask_2: 0.3705  loss_dice_2: 1.246  loss_bbox_2: 0.3032  loss_giou_2: 0.4511  loss_ce_dn_2: 0.0005453  loss_mask_dn_2: 0.4164  loss_dice_dn_2: 1.269  loss_bbox_dn_2: 0.2801  loss_giou_dn_2: 0.4117  loss_ce_3: 0.4458  loss_mask_3: 0.3584  loss_dice_3: 1.151  loss_bbox_3: 0.2676  loss_giou_3: 0.437  loss_ce_dn_3: 0.0005176  loss_mask_dn_3: 0.4212  loss_dice_dn_3: 1.276  loss_bbox_dn_3: 0.2452  loss_giou_dn_3: 0.373  loss_ce_4: 0.4343  loss_mask_4: 0.3652  loss_dice_4: 1.172  loss_bbox_4: 0.2637  loss_giou_4: 0.4781  loss_ce_dn_4: 0.0003295  loss_mask_dn_4: 0.4245  loss_dice_dn_4: 1.277  loss_bbox_dn_4: 0.2573  loss_giou_dn_4: 0.3865  loss_ce_5: 0.3755  loss_mask_5: 0.3701  loss_dice_5: 1.178  loss_bbox_5: 0.2619  loss_giou_5: 0.4609  loss_ce_dn_5: 0.0005535  loss_mask_dn_5: 0.4259  loss_dice_dn_5: 1.267  loss_bbox_dn_5: 0.2486  loss_giou_dn_5: 0.3821  loss_ce_6: 0.3943  loss_mask_6: 0.3633  loss_dice_6: 1.222  loss_bbox_6: 0.2626  loss_giou_6: 0.4622  loss_ce_dn_6: 0.0004076  loss_mask_dn_6: 0.4273  loss_dice_dn_6: 1.255  loss_bbox_dn_6: 0.2521  loss_giou_dn_6: 0.3889  loss_ce_7: 0.3791  loss_mask_7: 0.3809  loss_dice_7: 1.163  loss_bbox_7: 0.2567  loss_giou_7: 0.4474  loss_ce_dn_7: 0.0003547  loss_mask_dn_7: 0.4349  loss_dice_dn_7: 1.247  loss_bbox_dn_7: 0.2441  loss_giou_dn_7: 0.3756  loss_ce_8: 0.3509  loss_mask_8: 0.3752  loss_dice_8: 1.2  loss_bbox_8: 0.264  loss_giou_8: 0.4422  loss_ce_dn_8: 0.0002718  loss_mask_dn_8: 0.4326  loss_dice_dn_8: 1.251  loss_bbox_dn_8: 0.2452  loss_giou_dn_8: 0.383  loss_ce_interm: 0.7817  loss_mask_interm: 0.463  loss_dice_interm: 1.247  loss_bbox_interm: 0.3837  loss_giou_interm: 0.6399    time: 0.4898  last_time: 0.5157  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:26:51 d2.utils.events]:  eta: 0:10:45  iter: 2679  total_loss: 60.57  loss_ce: 0.4018  loss_mask: 0.264  loss_dice: 1.45  loss_bbox: 0.271  loss_giou: 0.5987  loss_ce_dn: 0.0002174  loss_mask_dn: 0.2721  loss_dice_dn: 1.484  loss_bbox_dn: 0.2162  loss_giou_dn: 0.4226  loss_ce_0: 0.5381  loss_mask_0: 0.2681  loss_dice_0: 1.513  loss_bbox_0: 0.6046  loss_giou_0: 1.113  loss_ce_dn_0: 0.04008  loss_mask_dn_0: 0.3868  loss_dice_dn_0: 2.616  loss_bbox_dn_0: 0.6328  loss_giou_dn_0: 0.8515  loss_ce_1: 0.8036  loss_mask_1: 0.2689  loss_dice_1: 1.6  loss_bbox_1: 0.3558  loss_giou_1: 0.6553  loss_ce_dn_1: 0.0005008  loss_mask_dn_1: 0.2822  loss_dice_dn_1: 1.574  loss_bbox_dn_1: 0.2738  loss_giou_dn_1: 0.5216  loss_ce_2: 0.5341  loss_mask_2: 0.249  loss_dice_2: 1.502  loss_bbox_2: 0.3198  loss_giou_2: 0.6154  loss_ce_dn_2: 0.0003927  loss_mask_dn_2: 0.2795  loss_dice_dn_2: 1.563  loss_bbox_dn_2: 0.2367  loss_giou_dn_2: 0.4593  loss_ce_3: 0.5098  loss_mask_3: 0.2647  loss_dice_3: 1.486  loss_bbox_3: 0.2698  loss_giou_3: 0.6206  loss_ce_dn_3: 0.00043  loss_mask_dn_3: 0.2713  loss_dice_dn_3: 1.559  loss_bbox_dn_3: 0.2116  loss_giou_dn_3: 0.4353  loss_ce_4: 0.4321  loss_mask_4: 0.2544  loss_dice_4: 1.512  loss_bbox_4: 0.2969  loss_giou_4: 0.5884  loss_ce_dn_4: 0.00028  loss_mask_dn_4: 0.2727  loss_dice_dn_4: 1.539  loss_bbox_dn_4: 0.2127  loss_giou_dn_4: 0.4346  loss_ce_5: 0.3548  loss_mask_5: 0.2697  loss_dice_5: 1.444  loss_bbox_5: 0.2619  loss_giou_5: 0.5959  loss_ce_dn_5: 0.0004073  loss_mask_dn_5: 0.274  loss_dice_dn_5: 1.523  loss_bbox_dn_5: 0.2223  loss_giou_dn_5: 0.4248  loss_ce_6: 0.4569  loss_mask_6: 0.2709  loss_dice_6: 1.491  loss_bbox_6: 0.2655  loss_giou_6: 0.5602  loss_ce_dn_6: 0.0003954  loss_mask_dn_6: 0.2748  loss_dice_dn_6: 1.501  loss_bbox_dn_6: 0.2085  loss_giou_dn_6: 0.4154  loss_ce_7: 0.415  loss_mask_7: 0.2783  loss_dice_7: 1.527  loss_bbox_7: 0.2689  loss_giou_7: 0.5954  loss_ce_dn_7: 0.0002547  loss_mask_dn_7: 0.2772  loss_dice_dn_7: 1.498  loss_bbox_dn_7: 0.2077  loss_giou_dn_7: 0.417  loss_ce_8: 0.4099  loss_mask_8: 0.2714  loss_dice_8: 1.487  loss_bbox_8: 0.2781  loss_giou_8: 0.6034  loss_ce_dn_8: 0.0002045  loss_mask_dn_8: 0.2715  loss_dice_dn_8: 1.492  loss_bbox_dn_8: 0.2166  loss_giou_dn_8: 0.4194  loss_ce_interm: 0.6098  loss_mask_interm: 0.2636  loss_dice_interm: 1.424  loss_bbox_interm: 0.3195  loss_giou_interm: 0.7604    time: 0.4897  last_time: 0.4702  data_time: 0.0035  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:02 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:27:02 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:27:02 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:27:02 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:27:03 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0022 s/iter. Inference: 0.0822 s/iter. Eval: 0.0091 s/iter. Total: 0.0935 s/iter. ETA=0:00:05\n",
      "[03/14 17:27:08 d2.evaluation.evaluator]: Inference done 66/67. Dataloading: 0.0013 s/iter. Inference: 0.0804 s/iter. Eval: 0.0097 s/iter. Total: 0.0915 s/iter. ETA=0:00:00\n",
      "[03/14 17:27:08 d2.evaluation.evaluator]: Total inference time: 0:00:05.707368 (0.092054 s / iter per device, on 1 devices)\n",
      "[03/14 17:27:08 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.080013 s / iter per device, on 1 devices)\n",
      "[03/14 17:27:08 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:27:08 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:27:08 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:27:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:27:08 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:27:08 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:27:08 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.793\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.595\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.557\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.618\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.578\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:27:08 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.882 | 79.321 | 59.353 | 49.031 | 59.549 |  nan  |\n",
      "[03/14 17:27:08 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:27:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:27:09 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "[03/14 17:27:09 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:27:09 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.057\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.363\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.342\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:27:09 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 25.699 | 71.270 | 5.655  | 23.054 | 36.066 |  nan  |\n",
      "[03/14 17:27:09 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:27:09 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: 48.8817,79.3214,59.3534,49.0314,59.5492,nan\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:27:09 d2.evaluation.testing]: copypaste: 25.6991,71.2705,5.6553,23.0539,36.0657,nan\n",
      "[03/14 17:27:09 d2.utils.events]:  eta: 0:10:35  iter: 2699  total_loss: 62.44  loss_ce: 0.3394  loss_mask: 0.432  loss_dice: 1.22  loss_bbox: 0.3317  loss_giou: 0.4684  loss_ce_dn: 0.0003339  loss_mask_dn: 0.4396  loss_dice_dn: 1.118  loss_bbox_dn: 0.289  loss_giou_dn: 0.3685  loss_ce_0: 0.786  loss_mask_0: 0.4513  loss_dice_0: 1.254  loss_bbox_0: 0.7767  loss_giou_0: 0.9355  loss_ce_dn_0: 0.01957  loss_mask_dn_0: 0.7921  loss_dice_dn_0: 2.408  loss_bbox_dn_0: 0.9343  loss_giou_dn_0: 0.8494  loss_ce_1: 0.7498  loss_mask_1: 0.4339  loss_dice_1: 1.194  loss_bbox_1: 0.3693  loss_giou_1: 0.5234  loss_ce_dn_1: 0.0007869  loss_mask_dn_1: 0.4249  loss_dice_dn_1: 1.153  loss_bbox_dn_1: 0.3498  loss_giou_dn_1: 0.4491  loss_ce_2: 0.5711  loss_mask_2: 0.4516  loss_dice_2: 1.218  loss_bbox_2: 0.3467  loss_giou_2: 0.4947  loss_ce_dn_2: 0.0005741  loss_mask_dn_2: 0.4389  loss_dice_dn_2: 1.161  loss_bbox_dn_2: 0.3589  loss_giou_dn_2: 0.4145  loss_ce_3: 0.3681  loss_mask_3: 0.4484  loss_dice_3: 1.216  loss_bbox_3: 0.3524  loss_giou_3: 0.4899  loss_ce_dn_3: 0.0005043  loss_mask_dn_3: 0.4358  loss_dice_dn_3: 1.15  loss_bbox_dn_3: 0.333  loss_giou_dn_3: 0.3997  loss_ce_4: 0.299  loss_mask_4: 0.4463  loss_dice_4: 1.234  loss_bbox_4: 0.338  loss_giou_4: 0.4804  loss_ce_dn_4: 0.000416  loss_mask_dn_4: 0.4345  loss_dice_dn_4: 1.151  loss_bbox_dn_4: 0.2872  loss_giou_dn_4: 0.3782  loss_ce_5: 0.2913  loss_mask_5: 0.453  loss_dice_5: 1.23  loss_bbox_5: 0.3388  loss_giou_5: 0.4863  loss_ce_dn_5: 0.0006055  loss_mask_dn_5: 0.4312  loss_dice_dn_5: 1.141  loss_bbox_dn_5: 0.306  loss_giou_dn_5: 0.3634  loss_ce_6: 0.2973  loss_mask_6: 0.4537  loss_dice_6: 1.198  loss_bbox_6: 0.3339  loss_giou_6: 0.4819  loss_ce_dn_6: 0.0005817  loss_mask_dn_6: 0.4407  loss_dice_dn_6: 1.141  loss_bbox_dn_6: 0.3038  loss_giou_dn_6: 0.3627  loss_ce_7: 0.336  loss_mask_7: 0.4496  loss_dice_7: 1.196  loss_bbox_7: 0.3385  loss_giou_7: 0.4784  loss_ce_dn_7: 0.0003298  loss_mask_dn_7: 0.4405  loss_dice_dn_7: 1.115  loss_bbox_dn_7: 0.289  loss_giou_dn_7: 0.3659  loss_ce_8: 0.3511  loss_mask_8: 0.4421  loss_dice_8: 1.221  loss_bbox_8: 0.3265  loss_giou_8: 0.4754  loss_ce_dn_8: 0.0003037  loss_mask_dn_8: 0.4434  loss_dice_dn_8: 1.127  loss_bbox_dn_8: 0.2843  loss_giou_dn_8: 0.3675  loss_ce_interm: 0.8162  loss_mask_interm: 0.4229  loss_dice_interm: 1.145  loss_bbox_interm: 0.4369  loss_giou_interm: 0.5862    time: 0.4898  last_time: 0.5021  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:18 d2.utils.events]:  eta: 0:10:26  iter: 2719  total_loss: 64.14  loss_ce: 0.4768  loss_mask: 0.343  loss_dice: 1.284  loss_bbox: 0.3265  loss_giou: 0.5301  loss_ce_dn: 0.0004034  loss_mask_dn: 0.3571  loss_dice_dn: 1.333  loss_bbox_dn: 0.2366  loss_giou_dn: 0.4601  loss_ce_0: 0.669  loss_mask_0: 0.3349  loss_dice_0: 1.468  loss_bbox_0: 0.6605  loss_giou_0: 0.9786  loss_ce_dn_0: 0.06026  loss_mask_dn_0: 0.6191  loss_dice_dn_0: 2.614  loss_bbox_dn_0: 0.6606  loss_giou_dn_0: 0.848  loss_ce_1: 0.6949  loss_mask_1: 0.3312  loss_dice_1: 1.31  loss_bbox_1: 0.4372  loss_giou_1: 0.5989  loss_ce_dn_1: 0.0008232  loss_mask_dn_1: 0.3765  loss_dice_dn_1: 1.333  loss_bbox_dn_1: 0.3268  loss_giou_dn_1: 0.5141  loss_ce_2: 0.5317  loss_mask_2: 0.3369  loss_dice_2: 1.213  loss_bbox_2: 0.3253  loss_giou_2: 0.5693  loss_ce_dn_2: 0.0006093  loss_mask_dn_2: 0.3685  loss_dice_dn_2: 1.351  loss_bbox_dn_2: 0.2936  loss_giou_dn_2: 0.4728  loss_ce_3: 0.4177  loss_mask_3: 0.3444  loss_dice_3: 1.192  loss_bbox_3: 0.3361  loss_giou_3: 0.5402  loss_ce_dn_3: 0.0007328  loss_mask_dn_3: 0.3584  loss_dice_dn_3: 1.345  loss_bbox_dn_3: 0.2796  loss_giou_dn_3: 0.4715  loss_ce_4: 0.4534  loss_mask_4: 0.3538  loss_dice_4: 1.236  loss_bbox_4: 0.3285  loss_giou_4: 0.5263  loss_ce_dn_4: 0.0004764  loss_mask_dn_4: 0.359  loss_dice_dn_4: 1.329  loss_bbox_dn_4: 0.2755  loss_giou_dn_4: 0.4526  loss_ce_5: 0.4047  loss_mask_5: 0.3358  loss_dice_5: 1.218  loss_bbox_5: 0.3457  loss_giou_5: 0.5257  loss_ce_dn_5: 0.0006191  loss_mask_dn_5: 0.3588  loss_dice_dn_5: 1.342  loss_bbox_dn_5: 0.2535  loss_giou_dn_5: 0.4548  loss_ce_6: 0.4905  loss_mask_6: 0.3538  loss_dice_6: 1.286  loss_bbox_6: 0.3422  loss_giou_6: 0.5319  loss_ce_dn_6: 0.0006181  loss_mask_dn_6: 0.3557  loss_dice_dn_6: 1.337  loss_bbox_dn_6: 0.2471  loss_giou_dn_6: 0.4523  loss_ce_7: 0.4811  loss_mask_7: 0.3514  loss_dice_7: 1.266  loss_bbox_7: 0.3392  loss_giou_7: 0.5365  loss_ce_dn_7: 0.0004796  loss_mask_dn_7: 0.3545  loss_dice_dn_7: 1.321  loss_bbox_dn_7: 0.2425  loss_giou_dn_7: 0.4617  loss_ce_8: 0.4  loss_mask_8: 0.3356  loss_dice_8: 1.298  loss_bbox_8: 0.3467  loss_giou_8: 0.5262  loss_ce_dn_8: 0.0004392  loss_mask_dn_8: 0.3546  loss_dice_dn_8: 1.335  loss_bbox_dn_8: 0.2412  loss_giou_dn_8: 0.4545  loss_ce_interm: 0.6973  loss_mask_interm: 0.3405  loss_dice_interm: 1.336  loss_bbox_interm: 0.4662  loss_giou_interm: 0.6614    time: 0.4898  last_time: 0.4837  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:28 d2.utils.events]:  eta: 0:10:16  iter: 2739  total_loss: 67.38  loss_ce: 0.1803  loss_mask: 0.4023  loss_dice: 1.222  loss_bbox: 0.3657  loss_giou: 0.5991  loss_ce_dn: 0.0001963  loss_mask_dn: 0.4132  loss_dice_dn: 1.367  loss_bbox_dn: 0.2389  loss_giou_dn: 0.4433  loss_ce_0: 0.593  loss_mask_0: 0.423  loss_dice_0: 1.251  loss_bbox_0: 0.6445  loss_giou_0: 0.9257  loss_ce_dn_0: 0.0399  loss_mask_dn_0: 0.5732  loss_dice_dn_0: 2.503  loss_bbox_dn_0: 0.7787  loss_giou_dn_0: 0.8452  loss_ce_1: 0.5799  loss_mask_1: 0.3831  loss_dice_1: 1.321  loss_bbox_1: 0.3418  loss_giou_1: 0.5853  loss_ce_dn_1: 0.0004337  loss_mask_dn_1: 0.4154  loss_dice_dn_1: 1.4  loss_bbox_dn_1: 0.2842  loss_giou_dn_1: 0.5055  loss_ce_2: 0.5658  loss_mask_2: 0.409  loss_dice_2: 1.343  loss_bbox_2: 0.3724  loss_giou_2: 0.5777  loss_ce_dn_2: 0.0004877  loss_mask_dn_2: 0.4077  loss_dice_dn_2: 1.393  loss_bbox_dn_2: 0.2531  loss_giou_dn_2: 0.456  loss_ce_3: 0.4176  loss_mask_3: 0.4167  loss_dice_3: 1.459  loss_bbox_3: 0.4025  loss_giou_3: 0.6175  loss_ce_dn_3: 0.0005096  loss_mask_dn_3: 0.4128  loss_dice_dn_3: 1.34  loss_bbox_dn_3: 0.2399  loss_giou_dn_3: 0.4466  loss_ce_4: 0.2521  loss_mask_4: 0.4122  loss_dice_4: 1.349  loss_bbox_4: 0.3783  loss_giou_4: 0.6055  loss_ce_dn_4: 0.0003362  loss_mask_dn_4: 0.4122  loss_dice_dn_4: 1.34  loss_bbox_dn_4: 0.2344  loss_giou_dn_4: 0.447  loss_ce_5: 0.1673  loss_mask_5: 0.4067  loss_dice_5: 1.384  loss_bbox_5: 0.3735  loss_giou_5: 0.6028  loss_ce_dn_5: 0.000615  loss_mask_dn_5: 0.4019  loss_dice_dn_5: 1.338  loss_bbox_dn_5: 0.2281  loss_giou_dn_5: 0.4397  loss_ce_6: 0.1592  loss_mask_6: 0.4052  loss_dice_6: 1.389  loss_bbox_6: 0.3706  loss_giou_6: 0.6158  loss_ce_dn_6: 0.0004314  loss_mask_dn_6: 0.4077  loss_dice_dn_6: 1.368  loss_bbox_dn_6: 0.2337  loss_giou_dn_6: 0.4408  loss_ce_7: 0.1627  loss_mask_7: 0.3938  loss_dice_7: 1.173  loss_bbox_7: 0.3731  loss_giou_7: 0.5842  loss_ce_dn_7: 0.0002714  loss_mask_dn_7: 0.4156  loss_dice_dn_7: 1.364  loss_bbox_dn_7: 0.2422  loss_giou_dn_7: 0.4429  loss_ce_8: 0.171  loss_mask_8: 0.3929  loss_dice_8: 1.279  loss_bbox_8: 0.3601  loss_giou_8: 0.5902  loss_ce_dn_8: 0.000201  loss_mask_dn_8: 0.4108  loss_dice_dn_8: 1.371  loss_bbox_dn_8: 0.2385  loss_giou_dn_8: 0.4382  loss_ce_interm: 0.5853  loss_mask_interm: 0.416  loss_dice_interm: 1.319  loss_bbox_interm: 0.3714  loss_giou_interm: 0.7094    time: 0.4898  last_time: 0.5237  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:38 d2.utils.events]:  eta: 0:10:06  iter: 2759  total_loss: 55.54  loss_ce: 0.1747  loss_mask: 0.3396  loss_dice: 1.303  loss_bbox: 0.2813  loss_giou: 0.4514  loss_ce_dn: 0.0001126  loss_mask_dn: 0.3237  loss_dice_dn: 1.216  loss_bbox_dn: 0.199  loss_giou_dn: 0.3937  loss_ce_0: 0.5346  loss_mask_0: 0.3281  loss_dice_0: 1.273  loss_bbox_0: 0.6854  loss_giou_0: 0.9404  loss_ce_dn_0: 0.03985  loss_mask_dn_0: 0.5308  loss_dice_dn_0: 2.556  loss_bbox_dn_0: 0.7502  loss_giou_dn_0: 0.8511  loss_ce_1: 0.6134  loss_mask_1: 0.323  loss_dice_1: 1.255  loss_bbox_1: 0.282  loss_giou_1: 0.5057  loss_ce_dn_1: 0.0004751  loss_mask_dn_1: 0.3237  loss_dice_dn_1: 1.271  loss_bbox_dn_1: 0.2754  loss_giou_dn_1: 0.4556  loss_ce_2: 0.3717  loss_mask_2: 0.323  loss_dice_2: 1.291  loss_bbox_2: 0.2496  loss_giou_2: 0.4659  loss_ce_dn_2: 0.0004148  loss_mask_dn_2: 0.3345  loss_dice_dn_2: 1.238  loss_bbox_dn_2: 0.2354  loss_giou_dn_2: 0.4362  loss_ce_3: 0.2672  loss_mask_3: 0.3405  loss_dice_3: 1.317  loss_bbox_3: 0.2852  loss_giou_3: 0.5147  loss_ce_dn_3: 0.0003597  loss_mask_dn_3: 0.3267  loss_dice_dn_3: 1.237  loss_bbox_dn_3: 0.216  loss_giou_dn_3: 0.4241  loss_ce_4: 0.194  loss_mask_4: 0.3283  loss_dice_4: 1.305  loss_bbox_4: 0.2858  loss_giou_4: 0.4867  loss_ce_dn_4: 0.0002174  loss_mask_dn_4: 0.3186  loss_dice_dn_4: 1.23  loss_bbox_dn_4: 0.1966  loss_giou_dn_4: 0.4048  loss_ce_5: 0.1811  loss_mask_5: 0.3275  loss_dice_5: 1.314  loss_bbox_5: 0.2914  loss_giou_5: 0.4791  loss_ce_dn_5: 0.0002931  loss_mask_dn_5: 0.318  loss_dice_dn_5: 1.223  loss_bbox_dn_5: 0.1967  loss_giou_dn_5: 0.3984  loss_ce_6: 0.1795  loss_mask_6: 0.3212  loss_dice_6: 1.303  loss_bbox_6: 0.2769  loss_giou_6: 0.4505  loss_ce_dn_6: 0.0003513  loss_mask_dn_6: 0.3235  loss_dice_dn_6: 1.21  loss_bbox_dn_6: 0.1944  loss_giou_dn_6: 0.3946  loss_ce_7: 0.1802  loss_mask_7: 0.3081  loss_dice_7: 1.219  loss_bbox_7: 0.2794  loss_giou_7: 0.4615  loss_ce_dn_7: 0.0001684  loss_mask_dn_7: 0.3268  loss_dice_dn_7: 1.208  loss_bbox_dn_7: 0.1947  loss_giou_dn_7: 0.3959  loss_ce_8: 0.178  loss_mask_8: 0.3316  loss_dice_8: 1.285  loss_bbox_8: 0.2787  loss_giou_8: 0.4511  loss_ce_dn_8: 0.0001354  loss_mask_dn_8: 0.3191  loss_dice_dn_8: 1.216  loss_bbox_dn_8: 0.197  loss_giou_dn_8: 0.3936  loss_ce_interm: 0.5346  loss_mask_interm: 0.3164  loss_dice_interm: 1.294  loss_bbox_interm: 0.3174  loss_giou_interm: 0.5797    time: 0.4897  last_time: 0.4511  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:48 d2.utils.events]:  eta: 0:09:57  iter: 2779  total_loss: 66.02  loss_ce: 0.455  loss_mask: 0.2836  loss_dice: 1.385  loss_bbox: 0.3128  loss_giou: 0.5686  loss_ce_dn: 0.0001891  loss_mask_dn: 0.2815  loss_dice_dn: 1.257  loss_bbox_dn: 0.2612  loss_giou_dn: 0.5016  loss_ce_0: 0.67  loss_mask_0: 0.2922  loss_dice_0: 1.309  loss_bbox_0: 0.7419  loss_giou_0: 1.026  loss_ce_dn_0: 0.05195  loss_mask_dn_0: 0.5193  loss_dice_dn_0: 2.555  loss_bbox_dn_0: 0.7589  loss_giou_dn_0: 0.8538  loss_ce_1: 0.8609  loss_mask_1: 0.2852  loss_dice_1: 1.36  loss_bbox_1: 0.378  loss_giou_1: 0.6154  loss_ce_dn_1: 0.000489  loss_mask_dn_1: 0.3107  loss_dice_dn_1: 1.282  loss_bbox_dn_1: 0.2972  loss_giou_dn_1: 0.5119  loss_ce_2: 0.7119  loss_mask_2: 0.2887  loss_dice_2: 1.335  loss_bbox_2: 0.3106  loss_giou_2: 0.591  loss_ce_dn_2: 0.0003412  loss_mask_dn_2: 0.2969  loss_dice_dn_2: 1.273  loss_bbox_dn_2: 0.2624  loss_giou_dn_2: 0.5008  loss_ce_3: 0.6434  loss_mask_3: 0.2977  loss_dice_3: 1.38  loss_bbox_3: 0.319  loss_giou_3: 0.6018  loss_ce_dn_3: 0.0004066  loss_mask_dn_3: 0.2916  loss_dice_dn_3: 1.29  loss_bbox_dn_3: 0.2454  loss_giou_dn_3: 0.4948  loss_ce_4: 0.5261  loss_mask_4: 0.3204  loss_dice_4: 1.3  loss_bbox_4: 0.2994  loss_giou_4: 0.6194  loss_ce_dn_4: 0.0003036  loss_mask_dn_4: 0.2842  loss_dice_dn_4: 1.297  loss_bbox_dn_4: 0.2671  loss_giou_dn_4: 0.4957  loss_ce_5: 0.5654  loss_mask_5: 0.2892  loss_dice_5: 1.361  loss_bbox_5: 0.3181  loss_giou_5: 0.6373  loss_ce_dn_5: 0.0004407  loss_mask_dn_5: 0.2843  loss_dice_dn_5: 1.296  loss_bbox_dn_5: 0.2518  loss_giou_dn_5: 0.4876  loss_ce_6: 0.5187  loss_mask_6: 0.3009  loss_dice_6: 1.378  loss_bbox_6: 0.3152  loss_giou_6: 0.6092  loss_ce_dn_6: 0.0003867  loss_mask_dn_6: 0.2844  loss_dice_dn_6: 1.288  loss_bbox_dn_6: 0.2521  loss_giou_dn_6: 0.4979  loss_ce_7: 0.4305  loss_mask_7: 0.299  loss_dice_7: 1.361  loss_bbox_7: 0.317  loss_giou_7: 0.6423  loss_ce_dn_7: 0.0002389  loss_mask_dn_7: 0.2862  loss_dice_dn_7: 1.263  loss_bbox_dn_7: 0.2602  loss_giou_dn_7: 0.4976  loss_ce_8: 0.465  loss_mask_8: 0.2914  loss_dice_8: 1.368  loss_bbox_8: 0.311  loss_giou_8: 0.609  loss_ce_dn_8: 0.0002243  loss_mask_dn_8: 0.2824  loss_dice_dn_8: 1.253  loss_bbox_dn_8: 0.2616  loss_giou_dn_8: 0.5024  loss_ce_interm: 0.698  loss_mask_interm: 0.2856  loss_dice_interm: 1.29  loss_bbox_interm: 0.3406  loss_giou_interm: 0.6863    time: 0.4899  last_time: 0.5428  data_time: 0.0040  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:27:58 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:27:58 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:27:58 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:27:58 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:28:00 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0784 s/iter. Eval: 0.0107 s/iter. Total: 0.0900 s/iter. ETA=0:00:05\n",
      "[03/14 17:28:05 d2.evaluation.evaluator]: Total inference time: 0:00:05.536660 (0.089301 s / iter per device, on 1 devices)\n",
      "[03/14 17:28:05 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076972 s / iter per device, on 1 devices)\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.823\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.543\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.469\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.590\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.361\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.560\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.573\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.719\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 47.554 | 82.291 | 54.283 | 46.891 | 58.968 |  nan  |\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:28:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.702\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.344\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 25.419 | 70.244 | 4.807  | 23.072 | 34.949 |  nan  |\n",
      "[03/14 17:28:05 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:28:05 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: 47.5544,82.2905,54.2828,46.8907,58.9682,nan\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:28:05 d2.evaluation.testing]: copypaste: 25.4189,70.2438,4.8069,23.0724,34.9494,nan\n",
      "[03/14 17:28:05 d2.utils.events]:  eta: 0:09:48  iter: 2799  total_loss: 71.11  loss_ce: 0.1527  loss_mask: 0.4596  loss_dice: 1.314  loss_bbox: 0.3651  loss_giou: 0.5764  loss_ce_dn: 0.0002666  loss_mask_dn: 0.4243  loss_dice_dn: 1.186  loss_bbox_dn: 0.304  loss_giou_dn: 0.4447  loss_ce_0: 0.6125  loss_mask_0: 0.511  loss_dice_0: 1.38  loss_bbox_0: 0.7768  loss_giou_0: 0.8374  loss_ce_dn_0: 0.03976  loss_mask_dn_0: 0.5791  loss_dice_dn_0: 2.454  loss_bbox_dn_0: 0.9444  loss_giou_dn_0: 0.8536  loss_ce_1: 0.7555  loss_mask_1: 0.5037  loss_dice_1: 1.387  loss_bbox_1: 0.4405  loss_giou_1: 0.641  loss_ce_dn_1: 0.000596  loss_mask_dn_1: 0.4538  loss_dice_dn_1: 1.28  loss_bbox_dn_1: 0.3782  loss_giou_dn_1: 0.5169  loss_ce_2: 0.5242  loss_mask_2: 0.4677  loss_dice_2: 1.319  loss_bbox_2: 0.3936  loss_giou_2: 0.6951  loss_ce_dn_2: 0.0005509  loss_mask_dn_2: 0.4442  loss_dice_dn_2: 1.225  loss_bbox_dn_2: 0.3418  loss_giou_dn_2: 0.471  loss_ce_3: 0.5508  loss_mask_3: 0.4625  loss_dice_3: 1.358  loss_bbox_3: 0.3926  loss_giou_3: 0.6285  loss_ce_dn_3: 0.0006093  loss_mask_dn_3: 0.4341  loss_dice_dn_3: 1.192  loss_bbox_dn_3: 0.3197  loss_giou_dn_3: 0.4687  loss_ce_4: 0.379  loss_mask_4: 0.4507  loss_dice_4: 1.386  loss_bbox_4: 0.3796  loss_giou_4: 0.6225  loss_ce_dn_4: 0.0004822  loss_mask_dn_4: 0.4213  loss_dice_dn_4: 1.178  loss_bbox_dn_4: 0.3051  loss_giou_dn_4: 0.4501  loss_ce_5: 0.3468  loss_mask_5: 0.4666  loss_dice_5: 1.363  loss_bbox_5: 0.3784  loss_giou_5: 0.6261  loss_ce_dn_5: 0.0005247  loss_mask_dn_5: 0.4228  loss_dice_dn_5: 1.194  loss_bbox_dn_5: 0.3049  loss_giou_dn_5: 0.4416  loss_ce_6: 0.1699  loss_mask_6: 0.4792  loss_dice_6: 1.392  loss_bbox_6: 0.407  loss_giou_6: 0.606  loss_ce_dn_6: 0.0005548  loss_mask_dn_6: 0.4314  loss_dice_dn_6: 1.177  loss_bbox_dn_6: 0.3018  loss_giou_dn_6: 0.453  loss_ce_7: 0.1313  loss_mask_7: 0.4847  loss_dice_7: 1.372  loss_bbox_7: 0.3725  loss_giou_7: 0.6082  loss_ce_dn_7: 0.0003312  loss_mask_dn_7: 0.4316  loss_dice_dn_7: 1.19  loss_bbox_dn_7: 0.301  loss_giou_dn_7: 0.4405  loss_ce_8: 0.1359  loss_mask_8: 0.476  loss_dice_8: 1.407  loss_bbox_8: 0.3857  loss_giou_8: 0.5809  loss_ce_dn_8: 0.0002676  loss_mask_dn_8: 0.4236  loss_dice_dn_8: 1.189  loss_bbox_dn_8: 0.3065  loss_giou_dn_8: 0.4494  loss_ce_interm: 0.7059  loss_mask_interm: 0.5117  loss_dice_interm: 1.255  loss_bbox_interm: 0.4295  loss_giou_interm: 0.6677    time: 0.4899  last_time: 0.4427  data_time: 0.0035  last_data_time: 0.0029   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:28:15 d2.utils.events]:  eta: 0:09:38  iter: 2819  total_loss: 57.98  loss_ce: 0.2494  loss_mask: 0.3697  loss_dice: 1.185  loss_bbox: 0.2728  loss_giou: 0.4664  loss_ce_dn: 0.0002131  loss_mask_dn: 0.3526  loss_dice_dn: 1.126  loss_bbox_dn: 0.2075  loss_giou_dn: 0.3664  loss_ce_0: 0.6879  loss_mask_0: 0.4121  loss_dice_0: 1.245  loss_bbox_0: 0.7423  loss_giou_0: 0.8724  loss_ce_dn_0: 0.03158  loss_mask_dn_0: 0.4925  loss_dice_dn_0: 2.744  loss_bbox_dn_0: 0.7206  loss_giou_dn_0: 0.843  loss_ce_1: 0.6385  loss_mask_1: 0.4137  loss_dice_1: 1.238  loss_bbox_1: 0.3496  loss_giou_1: 0.5195  loss_ce_dn_1: 0.0005727  loss_mask_dn_1: 0.3487  loss_dice_dn_1: 1.193  loss_bbox_dn_1: 0.345  loss_giou_dn_1: 0.4562  loss_ce_2: 0.5093  loss_mask_2: 0.4003  loss_dice_2: 1.203  loss_bbox_2: 0.304  loss_giou_2: 0.5043  loss_ce_dn_2: 0.0003801  loss_mask_dn_2: 0.3587  loss_dice_dn_2: 1.191  loss_bbox_dn_2: 0.229  loss_giou_dn_2: 0.4017  loss_ce_3: 0.3861  loss_mask_3: 0.3972  loss_dice_3: 1.216  loss_bbox_3: 0.2747  loss_giou_3: 0.4981  loss_ce_dn_3: 0.0005043  loss_mask_dn_3: 0.3533  loss_dice_dn_3: 1.175  loss_bbox_dn_3: 0.2123  loss_giou_dn_3: 0.3992  loss_ce_4: 0.3318  loss_mask_4: 0.3996  loss_dice_4: 1.212  loss_bbox_4: 0.2774  loss_giou_4: 0.4938  loss_ce_dn_4: 0.0003592  loss_mask_dn_4: 0.3552  loss_dice_dn_4: 1.169  loss_bbox_dn_4: 0.2156  loss_giou_dn_4: 0.3706  loss_ce_5: 0.2383  loss_mask_5: 0.3726  loss_dice_5: 1.223  loss_bbox_5: 0.2853  loss_giou_5: 0.485  loss_ce_dn_5: 0.0004344  loss_mask_dn_5: 0.3624  loss_dice_dn_5: 1.145  loss_bbox_dn_5: 0.2076  loss_giou_dn_5: 0.3785  loss_ce_6: 0.2331  loss_mask_6: 0.356  loss_dice_6: 1.144  loss_bbox_6: 0.2792  loss_giou_6: 0.474  loss_ce_dn_6: 0.0004854  loss_mask_dn_6: 0.3606  loss_dice_dn_6: 1.15  loss_bbox_dn_6: 0.2073  loss_giou_dn_6: 0.3658  loss_ce_7: 0.2391  loss_mask_7: 0.3586  loss_dice_7: 1.19  loss_bbox_7: 0.2684  loss_giou_7: 0.46  loss_ce_dn_7: 0.0002773  loss_mask_dn_7: 0.3605  loss_dice_dn_7: 1.141  loss_bbox_dn_7: 0.2002  loss_giou_dn_7: 0.362  loss_ce_8: 0.2913  loss_mask_8: 0.3776  loss_dice_8: 1.151  loss_bbox_8: 0.2702  loss_giou_8: 0.4612  loss_ce_dn_8: 0.0002404  loss_mask_dn_8: 0.3535  loss_dice_dn_8: 1.139  loss_bbox_dn_8: 0.2059  loss_giou_dn_8: 0.3645  loss_ce_interm: 0.7491  loss_mask_interm: 0.397  loss_dice_interm: 1.204  loss_bbox_interm: 0.4501  loss_giou_interm: 0.5913    time: 0.4899  last_time: 0.5009  data_time: 0.0035  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:28:25 d2.utils.events]:  eta: 0:09:28  iter: 2839  total_loss: 65.72  loss_ce: 0.3106  loss_mask: 0.3416  loss_dice: 1.247  loss_bbox: 0.2866  loss_giou: 0.4418  loss_ce_dn: 0.0001473  loss_mask_dn: 0.3589  loss_dice_dn: 1.278  loss_bbox_dn: 0.2393  loss_giou_dn: 0.3801  loss_ce_0: 0.7965  loss_mask_0: 0.3599  loss_dice_0: 1.411  loss_bbox_0: 0.7124  loss_giou_0: 0.9773  loss_ce_dn_0: 0.01933  loss_mask_dn_0: 0.7255  loss_dice_dn_0: 2.238  loss_bbox_dn_0: 0.7881  loss_giou_dn_0: 0.8561  loss_ce_1: 0.6874  loss_mask_1: 0.4037  loss_dice_1: 1.341  loss_bbox_1: 0.3411  loss_giou_1: 0.5695  loss_ce_dn_1: 0.0006321  loss_mask_dn_1: 0.3547  loss_dice_dn_1: 1.263  loss_bbox_dn_1: 0.2827  loss_giou_dn_1: 0.44  loss_ce_2: 0.5654  loss_mask_2: 0.3528  loss_dice_2: 1.294  loss_bbox_2: 0.296  loss_giou_2: 0.5083  loss_ce_dn_2: 0.0005001  loss_mask_dn_2: 0.3559  loss_dice_dn_2: 1.266  loss_bbox_dn_2: 0.2522  loss_giou_dn_2: 0.3936  loss_ce_3: 0.491  loss_mask_3: 0.3484  loss_dice_3: 1.295  loss_bbox_3: 0.2943  loss_giou_3: 0.5482  loss_ce_dn_3: 0.0004499  loss_mask_dn_3: 0.3502  loss_dice_dn_3: 1.287  loss_bbox_dn_3: 0.2579  loss_giou_dn_3: 0.3947  loss_ce_4: 0.451  loss_mask_4: 0.3527  loss_dice_4: 1.231  loss_bbox_4: 0.2902  loss_giou_4: 0.4468  loss_ce_dn_4: 0.0003208  loss_mask_dn_4: 0.3561  loss_dice_dn_4: 1.272  loss_bbox_dn_4: 0.2453  loss_giou_dn_4: 0.373  loss_ce_5: 0.4035  loss_mask_5: 0.3351  loss_dice_5: 1.236  loss_bbox_5: 0.3025  loss_giou_5: 0.4681  loss_ce_dn_5: 0.0003937  loss_mask_dn_5: 0.3529  loss_dice_dn_5: 1.258  loss_bbox_dn_5: 0.2315  loss_giou_dn_5: 0.3806  loss_ce_6: 0.3822  loss_mask_6: 0.3505  loss_dice_6: 1.217  loss_bbox_6: 0.2912  loss_giou_6: 0.4529  loss_ce_dn_6: 0.0003332  loss_mask_dn_6: 0.3557  loss_dice_dn_6: 1.271  loss_bbox_dn_6: 0.2393  loss_giou_dn_6: 0.3754  loss_ce_7: 0.3623  loss_mask_7: 0.3636  loss_dice_7: 1.291  loss_bbox_7: 0.2893  loss_giou_7: 0.4461  loss_ce_dn_7: 0.0001989  loss_mask_dn_7: 0.3608  loss_dice_dn_7: 1.268  loss_bbox_dn_7: 0.2401  loss_giou_dn_7: 0.3797  loss_ce_8: 0.3009  loss_mask_8: 0.3542  loss_dice_8: 1.245  loss_bbox_8: 0.285  loss_giou_8: 0.4423  loss_ce_dn_8: 0.0001448  loss_mask_dn_8: 0.3558  loss_dice_dn_8: 1.273  loss_bbox_dn_8: 0.2404  loss_giou_dn_8: 0.383  loss_ce_interm: 0.8479  loss_mask_interm: 0.372  loss_dice_interm: 1.269  loss_bbox_interm: 0.3381  loss_giou_interm: 0.529    time: 0.4899  last_time: 0.4570  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:28:35 d2.utils.events]:  eta: 0:09:19  iter: 2859  total_loss: 63.01  loss_ce: 0.2199  loss_mask: 0.5462  loss_dice: 1.241  loss_bbox: 0.337  loss_giou: 0.5765  loss_ce_dn: 0.0001666  loss_mask_dn: 0.5199  loss_dice_dn: 1.209  loss_bbox_dn: 0.2813  loss_giou_dn: 0.4496  loss_ce_0: 0.6498  loss_mask_0: 0.574  loss_dice_0: 1.329  loss_bbox_0: 0.7935  loss_giou_0: 0.8332  loss_ce_dn_0: 0.01929  loss_mask_dn_0: 0.7906  loss_dice_dn_0: 2.394  loss_bbox_dn_0: 0.8945  loss_giou_dn_0: 0.8506  loss_ce_1: 0.6542  loss_mask_1: 0.5428  loss_dice_1: 1.284  loss_bbox_1: 0.458  loss_giou_1: 0.613  loss_ce_dn_1: 0.0006476  loss_mask_dn_1: 0.5079  loss_dice_dn_1: 1.24  loss_bbox_dn_1: 0.4143  loss_giou_dn_1: 0.5002  loss_ce_2: 0.3801  loss_mask_2: 0.5596  loss_dice_2: 1.272  loss_bbox_2: 0.3961  loss_giou_2: 0.5792  loss_ce_dn_2: 0.0006347  loss_mask_dn_2: 0.5127  loss_dice_dn_2: 1.224  loss_bbox_dn_2: 0.3371  loss_giou_dn_2: 0.4561  loss_ce_3: 0.263  loss_mask_3: 0.5069  loss_dice_3: 1.23  loss_bbox_3: 0.3728  loss_giou_3: 0.5873  loss_ce_dn_3: 0.0005363  loss_mask_dn_3: 0.4999  loss_dice_dn_3: 1.21  loss_bbox_dn_3: 0.3177  loss_giou_dn_3: 0.4599  loss_ce_4: 0.2884  loss_mask_4: 0.5219  loss_dice_4: 1.241  loss_bbox_4: 0.3683  loss_giou_4: 0.5495  loss_ce_dn_4: 0.0003135  loss_mask_dn_4: 0.5058  loss_dice_dn_4: 1.206  loss_bbox_dn_4: 0.3022  loss_giou_dn_4: 0.4389  loss_ce_5: 0.2811  loss_mask_5: 0.5242  loss_dice_5: 1.231  loss_bbox_5: 0.3456  loss_giou_5: 0.5599  loss_ce_dn_5: 0.0004555  loss_mask_dn_5: 0.4983  loss_dice_dn_5: 1.198  loss_bbox_dn_5: 0.2961  loss_giou_dn_5: 0.4395  loss_ce_6: 0.256  loss_mask_6: 0.5169  loss_dice_6: 1.238  loss_bbox_6: 0.3455  loss_giou_6: 0.5747  loss_ce_dn_6: 0.0004178  loss_mask_dn_6: 0.5038  loss_dice_dn_6: 1.198  loss_bbox_dn_6: 0.2755  loss_giou_dn_6: 0.4469  loss_ce_7: 0.2262  loss_mask_7: 0.5412  loss_dice_7: 1.239  loss_bbox_7: 0.3365  loss_giou_7: 0.5652  loss_ce_dn_7: 0.0002685  loss_mask_dn_7: 0.5117  loss_dice_dn_7: 1.193  loss_bbox_dn_7: 0.2775  loss_giou_dn_7: 0.4462  loss_ce_8: 0.2108  loss_mask_8: 0.5282  loss_dice_8: 1.246  loss_bbox_8: 0.353  loss_giou_8: 0.5717  loss_ce_dn_8: 0.0001891  loss_mask_dn_8: 0.5164  loss_dice_dn_8: 1.205  loss_bbox_dn_8: 0.2747  loss_giou_dn_8: 0.447  loss_ce_interm: 0.7397  loss_mask_interm: 0.5774  loss_dice_interm: 1.272  loss_bbox_interm: 0.466  loss_giou_interm: 0.6668    time: 0.4899  last_time: 0.4970  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:28:45 d2.utils.events]:  eta: 0:09:09  iter: 2879  total_loss: 59.6  loss_ce: 0.3213  loss_mask: 0.2742  loss_dice: 1.135  loss_bbox: 0.2977  loss_giou: 0.5211  loss_ce_dn: 0.0003832  loss_mask_dn: 0.278  loss_dice_dn: 1.131  loss_bbox_dn: 0.2347  loss_giou_dn: 0.4083  loss_ce_0: 0.6204  loss_mask_0: 0.3048  loss_dice_0: 1.347  loss_bbox_0: 0.5885  loss_giou_0: 0.8042  loss_ce_dn_0: 0.05986  loss_mask_dn_0: 0.7021  loss_dice_dn_0: 3.053  loss_bbox_dn_0: 0.7325  loss_giou_dn_0: 0.8456  loss_ce_1: 0.6145  loss_mask_1: 0.2747  loss_dice_1: 1.211  loss_bbox_1: 0.3364  loss_giou_1: 0.5503  loss_ce_dn_1: 0.0008061  loss_mask_dn_1: 0.2742  loss_dice_dn_1: 1.212  loss_bbox_dn_1: 0.3224  loss_giou_dn_1: 0.4655  loss_ce_2: 0.5659  loss_mask_2: 0.2768  loss_dice_2: 1.183  loss_bbox_2: 0.3164  loss_giou_2: 0.5467  loss_ce_dn_2: 0.0005078  loss_mask_dn_2: 0.2791  loss_dice_dn_2: 1.156  loss_bbox_dn_2: 0.2748  loss_giou_dn_2: 0.4399  loss_ce_3: 0.4024  loss_mask_3: 0.2704  loss_dice_3: 1.154  loss_bbox_3: 0.2841  loss_giou_3: 0.5102  loss_ce_dn_3: 0.0006478  loss_mask_dn_3: 0.2808  loss_dice_dn_3: 1.169  loss_bbox_dn_3: 0.2356  loss_giou_dn_3: 0.4218  loss_ce_4: 0.3949  loss_mask_4: 0.2675  loss_dice_4: 1.192  loss_bbox_4: 0.2714  loss_giou_4: 0.509  loss_ce_dn_4: 0.0004157  loss_mask_dn_4: 0.2772  loss_dice_dn_4: 1.17  loss_bbox_dn_4: 0.2377  loss_giou_dn_4: 0.4282  loss_ce_5: 0.4389  loss_mask_5: 0.2754  loss_dice_5: 1.149  loss_bbox_5: 0.2656  loss_giou_5: 0.4898  loss_ce_dn_5: 0.0005928  loss_mask_dn_5: 0.276  loss_dice_dn_5: 1.129  loss_bbox_dn_5: 0.2328  loss_giou_dn_5: 0.4166  loss_ce_6: 0.3642  loss_mask_6: 0.2794  loss_dice_6: 1.144  loss_bbox_6: 0.266  loss_giou_6: 0.4922  loss_ce_dn_6: 0.000459  loss_mask_dn_6: 0.2826  loss_dice_dn_6: 1.131  loss_bbox_dn_6: 0.232  loss_giou_dn_6: 0.4119  loss_ce_7: 0.3153  loss_mask_7: 0.2877  loss_dice_7: 1.232  loss_bbox_7: 0.295  loss_giou_7: 0.5409  loss_ce_dn_7: 0.000315  loss_mask_dn_7: 0.2805  loss_dice_dn_7: 1.136  loss_bbox_dn_7: 0.2304  loss_giou_dn_7: 0.4111  loss_ce_8: 0.3145  loss_mask_8: 0.2788  loss_dice_8: 1.16  loss_bbox_8: 0.2953  loss_giou_8: 0.5257  loss_ce_dn_8: 0.0003419  loss_mask_dn_8: 0.279  loss_dice_dn_8: 1.137  loss_bbox_dn_8: 0.2322  loss_giou_dn_8: 0.408  loss_ce_interm: 0.8105  loss_mask_interm: 0.3142  loss_dice_interm: 1.338  loss_bbox_interm: 0.4545  loss_giou_interm: 0.6231    time: 0.4900  last_time: 0.4995  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:28:55 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:28:55 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:28:55 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:28:55 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:28:57 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0826 s/iter. Eval: 0.0103 s/iter. Total: 0.0938 s/iter. ETA=0:00:05\n",
      "[03/14 17:29:02 d2.evaluation.evaluator]: Inference done 67/67. Dataloading: 0.0011 s/iter. Inference: 0.0797 s/iter. Eval: 0.0098 s/iter. Total: 0.0906 s/iter. ETA=0:00:00\n",
      "[03/14 17:29:02 d2.evaluation.evaluator]: Total inference time: 0:00:05.690960 (0.091790 s / iter per device, on 1 devices)\n",
      "[03/14 17:29:02 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.079679 s / iter per device, on 1 devices)\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.816\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.523\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.470\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.715 | 81.588 | 52.306 | 46.998 | 60.876 |  nan  |\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:29:02 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.727\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.240\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.365\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.337\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.342\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.611 | 72.733 | 5.646  | 23.985 | 36.466 |  nan  |\n",
      "[03/14 17:29:02 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:29:02 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: 48.7147,81.5884,52.3060,46.9981,60.8759,nan\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:29:02 d2.evaluation.testing]: copypaste: 26.6107,72.7335,5.6457,23.9850,36.4657,nan\n",
      "[03/14 17:29:02 d2.utils.events]:  eta: 0:09:00  iter: 2899  total_loss: 64.01  loss_ce: 0.2014  loss_mask: 0.3666  loss_dice: 1.118  loss_bbox: 0.2709  loss_giou: 0.4524  loss_ce_dn: 0.0001352  loss_mask_dn: 0.3834  loss_dice_dn: 1.076  loss_bbox_dn: 0.214  loss_giou_dn: 0.3912  loss_ce_0: 0.7323  loss_mask_0: 0.375  loss_dice_0: 1.254  loss_bbox_0: 0.7096  loss_giou_0: 0.9004  loss_ce_dn_0: 0.01917  loss_mask_dn_0: 0.5587  loss_dice_dn_0: 2.47  loss_bbox_dn_0: 0.8426  loss_giou_dn_0: 0.8531  loss_ce_1: 0.643  loss_mask_1: 0.3879  loss_dice_1: 1.183  loss_bbox_1: 0.3351  loss_giou_1: 0.5509  loss_ce_dn_1: 0.0005955  loss_mask_dn_1: 0.3966  loss_dice_dn_1: 1.106  loss_bbox_dn_1: 0.3406  loss_giou_dn_1: 0.4824  loss_ce_2: 0.4023  loss_mask_2: 0.3924  loss_dice_2: 1.141  loss_bbox_2: 0.2976  loss_giou_2: 0.5106  loss_ce_dn_2: 0.000463  loss_mask_dn_2: 0.3946  loss_dice_dn_2: 1.074  loss_bbox_dn_2: 0.2346  loss_giou_dn_2: 0.4353  loss_ce_3: 0.3216  loss_mask_3: 0.3895  loss_dice_3: 1.152  loss_bbox_3: 0.3011  loss_giou_3: 0.5045  loss_ce_dn_3: 0.0004735  loss_mask_dn_3: 0.3853  loss_dice_dn_3: 1.081  loss_bbox_dn_3: 0.2075  loss_giou_dn_3: 0.3991  loss_ce_4: 0.2557  loss_mask_4: 0.3748  loss_dice_4: 1.174  loss_bbox_4: 0.2831  loss_giou_4: 0.5026  loss_ce_dn_4: 0.0003058  loss_mask_dn_4: 0.3975  loss_dice_dn_4: 1.071  loss_bbox_dn_4: 0.2169  loss_giou_dn_4: 0.3929  loss_ce_5: 0.1965  loss_mask_5: 0.3813  loss_dice_5: 1.196  loss_bbox_5: 0.2893  loss_giou_5: 0.481  loss_ce_dn_5: 0.0003469  loss_mask_dn_5: 0.3846  loss_dice_dn_5: 1.074  loss_bbox_dn_5: 0.2182  loss_giou_dn_5: 0.4017  loss_ce_6: 0.1788  loss_mask_6: 0.3855  loss_dice_6: 1.125  loss_bbox_6: 0.2725  loss_giou_6: 0.4937  loss_ce_dn_6: 0.0003014  loss_mask_dn_6: 0.3808  loss_dice_dn_6: 1.068  loss_bbox_dn_6: 0.2114  loss_giou_dn_6: 0.3902  loss_ce_7: 0.1668  loss_mask_7: 0.3799  loss_dice_7: 1.117  loss_bbox_7: 0.2787  loss_giou_7: 0.4564  loss_ce_dn_7: 0.0001909  loss_mask_dn_7: 0.3875  loss_dice_dn_7: 1.076  loss_bbox_dn_7: 0.2141  loss_giou_dn_7: 0.3939  loss_ce_8: 0.1847  loss_mask_8: 0.366  loss_dice_8: 1.124  loss_bbox_8: 0.283  loss_giou_8: 0.4862  loss_ce_dn_8: 0.0001276  loss_mask_dn_8: 0.3836  loss_dice_dn_8: 1.078  loss_bbox_dn_8: 0.2121  loss_giou_dn_8: 0.3889  loss_ce_interm: 0.7652  loss_mask_interm: 0.4204  loss_dice_interm: 1.176  loss_bbox_interm: 0.3809  loss_giou_interm: 0.5881    time: 0.4901  last_time: 0.4963  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:29:12 d2.utils.events]:  eta: 0:08:50  iter: 2919  total_loss: 56.27  loss_ce: 0.2589  loss_mask: 0.3115  loss_dice: 1.266  loss_bbox: 0.3192  loss_giou: 0.4313  loss_ce_dn: 0.0001778  loss_mask_dn: 0.3292  loss_dice_dn: 1.274  loss_bbox_dn: 0.2779  loss_giou_dn: 0.4074  loss_ce_0: 0.6221  loss_mask_0: 0.3683  loss_dice_0: 1.331  loss_bbox_0: 0.8006  loss_giou_0: 0.9263  loss_ce_dn_0: 0.0394  loss_mask_dn_0: 0.4936  loss_dice_dn_0: 2.254  loss_bbox_dn_0: 0.7776  loss_giou_dn_0: 0.8538  loss_ce_1: 0.6334  loss_mask_1: 0.3413  loss_dice_1: 1.272  loss_bbox_1: 0.2989  loss_giou_1: 0.5331  loss_ce_dn_1: 0.0006219  loss_mask_dn_1: 0.3347  loss_dice_dn_1: 1.269  loss_bbox_dn_1: 0.3018  loss_giou_dn_1: 0.4748  loss_ce_2: 0.4357  loss_mask_2: 0.3223  loss_dice_2: 1.28  loss_bbox_2: 0.2674  loss_giou_2: 0.5119  loss_ce_dn_2: 0.0003855  loss_mask_dn_2: 0.3267  loss_dice_dn_2: 1.264  loss_bbox_dn_2: 0.2768  loss_giou_dn_2: 0.4116  loss_ce_3: 0.2975  loss_mask_3: 0.3192  loss_dice_3: 1.277  loss_bbox_3: 0.3141  loss_giou_3: 0.4831  loss_ce_dn_3: 0.0003376  loss_mask_dn_3: 0.3191  loss_dice_dn_3: 1.255  loss_bbox_dn_3: 0.2765  loss_giou_dn_3: 0.4145  loss_ce_4: 0.2641  loss_mask_4: 0.3237  loss_dice_4: 1.269  loss_bbox_4: 0.2964  loss_giou_4: 0.4586  loss_ce_dn_4: 0.0002324  loss_mask_dn_4: 0.3291  loss_dice_dn_4: 1.275  loss_bbox_dn_4: 0.27  loss_giou_dn_4: 0.4099  loss_ce_5: 0.2756  loss_mask_5: 0.3111  loss_dice_5: 1.268  loss_bbox_5: 0.3225  loss_giou_5: 0.459  loss_ce_dn_5: 0.0003694  loss_mask_dn_5: 0.3212  loss_dice_dn_5: 1.274  loss_bbox_dn_5: 0.2786  loss_giou_dn_5: 0.4089  loss_ce_6: 0.2667  loss_mask_6: 0.3066  loss_dice_6: 1.315  loss_bbox_6: 0.3255  loss_giou_6: 0.4394  loss_ce_dn_6: 0.0002904  loss_mask_dn_6: 0.3225  loss_dice_dn_6: 1.265  loss_bbox_dn_6: 0.2743  loss_giou_dn_6: 0.4035  loss_ce_7: 0.262  loss_mask_7: 0.3214  loss_dice_7: 1.183  loss_bbox_7: 0.3222  loss_giou_7: 0.4402  loss_ce_dn_7: 0.0001913  loss_mask_dn_7: 0.3295  loss_dice_dn_7: 1.266  loss_bbox_dn_7: 0.2735  loss_giou_dn_7: 0.4066  loss_ce_8: 0.2545  loss_mask_8: 0.328  loss_dice_8: 1.325  loss_bbox_8: 0.3145  loss_giou_8: 0.4434  loss_ce_dn_8: 0.0001706  loss_mask_dn_8: 0.3287  loss_dice_dn_8: 1.268  loss_bbox_dn_8: 0.2758  loss_giou_dn_8: 0.4072  loss_ce_interm: 0.7487  loss_mask_interm: 0.3688  loss_dice_interm: 1.293  loss_bbox_interm: 0.2994  loss_giou_interm: 0.5981    time: 0.4901  last_time: 0.5138  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:29:22 d2.utils.events]:  eta: 0:08:41  iter: 2939  total_loss: 58.1  loss_ce: 0.2128  loss_mask: 0.3006  loss_dice: 1.078  loss_bbox: 0.3013  loss_giou: 0.5177  loss_ce_dn: 0.0001647  loss_mask_dn: 0.3252  loss_dice_dn: 1.194  loss_bbox_dn: 0.2377  loss_giou_dn: 0.4417  loss_ce_0: 0.62  loss_mask_0: 0.2982  loss_dice_0: 1.195  loss_bbox_0: 0.7076  loss_giou_0: 1.032  loss_ce_dn_0: 0.01908  loss_mask_dn_0: 0.5167  loss_dice_dn_0: 2.164  loss_bbox_dn_0: 0.849  loss_giou_dn_0: 0.8506  loss_ce_1: 0.6002  loss_mask_1: 0.326  loss_dice_1: 1.241  loss_bbox_1: 0.2912  loss_giou_1: 0.5627  loss_ce_dn_1: 0.0006513  loss_mask_dn_1: 0.3264  loss_dice_dn_1: 1.169  loss_bbox_dn_1: 0.3445  loss_giou_dn_1: 0.4783  loss_ce_2: 0.5218  loss_mask_2: 0.2957  loss_dice_2: 1.184  loss_bbox_2: 0.3059  loss_giou_2: 0.5798  loss_ce_dn_2: 0.0003814  loss_mask_dn_2: 0.3211  loss_dice_dn_2: 1.161  loss_bbox_dn_2: 0.285  loss_giou_dn_2: 0.4634  loss_ce_3: 0.4408  loss_mask_3: 0.3209  loss_dice_3: 1.165  loss_bbox_3: 0.291  loss_giou_3: 0.5414  loss_ce_dn_3: 0.0003591  loss_mask_dn_3: 0.3221  loss_dice_dn_3: 1.164  loss_bbox_dn_3: 0.2824  loss_giou_dn_3: 0.456  loss_ce_4: 0.2789  loss_mask_4: 0.2954  loss_dice_4: 1.117  loss_bbox_4: 0.3082  loss_giou_4: 0.5637  loss_ce_dn_4: 0.0002137  loss_mask_dn_4: 0.321  loss_dice_dn_4: 1.192  loss_bbox_dn_4: 0.2441  loss_giou_dn_4: 0.4472  loss_ce_5: 0.1794  loss_mask_5: 0.3074  loss_dice_5: 1.143  loss_bbox_5: 0.3095  loss_giou_5: 0.5252  loss_ce_dn_5: 0.0002951  loss_mask_dn_5: 0.3239  loss_dice_dn_5: 1.195  loss_bbox_dn_5: 0.2446  loss_giou_dn_5: 0.4339  loss_ce_6: 0.2403  loss_mask_6: 0.3227  loss_dice_6: 1.171  loss_bbox_6: 0.2986  loss_giou_6: 0.5204  loss_ce_dn_6: 0.0002797  loss_mask_dn_6: 0.3205  loss_dice_dn_6: 1.191  loss_bbox_dn_6: 0.2398  loss_giou_dn_6: 0.4383  loss_ce_7: 0.2292  loss_mask_7: 0.3095  loss_dice_7: 1.144  loss_bbox_7: 0.3134  loss_giou_7: 0.5137  loss_ce_dn_7: 0.0002142  loss_mask_dn_7: 0.3256  loss_dice_dn_7: 1.193  loss_bbox_dn_7: 0.2428  loss_giou_dn_7: 0.4371  loss_ce_8: 0.2137  loss_mask_8: 0.3238  loss_dice_8: 1.159  loss_bbox_8: 0.3072  loss_giou_8: 0.5138  loss_ce_dn_8: 0.0001753  loss_mask_dn_8: 0.3262  loss_dice_dn_8: 1.197  loss_bbox_dn_8: 0.2408  loss_giou_dn_8: 0.4415  loss_ce_interm: 0.5948  loss_mask_interm: 0.3135  loss_dice_interm: 1.177  loss_bbox_interm: 0.3508  loss_giou_interm: 0.6219    time: 0.4902  last_time: 0.4919  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:29:32 d2.utils.events]:  eta: 0:08:31  iter: 2959  total_loss: 65.92  loss_ce: 0.2604  loss_mask: 0.4194  loss_dice: 1.058  loss_bbox: 0.3734  loss_giou: 0.5931  loss_ce_dn: 0.0002925  loss_mask_dn: 0.4074  loss_dice_dn: 1.108  loss_bbox_dn: 0.2344  loss_giou_dn: 0.412  loss_ce_0: 0.5505  loss_mask_0: 0.4719  loss_dice_0: 1.169  loss_bbox_0: 0.7188  loss_giou_0: 0.8644  loss_ce_dn_0: 0.0191  loss_mask_dn_0: 0.626  loss_dice_dn_0: 2.205  loss_bbox_dn_0: 0.8049  loss_giou_dn_0: 0.8521  loss_ce_1: 0.7393  loss_mask_1: 0.4427  loss_dice_1: 1.1  loss_bbox_1: 0.3992  loss_giou_1: 0.5596  loss_ce_dn_1: 0.000586  loss_mask_dn_1: 0.4146  loss_dice_dn_1: 1.148  loss_bbox_dn_1: 0.3546  loss_giou_dn_1: 0.4851  loss_ce_2: 0.6177  loss_mask_2: 0.4008  loss_dice_2: 1.068  loss_bbox_2: 0.3842  loss_giou_2: 0.5948  loss_ce_dn_2: 0.0005588  loss_mask_dn_2: 0.4115  loss_dice_dn_2: 1.111  loss_bbox_dn_2: 0.3063  loss_giou_dn_2: 0.4398  loss_ce_3: 0.5267  loss_mask_3: 0.3978  loss_dice_3: 1.169  loss_bbox_3: 0.3601  loss_giou_3: 0.5688  loss_ce_dn_3: 0.0006091  loss_mask_dn_3: 0.398  loss_dice_dn_3: 1.106  loss_bbox_dn_3: 0.2634  loss_giou_dn_3: 0.4279  loss_ce_4: 0.3973  loss_mask_4: 0.3904  loss_dice_4: 1.109  loss_bbox_4: 0.363  loss_giou_4: 0.5433  loss_ce_dn_4: 0.0004222  loss_mask_dn_4: 0.4132  loss_dice_dn_4: 1.09  loss_bbox_dn_4: 0.2406  loss_giou_dn_4: 0.4205  loss_ce_5: 0.2764  loss_mask_5: 0.4034  loss_dice_5: 1.054  loss_bbox_5: 0.377  loss_giou_5: 0.576  loss_ce_dn_5: 0.0004928  loss_mask_dn_5: 0.4058  loss_dice_dn_5: 1.113  loss_bbox_dn_5: 0.2407  loss_giou_dn_5: 0.4132  loss_ce_6: 0.2345  loss_mask_6: 0.4063  loss_dice_6: 1.083  loss_bbox_6: 0.3686  loss_giou_6: 0.5882  loss_ce_dn_6: 0.0005304  loss_mask_dn_6: 0.4028  loss_dice_dn_6: 1.097  loss_bbox_dn_6: 0.2404  loss_giou_dn_6: 0.4154  loss_ce_7: 0.2448  loss_mask_7: 0.41  loss_dice_7: 1.058  loss_bbox_7: 0.3614  loss_giou_7: 0.5865  loss_ce_dn_7: 0.0003496  loss_mask_dn_7: 0.4128  loss_dice_dn_7: 1.11  loss_bbox_dn_7: 0.2368  loss_giou_dn_7: 0.4146  loss_ce_8: 0.247  loss_mask_8: 0.4121  loss_dice_8: 1.077  loss_bbox_8: 0.3711  loss_giou_8: 0.587  loss_ce_dn_8: 0.0002847  loss_mask_dn_8: 0.409  loss_dice_dn_8: 1.106  loss_bbox_dn_8: 0.2336  loss_giou_dn_8: 0.4143  loss_ce_interm: 0.8374  loss_mask_interm: 0.42  loss_dice_interm: 1.171  loss_bbox_interm: 0.3419  loss_giou_interm: 0.6753    time: 0.4902  last_time: 0.5073  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:29:42 d2.utils.events]:  eta: 0:08:21  iter: 2979  total_loss: 67.52  loss_ce: 0.4871  loss_mask: 0.3442  loss_dice: 1.572  loss_bbox: 0.3329  loss_giou: 0.6263  loss_ce_dn: 0.0002745  loss_mask_dn: 0.3409  loss_dice_dn: 1.399  loss_bbox_dn: 0.2442  loss_giou_dn: 0.4916  loss_ce_0: 0.7998  loss_mask_0: 0.3831  loss_dice_0: 1.645  loss_bbox_0: 0.722  loss_giou_0: 1.002  loss_ce_dn_0: 0.03927  loss_mask_dn_0: 0.6161  loss_dice_dn_0: 2.755  loss_bbox_dn_0: 0.796  loss_giou_dn_0: 0.8491  loss_ce_1: 0.8585  loss_mask_1: 0.3635  loss_dice_1: 1.593  loss_bbox_1: 0.3878  loss_giou_1: 0.6386  loss_ce_dn_1: 0.0005792  loss_mask_dn_1: 0.3423  loss_dice_dn_1: 1.46  loss_bbox_dn_1: 0.317  loss_giou_dn_1: 0.5242  loss_ce_2: 0.5707  loss_mask_2: 0.3929  loss_dice_2: 1.503  loss_bbox_2: 0.3871  loss_giou_2: 0.6042  loss_ce_dn_2: 0.0005837  loss_mask_dn_2: 0.3283  loss_dice_dn_2: 1.393  loss_bbox_dn_2: 0.3024  loss_giou_dn_2: 0.529  loss_ce_3: 0.5448  loss_mask_3: 0.3441  loss_dice_3: 1.491  loss_bbox_3: 0.3149  loss_giou_3: 0.654  loss_ce_dn_3: 0.0005494  loss_mask_dn_3: 0.3398  loss_dice_dn_3: 1.382  loss_bbox_dn_3: 0.2712  loss_giou_dn_3: 0.5232  loss_ce_4: 0.5632  loss_mask_4: 0.3416  loss_dice_4: 1.55  loss_bbox_4: 0.3307  loss_giou_4: 0.6688  loss_ce_dn_4: 0.0002444  loss_mask_dn_4: 0.3368  loss_dice_dn_4: 1.382  loss_bbox_dn_4: 0.2804  loss_giou_dn_4: 0.5165  loss_ce_5: 0.5615  loss_mask_5: 0.3556  loss_dice_5: 1.531  loss_bbox_5: 0.3443  loss_giou_5: 0.6562  loss_ce_dn_5: 0.0003237  loss_mask_dn_5: 0.3476  loss_dice_dn_5: 1.406  loss_bbox_dn_5: 0.2522  loss_giou_dn_5: 0.4998  loss_ce_6: 0.4847  loss_mask_6: 0.3651  loss_dice_6: 1.53  loss_bbox_6: 0.3221  loss_giou_6: 0.65  loss_ce_dn_6: 0.0003987  loss_mask_dn_6: 0.3426  loss_dice_dn_6: 1.403  loss_bbox_dn_6: 0.2479  loss_giou_dn_6: 0.4918  loss_ce_7: 0.5431  loss_mask_7: 0.3511  loss_dice_7: 1.55  loss_bbox_7: 0.321  loss_giou_7: 0.6619  loss_ce_dn_7: 0.0002159  loss_mask_dn_7: 0.3381  loss_dice_dn_7: 1.418  loss_bbox_dn_7: 0.2427  loss_giou_dn_7: 0.4947  loss_ce_8: 0.4857  loss_mask_8: 0.347  loss_dice_8: 1.564  loss_bbox_8: 0.3257  loss_giou_8: 0.6293  loss_ce_dn_8: 0.0002374  loss_mask_dn_8: 0.3442  loss_dice_dn_8: 1.427  loss_bbox_dn_8: 0.2448  loss_giou_dn_8: 0.4927  loss_ce_interm: 0.8942  loss_mask_interm: 0.4114  loss_dice_interm: 1.642  loss_bbox_interm: 0.4367  loss_giou_interm: 0.5939    time: 0.4902  last_time: 0.4995  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:29:53 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:29:53 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:29:53 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:29:53 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:29:54 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0761 s/iter. Eval: 0.0108 s/iter. Total: 0.0877 s/iter. ETA=0:00:04\n",
      "[03/14 17:29:59 d2.evaluation.evaluator]: Inference done 65/67. Dataloading: 0.0010 s/iter. Inference: 0.0815 s/iter. Eval: 0.0096 s/iter. Total: 0.0922 s/iter. ETA=0:00:00\n",
      "[03/14 17:30:00 d2.evaluation.evaluator]: Total inference time: 0:00:05.726774 (0.092367 s / iter per device, on 1 devices)\n",
      "[03/14 17:30:00 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.080542 s / iter per device, on 1 devices)\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.825\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.371\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.574\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 49.729 | 82.517 | 55.049 | 48.472 | 62.106 |  nan  |\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:30:00 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.237\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.332\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.348\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 25.657 | 72.385 | 4.731  | 23.740 | 34.396 |  nan  |\n",
      "[03/14 17:30:00 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:30:00 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: 49.7291,82.5170,55.0493,48.4717,62.1062,nan\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:30:00 d2.evaluation.testing]: copypaste: 25.6570,72.3854,4.7313,23.7402,34.3963,nan\n",
      "[03/14 17:30:00 d2.utils.events]:  eta: 0:08:12  iter: 2999  total_loss: 53.74  loss_ce: 0.06764  loss_mask: 0.3951  loss_dice: 1.135  loss_bbox: 0.2758  loss_giou: 0.3789  loss_ce_dn: 8.695e-05  loss_mask_dn: 0.3973  loss_dice_dn: 1.078  loss_bbox_dn: 0.2218  loss_giou_dn: 0.3246  loss_ce_0: 0.5499  loss_mask_0: 0.4164  loss_dice_0: 1.043  loss_bbox_0: 0.7251  loss_giou_0: 0.8923  loss_ce_dn_0: 0.019  loss_mask_dn_0: 0.5689  loss_dice_dn_0: 1.708  loss_bbox_dn_0: 0.8244  loss_giou_dn_0: 0.8391  loss_ce_1: 0.7025  loss_mask_1: 0.4172  loss_dice_1: 1.11  loss_bbox_1: 0.328  loss_giou_1: 0.4123  loss_ce_dn_1: 0.0005423  loss_mask_dn_1: 0.4282  loss_dice_dn_1: 1.103  loss_bbox_dn_1: 0.3319  loss_giou_dn_1: 0.4013  loss_ce_2: 0.4063  loss_mask_2: 0.4121  loss_dice_2: 1.144  loss_bbox_2: 0.297  loss_giou_2: 0.4162  loss_ce_dn_2: 0.0005042  loss_mask_dn_2: 0.4209  loss_dice_dn_2: 1.063  loss_bbox_dn_2: 0.2512  loss_giou_dn_2: 0.3454  loss_ce_3: 0.2455  loss_mask_3: 0.4327  loss_dice_3: 1.109  loss_bbox_3: 0.2801  loss_giou_3: 0.396  loss_ce_dn_3: 0.000453  loss_mask_dn_3: 0.4028  loss_dice_dn_3: 1.065  loss_bbox_dn_3: 0.2241  loss_giou_dn_3: 0.3326  loss_ce_4: 0.144  loss_mask_4: 0.3795  loss_dice_4: 1.083  loss_bbox_4: 0.2874  loss_giou_4: 0.4066  loss_ce_dn_4: 0.0002629  loss_mask_dn_4: 0.4045  loss_dice_dn_4: 1.066  loss_bbox_dn_4: 0.2273  loss_giou_dn_4: 0.3342  loss_ce_5: 0.1064  loss_mask_5: 0.3813  loss_dice_5: 1.097  loss_bbox_5: 0.2884  loss_giou_5: 0.3965  loss_ce_dn_5: 0.0003367  loss_mask_dn_5: 0.3959  loss_dice_dn_5: 1.072  loss_bbox_dn_5: 0.2175  loss_giou_dn_5: 0.3221  loss_ce_6: 0.07994  loss_mask_6: 0.4027  loss_dice_6: 1.089  loss_bbox_6: 0.274  loss_giou_6: 0.3798  loss_ce_dn_6: 0.0003026  loss_mask_dn_6: 0.3998  loss_dice_dn_6: 1.067  loss_bbox_dn_6: 0.2259  loss_giou_dn_6: 0.3256  loss_ce_7: 0.06507  loss_mask_7: 0.3766  loss_dice_7: 1.086  loss_bbox_7: 0.2688  loss_giou_7: 0.3814  loss_ce_dn_7: 0.0001334  loss_mask_dn_7: 0.3942  loss_dice_dn_7: 1.081  loss_bbox_dn_7: 0.2178  loss_giou_dn_7: 0.3215  loss_ce_8: 0.06575  loss_mask_8: 0.3919  loss_dice_8: 1.112  loss_bbox_8: 0.2787  loss_giou_8: 0.3798  loss_ce_dn_8: 9.574e-05  loss_mask_dn_8: 0.398  loss_dice_dn_8: 1.07  loss_bbox_dn_8: 0.219  loss_giou_dn_8: 0.3228  loss_ce_interm: 0.5923  loss_mask_interm: 0.4205  loss_dice_interm: 1.071  loss_bbox_interm: 0.3131  loss_giou_interm: 0.4672    time: 0.4904  last_time: 0.5220  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:30:10 d2.utils.events]:  eta: 0:08:03  iter: 3019  total_loss: 71.26  loss_ce: 0.4305  loss_mask: 0.2854  loss_dice: 1.285  loss_bbox: 0.2997  loss_giou: 0.6022  loss_ce_dn: 0.00026  loss_mask_dn: 0.2805  loss_dice_dn: 1.327  loss_bbox_dn: 0.2347  loss_giou_dn: 0.4775  loss_ce_0: 0.7343  loss_mask_0: 0.3057  loss_dice_0: 1.354  loss_bbox_0: 0.6365  loss_giou_0: 1.074  loss_ce_dn_0: 0.01886  loss_mask_dn_0: 0.4983  loss_dice_dn_0: 2.651  loss_bbox_dn_0: 0.739  loss_giou_dn_0: 0.8439  loss_ce_1: 0.694  loss_mask_1: 0.2936  loss_dice_1: 1.296  loss_bbox_1: 0.3937  loss_giou_1: 0.5968  loss_ce_dn_1: 0.0004262  loss_mask_dn_1: 0.2863  loss_dice_dn_1: 1.328  loss_bbox_dn_1: 0.344  loss_giou_dn_1: 0.5005  loss_ce_2: 0.632  loss_mask_2: 0.282  loss_dice_2: 1.322  loss_bbox_2: 0.3231  loss_giou_2: 0.6099  loss_ce_dn_2: 0.0003446  loss_mask_dn_2: 0.2867  loss_dice_dn_2: 1.327  loss_bbox_dn_2: 0.256  loss_giou_dn_2: 0.4899  loss_ce_3: 0.6161  loss_mask_3: 0.2734  loss_dice_3: 1.347  loss_bbox_3: 0.2934  loss_giou_3: 0.5994  loss_ce_dn_3: 0.0003812  loss_mask_dn_3: 0.2755  loss_dice_dn_3: 1.344  loss_bbox_dn_3: 0.239  loss_giou_dn_3: 0.4792  loss_ce_4: 0.5717  loss_mask_4: 0.2843  loss_dice_4: 1.308  loss_bbox_4: 0.2891  loss_giou_4: 0.6119  loss_ce_dn_4: 0.0003056  loss_mask_dn_4: 0.2853  loss_dice_dn_4: 1.339  loss_bbox_dn_4: 0.2421  loss_giou_dn_4: 0.4866  loss_ce_5: 0.4978  loss_mask_5: 0.2815  loss_dice_5: 1.282  loss_bbox_5: 0.2961  loss_giou_5: 0.5962  loss_ce_dn_5: 0.000477  loss_mask_dn_5: 0.2843  loss_dice_dn_5: 1.34  loss_bbox_dn_5: 0.2399  loss_giou_dn_5: 0.4708  loss_ce_6: 0.4898  loss_mask_6: 0.2926  loss_dice_6: 1.272  loss_bbox_6: 0.2977  loss_giou_6: 0.6056  loss_ce_dn_6: 0.0004647  loss_mask_dn_6: 0.2848  loss_dice_dn_6: 1.339  loss_bbox_dn_6: 0.2326  loss_giou_dn_6: 0.4727  loss_ce_7: 0.5085  loss_mask_7: 0.2937  loss_dice_7: 1.312  loss_bbox_7: 0.2987  loss_giou_7: 0.6011  loss_ce_dn_7: 0.0003377  loss_mask_dn_7: 0.2805  loss_dice_dn_7: 1.331  loss_bbox_dn_7: 0.2381  loss_giou_dn_7: 0.4811  loss_ce_8: 0.4229  loss_mask_8: 0.2898  loss_dice_8: 1.292  loss_bbox_8: 0.2952  loss_giou_8: 0.607  loss_ce_dn_8: 0.0002928  loss_mask_dn_8: 0.2796  loss_dice_dn_8: 1.324  loss_bbox_dn_8: 0.2377  loss_giou_dn_8: 0.4766  loss_ce_interm: 0.7433  loss_mask_interm: 0.2957  loss_dice_interm: 1.277  loss_bbox_interm: 0.4157  loss_giou_interm: 0.6784    time: 0.4904  last_time: 0.4412  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:30:20 d2.utils.events]:  eta: 0:07:53  iter: 3039  total_loss: 60  loss_ce: 0.2965  loss_mask: 0.2503  loss_dice: 1.435  loss_bbox: 0.2704  loss_giou: 0.5259  loss_ce_dn: 0.0001726  loss_mask_dn: 0.2634  loss_dice_dn: 1.213  loss_bbox_dn: 0.1859  loss_giou_dn: 0.4332  loss_ce_0: 0.5483  loss_mask_0: 0.254  loss_dice_0: 1.287  loss_bbox_0: 0.6282  loss_giou_0: 1.07  loss_ce_dn_0: 0.01876  loss_mask_dn_0: 0.4338  loss_dice_dn_0: 2.548  loss_bbox_dn_0: 0.7129  loss_giou_dn_0: 0.8433  loss_ce_1: 0.5775  loss_mask_1: 0.3158  loss_dice_1: 1.348  loss_bbox_1: 0.3405  loss_giou_1: 0.6505  loss_ce_dn_1: 0.0004528  loss_mask_dn_1: 0.279  loss_dice_dn_1: 1.286  loss_bbox_dn_1: 0.2622  loss_giou_dn_1: 0.4943  loss_ce_2: 0.4322  loss_mask_2: 0.3099  loss_dice_2: 1.395  loss_bbox_2: 0.3133  loss_giou_2: 0.6383  loss_ce_dn_2: 0.0003849  loss_mask_dn_2: 0.2638  loss_dice_dn_2: 1.242  loss_bbox_dn_2: 0.224  loss_giou_dn_2: 0.4683  loss_ce_3: 0.3772  loss_mask_3: 0.3023  loss_dice_3: 1.354  loss_bbox_3: 0.2943  loss_giou_3: 0.6299  loss_ce_dn_3: 0.0003653  loss_mask_dn_3: 0.2736  loss_dice_dn_3: 1.265  loss_bbox_dn_3: 0.1979  loss_giou_dn_3: 0.4488  loss_ce_4: 0.2983  loss_mask_4: 0.2876  loss_dice_4: 1.339  loss_bbox_4: 0.2988  loss_giou_4: 0.6353  loss_ce_dn_4: 0.0001935  loss_mask_dn_4: 0.2732  loss_dice_dn_4: 1.236  loss_bbox_dn_4: 0.1975  loss_giou_dn_4: 0.4395  loss_ce_5: 0.3064  loss_mask_5: 0.2856  loss_dice_5: 1.335  loss_bbox_5: 0.2668  loss_giou_5: 0.5719  loss_ce_dn_5: 0.0003762  loss_mask_dn_5: 0.2755  loss_dice_dn_5: 1.227  loss_bbox_dn_5: 0.1892  loss_giou_dn_5: 0.4427  loss_ce_6: 0.282  loss_mask_6: 0.2867  loss_dice_6: 1.362  loss_bbox_6: 0.2885  loss_giou_6: 0.6121  loss_ce_dn_6: 0.0003433  loss_mask_dn_6: 0.2593  loss_dice_dn_6: 1.227  loss_bbox_dn_6: 0.1907  loss_giou_dn_6: 0.4312  loss_ce_7: 0.2822  loss_mask_7: 0.2417  loss_dice_7: 1.38  loss_bbox_7: 0.2687  loss_giou_7: 0.5365  loss_ce_dn_7: 0.0002405  loss_mask_dn_7: 0.2656  loss_dice_dn_7: 1.22  loss_bbox_dn_7: 0.187  loss_giou_dn_7: 0.4295  loss_ce_8: 0.286  loss_mask_8: 0.2327  loss_dice_8: 1.273  loss_bbox_8: 0.272  loss_giou_8: 0.5337  loss_ce_dn_8: 0.0001801  loss_mask_dn_8: 0.2596  loss_dice_dn_8: 1.217  loss_bbox_dn_8: 0.1871  loss_giou_dn_8: 0.4372  loss_ce_interm: 0.619  loss_mask_interm: 0.279  loss_dice_interm: 1.445  loss_bbox_interm: 0.3714  loss_giou_interm: 0.7176    time: 0.4904  last_time: 0.4652  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:30:30 d2.utils.events]:  eta: 0:07:43  iter: 3059  total_loss: 61.65  loss_ce: 0.2212  loss_mask: 0.2817  loss_dice: 1.327  loss_bbox: 0.2523  loss_giou: 0.5067  loss_ce_dn: 0.0001069  loss_mask_dn: 0.295  loss_dice_dn: 1.242  loss_bbox_dn: 0.2182  loss_giou_dn: 0.4135  loss_ce_0: 0.6315  loss_mask_0: 0.3391  loss_dice_0: 1.349  loss_bbox_0: 0.626  loss_giou_0: 0.9981  loss_ce_dn_0: 0.039  loss_mask_dn_0: 0.6519  loss_dice_dn_0: 2.727  loss_bbox_dn_0: 0.7606  loss_giou_dn_0: 0.8471  loss_ce_1: 0.6216  loss_mask_1: 0.306  loss_dice_1: 1.283  loss_bbox_1: 0.3161  loss_giou_1: 0.529  loss_ce_dn_1: 0.0006252  loss_mask_dn_1: 0.3119  loss_dice_dn_1: 1.299  loss_bbox_dn_1: 0.3091  loss_giou_dn_1: 0.4755  loss_ce_2: 0.4308  loss_mask_2: 0.3061  loss_dice_2: 1.308  loss_bbox_2: 0.2889  loss_giou_2: 0.5484  loss_ce_dn_2: 0.0003348  loss_mask_dn_2: 0.2966  loss_dice_dn_2: 1.291  loss_bbox_dn_2: 0.2644  loss_giou_dn_2: 0.461  loss_ce_3: 0.2796  loss_mask_3: 0.3202  loss_dice_3: 1.389  loss_bbox_3: 0.2929  loss_giou_3: 0.5102  loss_ce_dn_3: 0.0003321  loss_mask_dn_3: 0.2922  loss_dice_dn_3: 1.299  loss_bbox_dn_3: 0.2548  loss_giou_dn_3: 0.4304  loss_ce_4: 0.2825  loss_mask_4: 0.3331  loss_dice_4: 1.383  loss_bbox_4: 0.2656  loss_giou_4: 0.5274  loss_ce_dn_4: 0.0001901  loss_mask_dn_4: 0.2944  loss_dice_dn_4: 1.28  loss_bbox_dn_4: 0.226  loss_giou_dn_4: 0.4304  loss_ce_5: 0.2879  loss_mask_5: 0.2918  loss_dice_5: 1.332  loss_bbox_5: 0.2614  loss_giou_5: 0.5031  loss_ce_dn_5: 0.0003099  loss_mask_dn_5: 0.307  loss_dice_dn_5: 1.256  loss_bbox_dn_5: 0.2213  loss_giou_dn_5: 0.4207  loss_ce_6: 0.2433  loss_mask_6: 0.2936  loss_dice_6: 1.358  loss_bbox_6: 0.2523  loss_giou_6: 0.5054  loss_ce_dn_6: 0.0003102  loss_mask_dn_6: 0.2965  loss_dice_dn_6: 1.255  loss_bbox_dn_6: 0.2202  loss_giou_dn_6: 0.4162  loss_ce_7: 0.2275  loss_mask_7: 0.2906  loss_dice_7: 1.353  loss_bbox_7: 0.2531  loss_giou_7: 0.5084  loss_ce_dn_7: 0.0001541  loss_mask_dn_7: 0.2967  loss_dice_dn_7: 1.247  loss_bbox_dn_7: 0.2244  loss_giou_dn_7: 0.4126  loss_ce_8: 0.2264  loss_mask_8: 0.2828  loss_dice_8: 1.329  loss_bbox_8: 0.247  loss_giou_8: 0.5113  loss_ce_dn_8: 0.0001124  loss_mask_dn_8: 0.2951  loss_dice_dn_8: 1.244  loss_bbox_dn_8: 0.2185  loss_giou_dn_8: 0.415  loss_ce_interm: 0.6584  loss_mask_interm: 0.3365  loss_dice_interm: 1.342  loss_bbox_interm: 0.4038  loss_giou_interm: 0.6599    time: 0.4904  last_time: 0.4839  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:30:40 d2.utils.events]:  eta: 0:07:33  iter: 3079  total_loss: 60.68  loss_ce: 0.2741  loss_mask: 0.3408  loss_dice: 1.191  loss_bbox: 0.3128  loss_giou: 0.5083  loss_ce_dn: 0.0002601  loss_mask_dn: 0.3103  loss_dice_dn: 1.156  loss_bbox_dn: 0.2405  loss_giou_dn: 0.4289  loss_ce_0: 0.6627  loss_mask_0: 0.3216  loss_dice_0: 1.162  loss_bbox_0: 0.726  loss_giou_0: 0.9138  loss_ce_dn_0: 0.05926  loss_mask_dn_0: 0.5718  loss_dice_dn_0: 2.613  loss_bbox_dn_0: 0.7842  loss_giou_dn_0: 0.8525  loss_ce_1: 0.6816  loss_mask_1: 0.3127  loss_dice_1: 1.168  loss_bbox_1: 0.2994  loss_giou_1: 0.5266  loss_ce_dn_1: 0.0006943  loss_mask_dn_1: 0.3171  loss_dice_dn_1: 1.202  loss_bbox_dn_1: 0.3041  loss_giou_dn_1: 0.496  loss_ce_2: 0.5249  loss_mask_2: 0.3021  loss_dice_2: 1.171  loss_bbox_2: 0.3023  loss_giou_2: 0.4669  loss_ce_dn_2: 0.0004495  loss_mask_dn_2: 0.3232  loss_dice_dn_2: 1.185  loss_bbox_dn_2: 0.281  loss_giou_dn_2: 0.4504  loss_ce_3: 0.373  loss_mask_3: 0.289  loss_dice_3: 1.179  loss_bbox_3: 0.2919  loss_giou_3: 0.4411  loss_ce_dn_3: 0.0005258  loss_mask_dn_3: 0.3104  loss_dice_dn_3: 1.176  loss_bbox_dn_3: 0.2702  loss_giou_dn_3: 0.4398  loss_ce_4: 0.3217  loss_mask_4: 0.2859  loss_dice_4: 1.179  loss_bbox_4: 0.3107  loss_giou_4: 0.4493  loss_ce_dn_4: 0.0003068  loss_mask_dn_4: 0.3073  loss_dice_dn_4: 1.187  loss_bbox_dn_4: 0.25  loss_giou_dn_4: 0.4348  loss_ce_5: 0.292  loss_mask_5: 0.2829  loss_dice_5: 1.221  loss_bbox_5: 0.3095  loss_giou_5: 0.4679  loss_ce_dn_5: 0.0004641  loss_mask_dn_5: 0.3132  loss_dice_dn_5: 1.168  loss_bbox_dn_5: 0.2408  loss_giou_dn_5: 0.4329  loss_ce_6: 0.2566  loss_mask_6: 0.3314  loss_dice_6: 1.188  loss_bbox_6: 0.3126  loss_giou_6: 0.5009  loss_ce_dn_6: 0.0004747  loss_mask_dn_6: 0.316  loss_dice_dn_6: 1.163  loss_bbox_dn_6: 0.2396  loss_giou_dn_6: 0.4267  loss_ce_7: 0.2541  loss_mask_7: 0.3381  loss_dice_7: 1.184  loss_bbox_7: 0.3175  loss_giou_7: 0.5098  loss_ce_dn_7: 0.0003301  loss_mask_dn_7: 0.3081  loss_dice_dn_7: 1.161  loss_bbox_dn_7: 0.2407  loss_giou_dn_7: 0.4262  loss_ce_8: 0.2585  loss_mask_8: 0.3363  loss_dice_8: 1.187  loss_bbox_8: 0.3141  loss_giou_8: 0.507  loss_ce_dn_8: 0.0002896  loss_mask_dn_8: 0.3063  loss_dice_dn_8: 1.161  loss_bbox_dn_8: 0.2405  loss_giou_dn_8: 0.4263  loss_ce_interm: 0.6678  loss_mask_interm: 0.3152  loss_dice_interm: 1.187  loss_bbox_interm: 0.4139  loss_giou_interm: 0.645    time: 0.4904  last_time: 0.4832  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:30:50 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:30:50 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:30:50 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:30:50 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:30:51 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0736 s/iter. Eval: 0.0105 s/iter. Total: 0.0850 s/iter. ETA=0:00:04\n",
      "[03/14 17:30:56 d2.evaluation.evaluator]: Total inference time: 0:00:05.466034 (0.088162 s / iter per device, on 1 devices)\n",
      "[03/14 17:30:56 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.076778 s / iter per device, on 1 devices)\n",
      "[03/14 17:30:56 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:30:56 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:30:56 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:30:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:30:56 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:30:56 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:30:56 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.495\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.809\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.623\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.574\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.631\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:30:56 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 49.482 | 80.943 | 57.424 | 48.502 | 62.286 |  nan  |\n",
      "[03/14 17:30:56 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:30:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:30:57 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "[03/14 17:30:57 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:30:57 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.744\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.233\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.365\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.363\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:30:57 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.094 | 74.367 | 5.043  | 23.293 | 36.507 |  nan  |\n",
      "[03/14 17:30:57 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:30:57 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: 49.4819,80.9434,57.4245,48.5017,62.2855,nan\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:30:57 d2.evaluation.testing]: copypaste: 26.0943,74.3675,5.0433,23.2928,36.5070,nan\n",
      "[03/14 17:30:57 d2.utils.events]:  eta: 0:07:24  iter: 3099  total_loss: 58.66  loss_ce: 0.3368  loss_mask: 0.3124  loss_dice: 1.098  loss_bbox: 0.2762  loss_giou: 0.4305  loss_ce_dn: 0.0001532  loss_mask_dn: 0.3199  loss_dice_dn: 1.105  loss_bbox_dn: 0.1965  loss_giou_dn: 0.4061  loss_ce_0: 0.6961  loss_mask_0: 0.3399  loss_dice_0: 1.144  loss_bbox_0: 0.6857  loss_giou_0: 0.8501  loss_ce_dn_0: 0.01875  loss_mask_dn_0: 0.5539  loss_dice_dn_0: 2.361  loss_bbox_dn_0: 0.791  loss_giou_dn_0: 0.8502  loss_ce_1: 0.6619  loss_mask_1: 0.3314  loss_dice_1: 1.122  loss_bbox_1: 0.2954  loss_giou_1: 0.5258  loss_ce_dn_1: 0.000416  loss_mask_dn_1: 0.3243  loss_dice_dn_1: 1.114  loss_bbox_dn_1: 0.2956  loss_giou_dn_1: 0.4757  loss_ce_2: 0.5498  loss_mask_2: 0.3188  loss_dice_2: 1.146  loss_bbox_2: 0.2702  loss_giou_2: 0.5107  loss_ce_dn_2: 0.0004584  loss_mask_dn_2: 0.3225  loss_dice_dn_2: 1.095  loss_bbox_dn_2: 0.2562  loss_giou_dn_2: 0.3985  loss_ce_3: 0.3857  loss_mask_3: 0.3127  loss_dice_3: 1.143  loss_bbox_3: 0.2828  loss_giou_3: 0.4743  loss_ce_dn_3: 0.0003708  loss_mask_dn_3: 0.3194  loss_dice_dn_3: 1.117  loss_bbox_dn_3: 0.2198  loss_giou_dn_3: 0.378  loss_ce_4: 0.3535  loss_mask_4: 0.3091  loss_dice_4: 1.168  loss_bbox_4: 0.2944  loss_giou_4: 0.4563  loss_ce_dn_4: 0.0002095  loss_mask_dn_4: 0.3224  loss_dice_dn_4: 1.123  loss_bbox_dn_4: 0.192  loss_giou_dn_4: 0.3754  loss_ce_5: 0.3271  loss_mask_5: 0.3178  loss_dice_5: 1.116  loss_bbox_5: 0.2835  loss_giou_5: 0.466  loss_ce_dn_5: 0.0003187  loss_mask_dn_5: 0.3251  loss_dice_dn_5: 1.108  loss_bbox_dn_5: 0.2026  loss_giou_dn_5: 0.3908  loss_ce_6: 0.3404  loss_mask_6: 0.3191  loss_dice_6: 1.134  loss_bbox_6: 0.2771  loss_giou_6: 0.4272  loss_ce_dn_6: 0.0002809  loss_mask_dn_6: 0.3201  loss_dice_dn_6: 1.12  loss_bbox_dn_6: 0.2004  loss_giou_dn_6: 0.3953  loss_ce_7: 0.3461  loss_mask_7: 0.3025  loss_dice_7: 1.118  loss_bbox_7: 0.2753  loss_giou_7: 0.4339  loss_ce_dn_7: 0.0001856  loss_mask_dn_7: 0.3188  loss_dice_dn_7: 1.118  loss_bbox_dn_7: 0.1989  loss_giou_dn_7: 0.4104  loss_ce_8: 0.3286  loss_mask_8: 0.3133  loss_dice_8: 1.127  loss_bbox_8: 0.2784  loss_giou_8: 0.4226  loss_ce_dn_8: 0.0001418  loss_mask_dn_8: 0.318  loss_dice_dn_8: 1.104  loss_bbox_dn_8: 0.197  loss_giou_dn_8: 0.4032  loss_ce_interm: 0.7499  loss_mask_interm: 0.3405  loss_dice_interm: 1.103  loss_bbox_interm: 0.3848  loss_giou_interm: 0.6501    time: 0.4905  last_time: 0.5082  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:31:07 d2.utils.events]:  eta: 0:07:14  iter: 3119  total_loss: 60.58  loss_ce: 0.1495  loss_mask: 0.4805  loss_dice: 1.026  loss_bbox: 0.3056  loss_giou: 0.5016  loss_ce_dn: 0.0001126  loss_mask_dn: 0.5142  loss_dice_dn: 1.073  loss_bbox_dn: 0.2338  loss_giou_dn: 0.3598  loss_ce_0: 0.656  loss_mask_0: 0.4934  loss_dice_0: 1.091  loss_bbox_0: 0.8585  loss_giou_0: 0.9751  loss_ce_dn_0: 0.05891  loss_mask_dn_0: 0.9499  loss_dice_dn_0: 2.159  loss_bbox_dn_0: 0.9423  loss_giou_dn_0: 0.8533  loss_ce_1: 0.7827  loss_mask_1: 0.5036  loss_dice_1: 1.103  loss_bbox_1: 0.3101  loss_giou_1: 0.5062  loss_ce_dn_1: 0.0006437  loss_mask_dn_1: 0.5213  loss_dice_dn_1: 1.084  loss_bbox_dn_1: 0.3459  loss_giou_dn_1: 0.4534  loss_ce_2: 0.5351  loss_mask_2: 0.5412  loss_dice_2: 1.133  loss_bbox_2: 0.2945  loss_giou_2: 0.494  loss_ce_dn_2: 0.0004304  loss_mask_dn_2: 0.524  loss_dice_dn_2: 1.088  loss_bbox_dn_2: 0.3078  loss_giou_dn_2: 0.4014  loss_ce_3: 0.353  loss_mask_3: 0.4828  loss_dice_3: 1.041  loss_bbox_3: 0.2949  loss_giou_3: 0.4847  loss_ce_dn_3: 0.0005316  loss_mask_dn_3: 0.5201  loss_dice_dn_3: 1.111  loss_bbox_dn_3: 0.2867  loss_giou_dn_3: 0.3856  loss_ce_4: 0.3272  loss_mask_4: 0.4851  loss_dice_4: 1.053  loss_bbox_4: 0.2903  loss_giou_4: 0.4851  loss_ce_dn_4: 0.0002524  loss_mask_dn_4: 0.5244  loss_dice_dn_4: 1.093  loss_bbox_dn_4: 0.2727  loss_giou_dn_4: 0.3673  loss_ce_5: 0.169  loss_mask_5: 0.512  loss_dice_5: 1.08  loss_bbox_5: 0.287  loss_giou_5: 0.4975  loss_ce_dn_5: 0.0003518  loss_mask_dn_5: 0.5092  loss_dice_dn_5: 1.084  loss_bbox_dn_5: 0.245  loss_giou_dn_5: 0.3606  loss_ce_6: 0.1643  loss_mask_6: 0.4752  loss_dice_6: 1.05  loss_bbox_6: 0.3094  loss_giou_6: 0.5033  loss_ce_dn_6: 0.0003169  loss_mask_dn_6: 0.5046  loss_dice_dn_6: 1.083  loss_bbox_dn_6: 0.2388  loss_giou_dn_6: 0.3512  loss_ce_7: 0.1559  loss_mask_7: 0.4813  loss_dice_7: 1.028  loss_bbox_7: 0.3083  loss_giou_7: 0.5006  loss_ce_dn_7: 0.0001732  loss_mask_dn_7: 0.5025  loss_dice_dn_7: 1.069  loss_bbox_dn_7: 0.2461  loss_giou_dn_7: 0.3558  loss_ce_8: 0.1465  loss_mask_8: 0.4808  loss_dice_8: 1.027  loss_bbox_8: 0.3093  loss_giou_8: 0.4974  loss_ce_dn_8: 0.0001237  loss_mask_dn_8: 0.5127  loss_dice_dn_8: 1.081  loss_bbox_dn_8: 0.24  loss_giou_dn_8: 0.3596  loss_ce_interm: 0.6727  loss_mask_interm: 0.5293  loss_dice_interm: 1.113  loss_bbox_interm: 0.438  loss_giou_interm: 0.5125    time: 0.4906  last_time: 0.5500  data_time: 0.0036  last_data_time: 0.0028   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:31:17 d2.utils.events]:  eta: 0:07:04  iter: 3139  total_loss: 60.18  loss_ce: 0.3323  loss_mask: 0.2538  loss_dice: 1.183  loss_bbox: 0.2552  loss_giou: 0.5641  loss_ce_dn: 0.0002062  loss_mask_dn: 0.231  loss_dice_dn: 1.116  loss_bbox_dn: 0.2099  loss_giou_dn: 0.4491  loss_ce_0: 0.7087  loss_mask_0: 0.2509  loss_dice_0: 1.265  loss_bbox_0: 0.5587  loss_giou_0: 0.9453  loss_ce_dn_0: 0.03879  loss_mask_dn_0: 0.5076  loss_dice_dn_0: 2.88  loss_bbox_dn_0: 0.6347  loss_giou_dn_0: 0.8561  loss_ce_1: 0.7754  loss_mask_1: 0.255  loss_dice_1: 1.184  loss_bbox_1: 0.3041  loss_giou_1: 0.5977  loss_ce_dn_1: 0.0004151  loss_mask_dn_1: 0.2405  loss_dice_dn_1: 1.138  loss_bbox_dn_1: 0.2654  loss_giou_dn_1: 0.5036  loss_ce_2: 0.608  loss_mask_2: 0.2565  loss_dice_2: 1.197  loss_bbox_2: 0.2939  loss_giou_2: 0.5693  loss_ce_dn_2: 0.0004206  loss_mask_dn_2: 0.2309  loss_dice_dn_2: 1.128  loss_bbox_dn_2: 0.242  loss_giou_dn_2: 0.4624  loss_ce_3: 0.4547  loss_mask_3: 0.2439  loss_dice_3: 1.245  loss_bbox_3: 0.3143  loss_giou_3: 0.5453  loss_ce_dn_3: 0.0005022  loss_mask_dn_3: 0.2319  loss_dice_dn_3: 1.116  loss_bbox_dn_3: 0.2289  loss_giou_dn_3: 0.456  loss_ce_4: 0.3309  loss_mask_4: 0.2468  loss_dice_4: 1.225  loss_bbox_4: 0.2756  loss_giou_4: 0.5087  loss_ce_dn_4: 0.0003135  loss_mask_dn_4: 0.2332  loss_dice_dn_4: 1.111  loss_bbox_dn_4: 0.2135  loss_giou_dn_4: 0.4548  loss_ce_5: 0.3453  loss_mask_5: 0.238  loss_dice_5: 1.219  loss_bbox_5: 0.2888  loss_giou_5: 0.5406  loss_ce_dn_5: 0.0004738  loss_mask_dn_5: 0.2316  loss_dice_dn_5: 1.11  loss_bbox_dn_5: 0.213  loss_giou_dn_5: 0.456  loss_ce_6: 0.3313  loss_mask_6: 0.2324  loss_dice_6: 1.193  loss_bbox_6: 0.2773  loss_giou_6: 0.5508  loss_ce_dn_6: 0.0004243  loss_mask_dn_6: 0.2293  loss_dice_dn_6: 1.108  loss_bbox_dn_6: 0.2062  loss_giou_dn_6: 0.4623  loss_ce_7: 0.3214  loss_mask_7: 0.2252  loss_dice_7: 1.216  loss_bbox_7: 0.2591  loss_giou_7: 0.5492  loss_ce_dn_7: 0.0002891  loss_mask_dn_7: 0.2289  loss_dice_dn_7: 1.123  loss_bbox_dn_7: 0.2059  loss_giou_dn_7: 0.4463  loss_ce_8: 0.3219  loss_mask_8: 0.2347  loss_dice_8: 1.205  loss_bbox_8: 0.2698  loss_giou_8: 0.5652  loss_ce_dn_8: 0.0002555  loss_mask_dn_8: 0.2287  loss_dice_dn_8: 1.111  loss_bbox_dn_8: 0.2072  loss_giou_dn_8: 0.451  loss_ce_interm: 0.7198  loss_mask_interm: 0.2457  loss_dice_interm: 1.181  loss_bbox_interm: 0.3488  loss_giou_interm: 0.6537    time: 0.4906  last_time: 0.5102  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:31:27 d2.utils.events]:  eta: 0:06:54  iter: 3159  total_loss: 57.87  loss_ce: 0.1573  loss_mask: 0.3441  loss_dice: 1.255  loss_bbox: 0.2862  loss_giou: 0.5357  loss_ce_dn: 0.0001161  loss_mask_dn: 0.3746  loss_dice_dn: 1.25  loss_bbox_dn: 0.2341  loss_giou_dn: 0.4171  loss_ce_0: 0.5834  loss_mask_0: 0.3831  loss_dice_0: 1.285  loss_bbox_0: 0.689  loss_giou_0: 0.9036  loss_ce_dn_0: 0.01863  loss_mask_dn_0: 0.6403  loss_dice_dn_0: 2.698  loss_bbox_dn_0: 0.7851  loss_giou_dn_0: 0.846  loss_ce_1: 0.6444  loss_mask_1: 0.3682  loss_dice_1: 1.269  loss_bbox_1: 0.2862  loss_giou_1: 0.5116  loss_ce_dn_1: 0.0004223  loss_mask_dn_1: 0.3755  loss_dice_dn_1: 1.281  loss_bbox_dn_1: 0.2964  loss_giou_dn_1: 0.4889  loss_ce_2: 0.4284  loss_mask_2: 0.3844  loss_dice_2: 1.281  loss_bbox_2: 0.309  loss_giou_2: 0.5454  loss_ce_dn_2: 0.0003717  loss_mask_dn_2: 0.3811  loss_dice_dn_2: 1.274  loss_bbox_dn_2: 0.2544  loss_giou_dn_2: 0.4302  loss_ce_3: 0.2113  loss_mask_3: 0.3825  loss_dice_3: 1.23  loss_bbox_3: 0.2923  loss_giou_3: 0.5303  loss_ce_dn_3: 0.0003966  loss_mask_dn_3: 0.3835  loss_dice_dn_3: 1.273  loss_bbox_dn_3: 0.2304  loss_giou_dn_3: 0.3987  loss_ce_4: 0.1167  loss_mask_4: 0.3412  loss_dice_4: 1.208  loss_bbox_4: 0.2949  loss_giou_4: 0.5299  loss_ce_dn_4: 0.0001934  loss_mask_dn_4: 0.3793  loss_dice_dn_4: 1.266  loss_bbox_dn_4: 0.2351  loss_giou_dn_4: 0.4006  loss_ce_5: 0.1361  loss_mask_5: 0.3688  loss_dice_5: 1.199  loss_bbox_5: 0.2874  loss_giou_5: 0.5446  loss_ce_dn_5: 0.0003333  loss_mask_dn_5: 0.378  loss_dice_dn_5: 1.261  loss_bbox_dn_5: 0.2262  loss_giou_dn_5: 0.4007  loss_ce_6: 0.154  loss_mask_6: 0.3721  loss_dice_6: 1.132  loss_bbox_6: 0.2882  loss_giou_6: 0.5203  loss_ce_dn_6: 0.0002719  loss_mask_dn_6: 0.3816  loss_dice_dn_6: 1.261  loss_bbox_dn_6: 0.233  loss_giou_dn_6: 0.4059  loss_ce_7: 0.1422  loss_mask_7: 0.3693  loss_dice_7: 1.314  loss_bbox_7: 0.2951  loss_giou_7: 0.5415  loss_ce_dn_7: 0.0001718  loss_mask_dn_7: 0.3806  loss_dice_dn_7: 1.25  loss_bbox_dn_7: 0.2278  loss_giou_dn_7: 0.4152  loss_ce_8: 0.1442  loss_mask_8: 0.3745  loss_dice_8: 1.268  loss_bbox_8: 0.2865  loss_giou_8: 0.5361  loss_ce_dn_8: 0.0001396  loss_mask_dn_8: 0.3792  loss_dice_dn_8: 1.257  loss_bbox_dn_8: 0.2351  loss_giou_dn_8: 0.4184  loss_ce_interm: 0.6911  loss_mask_interm: 0.3905  loss_dice_interm: 1.303  loss_bbox_interm: 0.3664  loss_giou_interm: 0.6295    time: 0.4906  last_time: 0.4879  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:31:37 d2.utils.events]:  eta: 0:06:44  iter: 3179  total_loss: 68.53  loss_ce: 0.4712  loss_mask: 0.3144  loss_dice: 1.387  loss_bbox: 0.3877  loss_giou: 0.5643  loss_ce_dn: 0.0003308  loss_mask_dn: 0.3115  loss_dice_dn: 1.495  loss_bbox_dn: 0.2659  loss_giou_dn: 0.4902  loss_ce_0: 0.776  loss_mask_0: 0.2885  loss_dice_0: 1.373  loss_bbox_0: 0.6931  loss_giou_0: 1.034  loss_ce_dn_0: 0.05878  loss_mask_dn_0: 0.5738  loss_dice_dn_0: 2.95  loss_bbox_dn_0: 0.7777  loss_giou_dn_0: 0.8463  loss_ce_1: 0.9371  loss_mask_1: 0.31  loss_dice_1: 1.565  loss_bbox_1: 0.3845  loss_giou_1: 0.5373  loss_ce_dn_1: 0.0007466  loss_mask_dn_1: 0.3154  loss_dice_dn_1: 1.513  loss_bbox_dn_1: 0.3518  loss_giou_dn_1: 0.5305  loss_ce_2: 0.7183  loss_mask_2: 0.3118  loss_dice_2: 1.388  loss_bbox_2: 0.3481  loss_giou_2: 0.5395  loss_ce_dn_2: 0.0004736  loss_mask_dn_2: 0.3099  loss_dice_dn_2: 1.512  loss_bbox_dn_2: 0.2993  loss_giou_dn_2: 0.5104  loss_ce_3: 0.4417  loss_mask_3: 0.3287  loss_dice_3: 1.373  loss_bbox_3: 0.3866  loss_giou_3: 0.6536  loss_ce_dn_3: 0.000505  loss_mask_dn_3: 0.3125  loss_dice_dn_3: 1.5  loss_bbox_dn_3: 0.2721  loss_giou_dn_3: 0.4848  loss_ce_4: 0.4286  loss_mask_4: 0.3076  loss_dice_4: 1.671  loss_bbox_4: 0.3809  loss_giou_4: 0.6443  loss_ce_dn_4: 0.0003171  loss_mask_dn_4: 0.3178  loss_dice_dn_4: 1.478  loss_bbox_dn_4: 0.2678  loss_giou_dn_4: 0.4827  loss_ce_5: 0.4672  loss_mask_5: 0.3124  loss_dice_5: 1.506  loss_bbox_5: 0.3675  loss_giou_5: 0.6058  loss_ce_dn_5: 0.0004407  loss_mask_dn_5: 0.3048  loss_dice_dn_5: 1.463  loss_bbox_dn_5: 0.2558  loss_giou_dn_5: 0.4891  loss_ce_6: 0.4472  loss_mask_6: 0.326  loss_dice_6: 1.418  loss_bbox_6: 0.3926  loss_giou_6: 0.5874  loss_ce_dn_6: 0.0004322  loss_mask_dn_6: 0.3121  loss_dice_dn_6: 1.479  loss_bbox_dn_6: 0.2585  loss_giou_dn_6: 0.4885  loss_ce_7: 0.4611  loss_mask_7: 0.3054  loss_dice_7: 1.489  loss_bbox_7: 0.3993  loss_giou_7: 0.5686  loss_ce_dn_7: 0.0002923  loss_mask_dn_7: 0.3156  loss_dice_dn_7: 1.489  loss_bbox_dn_7: 0.2642  loss_giou_dn_7: 0.4907  loss_ce_8: 0.4623  loss_mask_8: 0.3152  loss_dice_8: 1.376  loss_bbox_8: 0.3847  loss_giou_8: 0.5651  loss_ce_dn_8: 0.0003575  loss_mask_dn_8: 0.315  loss_dice_dn_8: 1.526  loss_bbox_dn_8: 0.2604  loss_giou_dn_8: 0.4851  loss_ce_interm: 0.9701  loss_mask_interm: 0.2954  loss_dice_interm: 1.531  loss_bbox_interm: 0.3881  loss_giou_interm: 0.7781    time: 0.4906  last_time: 0.4254  data_time: 0.0034  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:31:47 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:31:47 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:31:47 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:31:47 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:31:48 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.1012 s/iter. Eval: 0.0093 s/iter. Total: 0.1113 s/iter. ETA=0:00:06\n",
      "[03/14 17:31:53 d2.evaluation.evaluator]: Total inference time: 0:00:05.630714 (0.090818 s / iter per device, on 1 devices)\n",
      "[03/14 17:31:53 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.078792 s / iter per device, on 1 devices)\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.821\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.568\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.494\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.627\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.601\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.757\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 50.629 | 82.109 | 56.803 | 49.430 | 62.731 |  nan  |\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:31:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.733\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.242\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.356\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.370\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.592 | 73.337 | 5.404  | 24.187 | 35.631 |  nan  |\n",
      "[03/14 17:31:53 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:31:53 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: 50.6287,82.1091,56.8034,49.4297,62.7305,nan\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:31:53 d2.evaluation.testing]: copypaste: 26.5919,73.3367,5.4040,24.1866,35.6306,nan\n",
      "[03/14 17:31:53 d2.utils.events]:  eta: 0:06:34  iter: 3199  total_loss: 61.02  loss_ce: 0.297  loss_mask: 0.3645  loss_dice: 1.294  loss_bbox: 0.289  loss_giou: 0.466  loss_ce_dn: 9.884e-05  loss_mask_dn: 0.3583  loss_dice_dn: 1.239  loss_bbox_dn: 0.2069  loss_giou_dn: 0.3816  loss_ce_0: 0.6861  loss_mask_0: 0.3845  loss_dice_0: 1.465  loss_bbox_0: 0.6552  loss_giou_0: 0.9282  loss_ce_dn_0: 0.03054  loss_mask_dn_0: 0.6394  loss_dice_dn_0: 2.712  loss_bbox_dn_0: 0.826  loss_giou_dn_0: 0.8592  loss_ce_1: 0.5784  loss_mask_1: 0.3678  loss_dice_1: 1.393  loss_bbox_1: 0.3187  loss_giou_1: 0.5205  loss_ce_dn_1: 0.0005817  loss_mask_dn_1: 0.3794  loss_dice_dn_1: 1.36  loss_bbox_dn_1: 0.2606  loss_giou_dn_1: 0.4604  loss_ce_2: 0.3751  loss_mask_2: 0.3443  loss_dice_2: 1.489  loss_bbox_2: 0.2694  loss_giou_2: 0.4621  loss_ce_dn_2: 0.0005089  loss_mask_dn_2: 0.3625  loss_dice_dn_2: 1.293  loss_bbox_dn_2: 0.2276  loss_giou_dn_2: 0.3983  loss_ce_3: 0.2582  loss_mask_3: 0.3776  loss_dice_3: 1.437  loss_bbox_3: 0.2834  loss_giou_3: 0.4647  loss_ce_dn_3: 0.0004795  loss_mask_dn_3: 0.3643  loss_dice_dn_3: 1.273  loss_bbox_dn_3: 0.2362  loss_giou_dn_3: 0.3954  loss_ce_4: 0.3475  loss_mask_4: 0.3887  loss_dice_4: 1.459  loss_bbox_4: 0.3007  loss_giou_4: 0.4838  loss_ce_dn_4: 0.0002173  loss_mask_dn_4: 0.3605  loss_dice_dn_4: 1.27  loss_bbox_dn_4: 0.2196  loss_giou_dn_4: 0.3922  loss_ce_5: 0.3154  loss_mask_5: 0.3791  loss_dice_5: 1.403  loss_bbox_5: 0.3009  loss_giou_5: 0.4679  loss_ce_dn_5: 0.0003866  loss_mask_dn_5: 0.3669  loss_dice_dn_5: 1.27  loss_bbox_dn_5: 0.2133  loss_giou_dn_5: 0.3885  loss_ce_6: 0.2706  loss_mask_6: 0.3775  loss_dice_6: 1.396  loss_bbox_6: 0.2783  loss_giou_6: 0.4531  loss_ce_dn_6: 0.0003542  loss_mask_dn_6: 0.3611  loss_dice_dn_6: 1.251  loss_bbox_dn_6: 0.2103  loss_giou_dn_6: 0.3786  loss_ce_7: 0.2465  loss_mask_7: 0.3633  loss_dice_7: 1.282  loss_bbox_7: 0.2983  loss_giou_7: 0.4682  loss_ce_dn_7: 0.0001581  loss_mask_dn_7: 0.3617  loss_dice_dn_7: 1.253  loss_bbox_dn_7: 0.2096  loss_giou_dn_7: 0.3826  loss_ce_8: 0.2745  loss_mask_8: 0.3604  loss_dice_8: 1.36  loss_bbox_8: 0.302  loss_giou_8: 0.4691  loss_ce_dn_8: 0.0001344  loss_mask_dn_8: 0.3585  loss_dice_dn_8: 1.248  loss_bbox_dn_8: 0.2076  loss_giou_dn_8: 0.3824  loss_ce_interm: 0.6861  loss_mask_interm: 0.3765  loss_dice_interm: 1.489  loss_bbox_interm: 0.3785  loss_giou_interm: 0.6189    time: 0.4906  last_time: 0.5316  data_time: 0.0036  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:32:03 d2.utils.events]:  eta: 0:06:24  iter: 3219  total_loss: 61.49  loss_ce: 0.1238  loss_mask: 0.2982  loss_dice: 1.271  loss_bbox: 0.2755  loss_giou: 0.5631  loss_ce_dn: 9.677e-05  loss_mask_dn: 0.3149  loss_dice_dn: 1.287  loss_bbox_dn: 0.2226  loss_giou_dn: 0.3846  loss_ce_0: 0.6371  loss_mask_0: 0.341  loss_dice_0: 1.363  loss_bbox_0: 0.6264  loss_giou_0: 0.9147  loss_ce_dn_0: 0.01846  loss_mask_dn_0: 0.5269  loss_dice_dn_0: 2.289  loss_bbox_dn_0: 0.8123  loss_giou_dn_0: 0.8526  loss_ce_1: 0.6979  loss_mask_1: 0.3021  loss_dice_1: 1.332  loss_bbox_1: 0.2908  loss_giou_1: 0.5834  loss_ce_dn_1: 0.0007123  loss_mask_dn_1: 0.317  loss_dice_dn_1: 1.26  loss_bbox_dn_1: 0.3028  loss_giou_dn_1: 0.4492  loss_ce_2: 0.4783  loss_mask_2: 0.3374  loss_dice_2: 1.261  loss_bbox_2: 0.3108  loss_giou_2: 0.4765  loss_ce_dn_2: 0.0004183  loss_mask_dn_2: 0.3166  loss_dice_dn_2: 1.293  loss_bbox_dn_2: 0.2714  loss_giou_dn_2: 0.4264  loss_ce_3: 0.3038  loss_mask_3: 0.3412  loss_dice_3: 1.323  loss_bbox_3: 0.3044  loss_giou_3: 0.4741  loss_ce_dn_3: 0.0003896  loss_mask_dn_3: 0.313  loss_dice_dn_3: 1.303  loss_bbox_dn_3: 0.2419  loss_giou_dn_3: 0.3992  loss_ce_4: 0.1822  loss_mask_4: 0.3194  loss_dice_4: 1.214  loss_bbox_4: 0.2839  loss_giou_4: 0.538  loss_ce_dn_4: 0.0002114  loss_mask_dn_4: 0.3132  loss_dice_dn_4: 1.288  loss_bbox_dn_4: 0.2251  loss_giou_dn_4: 0.4048  loss_ce_5: 0.1448  loss_mask_5: 0.3196  loss_dice_5: 1.262  loss_bbox_5: 0.2876  loss_giou_5: 0.5909  loss_ce_dn_5: 0.0002595  loss_mask_dn_5: 0.3124  loss_dice_dn_5: 1.294  loss_bbox_dn_5: 0.2327  loss_giou_dn_5: 0.3955  loss_ce_6: 0.1419  loss_mask_6: 0.3246  loss_dice_6: 1.395  loss_bbox_6: 0.2818  loss_giou_6: 0.5847  loss_ce_dn_6: 0.0003081  loss_mask_dn_6: 0.3157  loss_dice_dn_6: 1.302  loss_bbox_dn_6: 0.2244  loss_giou_dn_6: 0.3916  loss_ce_7: 0.1137  loss_mask_7: 0.3123  loss_dice_7: 1.262  loss_bbox_7: 0.2757  loss_giou_7: 0.5749  loss_ce_dn_7: 0.0001451  loss_mask_dn_7: 0.313  loss_dice_dn_7: 1.282  loss_bbox_dn_7: 0.2185  loss_giou_dn_7: 0.3841  loss_ce_8: 0.1174  loss_mask_8: 0.3109  loss_dice_8: 1.271  loss_bbox_8: 0.2746  loss_giou_8: 0.5631  loss_ce_dn_8: 0.0001085  loss_mask_dn_8: 0.3142  loss_dice_dn_8: 1.29  loss_bbox_dn_8: 0.2255  loss_giou_dn_8: 0.3849  loss_ce_interm: 0.6443  loss_mask_interm: 0.317  loss_dice_interm: 1.365  loss_bbox_interm: 0.4366  loss_giou_interm: 0.6135    time: 0.4906  last_time: 0.4378  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:32:13 d2.utils.events]:  eta: 0:06:14  iter: 3239  total_loss: 59.52  loss_ce: 0.2433  loss_mask: 0.3413  loss_dice: 1.111  loss_bbox: 0.2665  loss_giou: 0.4354  loss_ce_dn: 0.0001487  loss_mask_dn: 0.3459  loss_dice_dn: 1.048  loss_bbox_dn: 0.2421  loss_giou_dn: 0.3907  loss_ce_0: 0.7236  loss_mask_0: 0.3782  loss_dice_0: 1.131  loss_bbox_0: 0.641  loss_giou_0: 0.7752  loss_ce_dn_0: 0.01853  loss_mask_dn_0: 0.7276  loss_dice_dn_0: 2.599  loss_bbox_dn_0: 0.8759  loss_giou_dn_0: 0.8535  loss_ce_1: 0.7092  loss_mask_1: 0.3573  loss_dice_1: 1.219  loss_bbox_1: 0.2938  loss_giou_1: 0.5238  loss_ce_dn_1: 0.0006203  loss_mask_dn_1: 0.3538  loss_dice_dn_1: 1.092  loss_bbox_dn_1: 0.3424  loss_giou_dn_1: 0.4251  loss_ce_2: 0.5476  loss_mask_2: 0.354  loss_dice_2: 1.161  loss_bbox_2: 0.3476  loss_giou_2: 0.4985  loss_ce_dn_2: 0.0005015  loss_mask_dn_2: 0.3501  loss_dice_dn_2: 1.056  loss_bbox_dn_2: 0.2768  loss_giou_dn_2: 0.4292  loss_ce_3: 0.3228  loss_mask_3: 0.3563  loss_dice_3: 1.126  loss_bbox_3: 0.3313  loss_giou_3: 0.4617  loss_ce_dn_3: 0.0004557  loss_mask_dn_3: 0.3474  loss_dice_dn_3: 1.073  loss_bbox_dn_3: 0.2585  loss_giou_dn_3: 0.4132  loss_ce_4: 0.3024  loss_mask_4: 0.3558  loss_dice_4: 1.082  loss_bbox_4: 0.2957  loss_giou_4: 0.4528  loss_ce_dn_4: 0.0002486  loss_mask_dn_4: 0.3475  loss_dice_dn_4: 1.062  loss_bbox_dn_4: 0.2433  loss_giou_dn_4: 0.4116  loss_ce_5: 0.1807  loss_mask_5: 0.3498  loss_dice_5: 1.167  loss_bbox_5: 0.2868  loss_giou_5: 0.4634  loss_ce_dn_5: 0.0003113  loss_mask_dn_5: 0.3485  loss_dice_dn_5: 1.066  loss_bbox_dn_5: 0.2464  loss_giou_dn_5: 0.3941  loss_ce_6: 0.1969  loss_mask_6: 0.3403  loss_dice_6: 1.142  loss_bbox_6: 0.2806  loss_giou_6: 0.4371  loss_ce_dn_6: 0.0003056  loss_mask_dn_6: 0.35  loss_dice_dn_6: 1.055  loss_bbox_dn_6: 0.2386  loss_giou_dn_6: 0.3947  loss_ce_7: 0.177  loss_mask_7: 0.3288  loss_dice_7: 1.134  loss_bbox_7: 0.2761  loss_giou_7: 0.423  loss_ce_dn_7: 0.0001755  loss_mask_dn_7: 0.3481  loss_dice_dn_7: 1.047  loss_bbox_dn_7: 0.247  loss_giou_dn_7: 0.3899  loss_ce_8: 0.1791  loss_mask_8: 0.3428  loss_dice_8: 1.065  loss_bbox_8: 0.2755  loss_giou_8: 0.4331  loss_ce_dn_8: 0.0001301  loss_mask_dn_8: 0.3442  loss_dice_dn_8: 1.054  loss_bbox_dn_8: 0.2416  loss_giou_dn_8: 0.3945  loss_ce_interm: 0.7445  loss_mask_interm: 0.3884  loss_dice_interm: 1.174  loss_bbox_interm: 0.407  loss_giou_interm: 0.5263    time: 0.4905  last_time: 0.4782  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:32:23 d2.utils.events]:  eta: 0:06:04  iter: 3259  total_loss: 58.8  loss_ce: 0.1561  loss_mask: 0.2923  loss_dice: 1.219  loss_bbox: 0.3846  loss_giou: 0.582  loss_ce_dn: 0.0001753  loss_mask_dn: 0.2968  loss_dice_dn: 1.115  loss_bbox_dn: 0.2561  loss_giou_dn: 0.4136  loss_ce_0: 0.6451  loss_mask_0: 0.2859  loss_dice_0: 1.234  loss_bbox_0: 0.8029  loss_giou_0: 1.042  loss_ce_dn_0: 0.01849  loss_mask_dn_0: 0.4562  loss_dice_dn_0: 2.189  loss_bbox_dn_0: 0.7323  loss_giou_dn_0: 0.8487  loss_ce_1: 0.6227  loss_mask_1: 0.3057  loss_dice_1: 1.174  loss_bbox_1: 0.365  loss_giou_1: 0.6137  loss_ce_dn_1: 0.0004447  loss_mask_dn_1: 0.3021  loss_dice_dn_1: 1.133  loss_bbox_dn_1: 0.2982  loss_giou_dn_1: 0.4534  loss_ce_2: 0.299  loss_mask_2: 0.3097  loss_dice_2: 1.249  loss_bbox_2: 0.3998  loss_giou_2: 0.5838  loss_ce_dn_2: 0.0004027  loss_mask_dn_2: 0.3008  loss_dice_dn_2: 1.135  loss_bbox_dn_2: 0.2839  loss_giou_dn_2: 0.4378  loss_ce_3: 0.2779  loss_mask_3: 0.3033  loss_dice_3: 1.179  loss_bbox_3: 0.3913  loss_giou_3: 0.5949  loss_ce_dn_3: 0.0003416  loss_mask_dn_3: 0.2937  loss_dice_dn_3: 1.118  loss_bbox_dn_3: 0.248  loss_giou_dn_3: 0.4302  loss_ce_4: 0.1685  loss_mask_4: 0.3013  loss_dice_4: 1.228  loss_bbox_4: 0.4012  loss_giou_4: 0.6423  loss_ce_dn_4: 0.0003402  loss_mask_dn_4: 0.2879  loss_dice_dn_4: 1.119  loss_bbox_dn_4: 0.234  loss_giou_dn_4: 0.4245  loss_ce_5: 0.1446  loss_mask_5: 0.3014  loss_dice_5: 1.193  loss_bbox_5: 0.4004  loss_giou_5: 0.6087  loss_ce_dn_5: 0.0005156  loss_mask_dn_5: 0.291  loss_dice_dn_5: 1.109  loss_bbox_dn_5: 0.249  loss_giou_dn_5: 0.4138  loss_ce_6: 0.1148  loss_mask_6: 0.2959  loss_dice_6: 1.178  loss_bbox_6: 0.3662  loss_giou_6: 0.5968  loss_ce_dn_6: 0.0003735  loss_mask_dn_6: 0.3011  loss_dice_dn_6: 1.114  loss_bbox_dn_6: 0.2501  loss_giou_dn_6: 0.4216  loss_ce_7: 0.1209  loss_mask_7: 0.2868  loss_dice_7: 1.153  loss_bbox_7: 0.3678  loss_giou_7: 0.5881  loss_ce_dn_7: 0.0002096  loss_mask_dn_7: 0.2969  loss_dice_dn_7: 1.12  loss_bbox_dn_7: 0.2547  loss_giou_dn_7: 0.4184  loss_ce_8: 0.137  loss_mask_8: 0.294  loss_dice_8: 1.201  loss_bbox_8: 0.3849  loss_giou_8: 0.5797  loss_ce_dn_8: 0.0001608  loss_mask_dn_8: 0.297  loss_dice_dn_8: 1.121  loss_bbox_dn_8: 0.2585  loss_giou_dn_8: 0.4129  loss_ce_interm: 0.6885  loss_mask_interm: 0.2968  loss_dice_interm: 1.274  loss_bbox_interm: 0.409  loss_giou_interm: 0.6079    time: 0.4907  last_time: 0.5563  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:32:37 d2.utils.events]:  eta: 0:05:55  iter: 3279  total_loss: 62.93  loss_ce: 0.2524  loss_mask: 0.3306  loss_dice: 1.3  loss_bbox: 0.2885  loss_giou: 0.6452  loss_ce_dn: 0.0001573  loss_mask_dn: 0.3414  loss_dice_dn: 1.205  loss_bbox_dn: 0.25  loss_giou_dn: 0.435  loss_ce_0: 0.6943  loss_mask_0: 0.375  loss_dice_0: 1.251  loss_bbox_0: 0.6998  loss_giou_0: 1.063  loss_ce_dn_0: 0.01846  loss_mask_dn_0: 0.5606  loss_dice_dn_0: 2.771  loss_bbox_dn_0: 0.797  loss_giou_dn_0: 0.8503  loss_ce_1: 0.5658  loss_mask_1: 0.3596  loss_dice_1: 1.282  loss_bbox_1: 0.3682  loss_giou_1: 0.6625  loss_ce_dn_1: 0.0005764  loss_mask_dn_1: 0.3575  loss_dice_dn_1: 1.234  loss_bbox_dn_1: 0.3087  loss_giou_dn_1: 0.4786  loss_ce_2: 0.291  loss_mask_2: 0.3556  loss_dice_2: 1.301  loss_bbox_2: 0.3339  loss_giou_2: 0.6276  loss_ce_dn_2: 0.0003805  loss_mask_dn_2: 0.3595  loss_dice_dn_2: 1.233  loss_bbox_dn_2: 0.2774  loss_giou_dn_2: 0.4585  loss_ce_3: 0.2158  loss_mask_3: 0.3389  loss_dice_3: 1.249  loss_bbox_3: 0.3016  loss_giou_3: 0.6499  loss_ce_dn_3: 0.0003049  loss_mask_dn_3: 0.3591  loss_dice_dn_3: 1.217  loss_bbox_dn_3: 0.2623  loss_giou_dn_3: 0.4396  loss_ce_4: 0.2103  loss_mask_4: 0.3406  loss_dice_4: 1.254  loss_bbox_4: 0.2939  loss_giou_4: 0.6393  loss_ce_dn_4: 0.0002172  loss_mask_dn_4: 0.3429  loss_dice_dn_4: 1.211  loss_bbox_dn_4: 0.2638  loss_giou_dn_4: 0.4365  loss_ce_5: 0.2103  loss_mask_5: 0.3348  loss_dice_5: 1.24  loss_bbox_5: 0.31  loss_giou_5: 0.6375  loss_ce_dn_5: 0.0002983  loss_mask_dn_5: 0.3478  loss_dice_dn_5: 1.196  loss_bbox_dn_5: 0.2582  loss_giou_dn_5: 0.4289  loss_ce_6: 0.2128  loss_mask_6: 0.3535  loss_dice_6: 1.259  loss_bbox_6: 0.3002  loss_giou_6: 0.6444  loss_ce_dn_6: 0.0003347  loss_mask_dn_6: 0.3444  loss_dice_dn_6: 1.197  loss_bbox_dn_6: 0.2569  loss_giou_dn_6: 0.432  loss_ce_7: 0.2121  loss_mask_7: 0.3364  loss_dice_7: 1.261  loss_bbox_7: 0.2943  loss_giou_7: 0.6378  loss_ce_dn_7: 0.0001802  loss_mask_dn_7: 0.342  loss_dice_dn_7: 1.196  loss_bbox_dn_7: 0.2554  loss_giou_dn_7: 0.4351  loss_ce_8: 0.2343  loss_mask_8: 0.3336  loss_dice_8: 1.255  loss_bbox_8: 0.298  loss_giou_8: 0.6401  loss_ce_dn_8: 0.0001765  loss_mask_dn_8: 0.3404  loss_dice_dn_8: 1.201  loss_bbox_dn_8: 0.2543  loss_giou_dn_8: 0.4327  loss_ce_interm: 0.694  loss_mask_interm: 0.387  loss_dice_interm: 1.267  loss_bbox_interm: 0.3778  loss_giou_interm: 0.6561    time: 0.4920  last_time: 1.0494  data_time: 0.0119  last_data_time: 0.0149   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:32:49 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:32:49 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:32:49 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:32:49 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:32:50 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0757 s/iter. Eval: 0.0130 s/iter. Total: 0.0896 s/iter. ETA=0:00:05\n",
      "[03/14 17:32:55 d2.evaluation.evaluator]: Total inference time: 0:00:05.390019 (0.086936 s / iter per device, on 1 devices)\n",
      "[03/14 17:32:55 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.074892 s / iter per device, on 1 devices)\n",
      "[03/14 17:32:55 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:32:55 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:32:55 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:32:55 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:32:55 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:32:55 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:32:55 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.815\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.359\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:32:55 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 48.580 | 81.519 | 55.022 | 48.006 | 59.274 |  nan  |\n",
      "[03/14 17:32:55 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:32:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:32:56 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:32:56 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:32:56 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.737\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.240\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.364\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.339\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:32:56 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.503 | 73.668 | 5.151  | 24.004 | 36.367 |  nan  |\n",
      "[03/14 17:32:56 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:32:56 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: 48.5801,81.5185,55.0221,48.0058,59.2737,nan\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:32:56 d2.evaluation.testing]: copypaste: 26.5031,73.6684,5.1509,24.0042,36.3671,nan\n",
      "[03/14 17:32:56 d2.utils.events]:  eta: 0:05:46  iter: 3299  total_loss: 63.06  loss_ce: 0.2301  loss_mask: 0.2363  loss_dice: 1.27  loss_bbox: 0.4001  loss_giou: 0.6715  loss_ce_dn: 0.0001933  loss_mask_dn: 0.2457  loss_dice_dn: 1.304  loss_bbox_dn: 0.2244  loss_giou_dn: 0.501  loss_ce_0: 0.5961  loss_mask_0: 0.2648  loss_dice_0: 1.363  loss_bbox_0: 0.7075  loss_giou_0: 1.036  loss_ce_dn_0: 0.03037  loss_mask_dn_0: 0.5225  loss_dice_dn_0: 2.758  loss_bbox_dn_0: 0.6901  loss_giou_dn_0: 0.8548  loss_ce_1: 0.6232  loss_mask_1: 0.2601  loss_dice_1: 1.317  loss_bbox_1: 0.367  loss_giou_1: 0.5973  loss_ce_dn_1: 0.0004132  loss_mask_dn_1: 0.2547  loss_dice_dn_1: 1.35  loss_bbox_dn_1: 0.2959  loss_giou_dn_1: 0.5404  loss_ce_2: 0.6166  loss_mask_2: 0.2355  loss_dice_2: 1.375  loss_bbox_2: 0.3378  loss_giou_2: 0.6374  loss_ce_dn_2: 0.0004087  loss_mask_dn_2: 0.2583  loss_dice_dn_2: 1.31  loss_bbox_dn_2: 0.2875  loss_giou_dn_2: 0.5094  loss_ce_3: 0.4274  loss_mask_3: 0.2306  loss_dice_3: 1.358  loss_bbox_3: 0.4221  loss_giou_3: 0.6646  loss_ce_dn_3: 0.0003891  loss_mask_dn_3: 0.2477  loss_dice_dn_3: 1.343  loss_bbox_dn_3: 0.2602  loss_giou_dn_3: 0.518  loss_ce_4: 0.2324  loss_mask_4: 0.2247  loss_dice_4: 1.303  loss_bbox_4: 0.3851  loss_giou_4: 0.6887  loss_ce_dn_4: 0.000244  loss_mask_dn_4: 0.2441  loss_dice_dn_4: 1.3  loss_bbox_dn_4: 0.236  loss_giou_dn_4: 0.5082  loss_ce_5: 0.2995  loss_mask_5: 0.231  loss_dice_5: 1.279  loss_bbox_5: 0.3741  loss_giou_5: 0.6824  loss_ce_dn_5: 0.0004178  loss_mask_dn_5: 0.2473  loss_dice_dn_5: 1.293  loss_bbox_dn_5: 0.2426  loss_giou_dn_5: 0.5046  loss_ce_6: 0.2719  loss_mask_6: 0.2263  loss_dice_6: 1.153  loss_bbox_6: 0.3806  loss_giou_6: 0.6923  loss_ce_dn_6: 0.0003954  loss_mask_dn_6: 0.2446  loss_dice_dn_6: 1.309  loss_bbox_dn_6: 0.229  loss_giou_dn_6: 0.4956  loss_ce_7: 0.2178  loss_mask_7: 0.223  loss_dice_7: 1.208  loss_bbox_7: 0.3954  loss_giou_7: 0.6767  loss_ce_dn_7: 0.0002174  loss_mask_dn_7: 0.248  loss_dice_dn_7: 1.298  loss_bbox_dn_7: 0.2327  loss_giou_dn_7: 0.4992  loss_ce_8: 0.1848  loss_mask_8: 0.226  loss_dice_8: 1.264  loss_bbox_8: 0.4069  loss_giou_8: 0.6731  loss_ce_dn_8: 0.0001956  loss_mask_dn_8: 0.2451  loss_dice_dn_8: 1.3  loss_bbox_dn_8: 0.2333  loss_giou_dn_8: 0.5004  loss_ce_interm: 0.6369  loss_mask_interm: 0.2594  loss_dice_interm: 1.336  loss_bbox_interm: 0.3555  loss_giou_interm: 0.7146    time: 0.4924  last_time: 0.4728  data_time: 0.0048  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:33:05 d2.utils.events]:  eta: 0:05:35  iter: 3319  total_loss: 56.11  loss_ce: 0.1633  loss_mask: 0.3046  loss_dice: 1.354  loss_bbox: 0.2704  loss_giou: 0.4836  loss_ce_dn: 0.000126  loss_mask_dn: 0.3052  loss_dice_dn: 1.301  loss_bbox_dn: 0.2373  loss_giou_dn: 0.4399  loss_ce_0: 0.4627  loss_mask_0: 0.3251  loss_dice_0: 1.402  loss_bbox_0: 0.7404  loss_giou_0: 0.9407  loss_ce_dn_0: 0.05821  loss_mask_dn_0: 0.4489  loss_dice_dn_0: 2.759  loss_bbox_dn_0: 0.7587  loss_giou_dn_0: 0.8532  loss_ce_1: 0.4957  loss_mask_1: 0.3112  loss_dice_1: 1.387  loss_bbox_1: 0.3197  loss_giou_1: 0.5527  loss_ce_dn_1: 0.0004523  loss_mask_dn_1: 0.3128  loss_dice_dn_1: 1.305  loss_bbox_dn_1: 0.3005  loss_giou_dn_1: 0.469  loss_ce_2: 0.3286  loss_mask_2: 0.3157  loss_dice_2: 1.348  loss_bbox_2: 0.3024  loss_giou_2: 0.518  loss_ce_dn_2: 0.0003404  loss_mask_dn_2: 0.3038  loss_dice_dn_2: 1.309  loss_bbox_dn_2: 0.2479  loss_giou_dn_2: 0.4462  loss_ce_3: 0.2995  loss_mask_3: 0.3025  loss_dice_3: 1.312  loss_bbox_3: 0.292  loss_giou_3: 0.4691  loss_ce_dn_3: 0.0003494  loss_mask_dn_3: 0.2983  loss_dice_dn_3: 1.295  loss_bbox_dn_3: 0.23  loss_giou_dn_3: 0.4428  loss_ce_4: 0.2536  loss_mask_4: 0.3154  loss_dice_4: 1.359  loss_bbox_4: 0.2928  loss_giou_4: 0.4758  loss_ce_dn_4: 0.0002113  loss_mask_dn_4: 0.3014  loss_dice_dn_4: 1.299  loss_bbox_dn_4: 0.2505  loss_giou_dn_4: 0.4429  loss_ce_5: 0.1731  loss_mask_5: 0.3083  loss_dice_5: 1.313  loss_bbox_5: 0.288  loss_giou_5: 0.4842  loss_ce_dn_5: 0.0003009  loss_mask_dn_5: 0.3054  loss_dice_dn_5: 1.299  loss_bbox_dn_5: 0.2395  loss_giou_dn_5: 0.4376  loss_ce_6: 0.1747  loss_mask_6: 0.3076  loss_dice_6: 1.261  loss_bbox_6: 0.2629  loss_giou_6: 0.4821  loss_ce_dn_6: 0.0002541  loss_mask_dn_6: 0.3069  loss_dice_dn_6: 1.306  loss_bbox_dn_6: 0.2335  loss_giou_dn_6: 0.4407  loss_ce_7: 0.1546  loss_mask_7: 0.2995  loss_dice_7: 1.244  loss_bbox_7: 0.2654  loss_giou_7: 0.4932  loss_ce_dn_7: 0.0001884  loss_mask_dn_7: 0.3031  loss_dice_dn_7: 1.309  loss_bbox_dn_7: 0.2368  loss_giou_dn_7: 0.4403  loss_ce_8: 0.1591  loss_mask_8: 0.2951  loss_dice_8: 1.365  loss_bbox_8: 0.2724  loss_giou_8: 0.4889  loss_ce_dn_8: 0.0001462  loss_mask_dn_8: 0.3036  loss_dice_dn_8: 1.308  loss_bbox_dn_8: 0.2343  loss_giou_dn_8: 0.4408  loss_ce_interm: 0.4908  loss_mask_interm: 0.3164  loss_dice_interm: 1.415  loss_bbox_interm: 0.3852  loss_giou_interm: 0.5745    time: 0.4922  last_time: 0.4252  data_time: 0.0034  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:33:15 d2.utils.events]:  eta: 0:05:26  iter: 3339  total_loss: 56.24  loss_ce: 0.188  loss_mask: 0.3289  loss_dice: 1.082  loss_bbox: 0.2942  loss_giou: 0.4674  loss_ce_dn: 7.413e-05  loss_mask_dn: 0.3463  loss_dice_dn: 1.068  loss_bbox_dn: 0.2558  loss_giou_dn: 0.4068  loss_ce_0: 0.6768  loss_mask_0: 0.3624  loss_dice_0: 1.095  loss_bbox_0: 0.6945  loss_giou_0: 0.8007  loss_ce_dn_0: 0.01827  loss_mask_dn_0: 0.4242  loss_dice_dn_0: 2.081  loss_bbox_dn_0: 0.9415  loss_giou_dn_0: 0.8514  loss_ce_1: 0.6671  loss_mask_1: 0.3446  loss_dice_1: 1.089  loss_bbox_1: 0.3508  loss_giou_1: 0.5892  loss_ce_dn_1: 0.0003645  loss_mask_dn_1: 0.3438  loss_dice_dn_1: 1.084  loss_bbox_dn_1: 0.3799  loss_giou_dn_1: 0.47  loss_ce_2: 0.4181  loss_mask_2: 0.3291  loss_dice_2: 1.094  loss_bbox_2: 0.3444  loss_giou_2: 0.57  loss_ce_dn_2: 0.0004084  loss_mask_dn_2: 0.3366  loss_dice_dn_2: 1.079  loss_bbox_dn_2: 0.283  loss_giou_dn_2: 0.4451  loss_ce_3: 0.2968  loss_mask_3: 0.3385  loss_dice_3: 1.102  loss_bbox_3: 0.3118  loss_giou_3: 0.5351  loss_ce_dn_3: 0.0003223  loss_mask_dn_3: 0.338  loss_dice_dn_3: 1.068  loss_bbox_dn_3: 0.2478  loss_giou_dn_3: 0.4434  loss_ce_4: 0.2771  loss_mask_4: 0.3283  loss_dice_4: 1.094  loss_bbox_4: 0.31  loss_giou_4: 0.5009  loss_ce_dn_4: 0.0001493  loss_mask_dn_4: 0.3406  loss_dice_dn_4: 1.059  loss_bbox_dn_4: 0.2607  loss_giou_dn_4: 0.4305  loss_ce_5: 0.2166  loss_mask_5: 0.3313  loss_dice_5: 1.064  loss_bbox_5: 0.2901  loss_giou_5: 0.48  loss_ce_dn_5: 0.0002252  loss_mask_dn_5: 0.3389  loss_dice_dn_5: 1.058  loss_bbox_dn_5: 0.2611  loss_giou_dn_5: 0.4161  loss_ce_6: 0.2331  loss_mask_6: 0.3429  loss_dice_6: 1.021  loss_bbox_6: 0.3118  loss_giou_6: 0.4849  loss_ce_dn_6: 0.0002601  loss_mask_dn_6: 0.3431  loss_dice_dn_6: 1.063  loss_bbox_dn_6: 0.2591  loss_giou_dn_6: 0.4102  loss_ce_7: 0.212  loss_mask_7: 0.3373  loss_dice_7: 1.103  loss_bbox_7: 0.3009  loss_giou_7: 0.4806  loss_ce_dn_7: 0.000114  loss_mask_dn_7: 0.3425  loss_dice_dn_7: 1.065  loss_bbox_dn_7: 0.2553  loss_giou_dn_7: 0.4047  loss_ce_8: 0.181  loss_mask_8: 0.3264  loss_dice_8: 1.051  loss_bbox_8: 0.296  loss_giou_8: 0.4679  loss_ce_dn_8: 8.478e-05  loss_mask_dn_8: 0.3461  loss_dice_dn_8: 1.066  loss_bbox_dn_8: 0.2606  loss_giou_dn_8: 0.4085  loss_ce_interm: 0.718  loss_mask_interm: 0.3667  loss_dice_interm: 1.099  loss_bbox_interm: 0.4152  loss_giou_interm: 0.6019    time: 0.4921  last_time: 0.4358  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:33:24 d2.utils.events]:  eta: 0:05:16  iter: 3359  total_loss: 65.42  loss_ce: 0.5583  loss_mask: 0.3197  loss_dice: 1.326  loss_bbox: 0.3755  loss_giou: 0.6409  loss_ce_dn: 0.0001647  loss_mask_dn: 0.3104  loss_dice_dn: 1.218  loss_bbox_dn: 0.2589  loss_giou_dn: 0.4749  loss_ce_0: 0.7  loss_mask_0: 0.3015  loss_dice_0: 1.467  loss_bbox_0: 0.6968  loss_giou_0: 1.144  loss_ce_dn_0: 0.01821  loss_mask_dn_0: 0.3963  loss_dice_dn_0: 2.579  loss_bbox_dn_0: 0.6556  loss_giou_dn_0: 0.8498  loss_ce_1: 0.8473  loss_mask_1: 0.333  loss_dice_1: 1.445  loss_bbox_1: 0.3938  loss_giou_1: 0.6922  loss_ce_dn_1: 0.0004518  loss_mask_dn_1: 0.3214  loss_dice_dn_1: 1.331  loss_bbox_dn_1: 0.3139  loss_giou_dn_1: 0.5649  loss_ce_2: 0.7961  loss_mask_2: 0.2988  loss_dice_2: 1.468  loss_bbox_2: 0.3431  loss_giou_2: 0.6354  loss_ce_dn_2: 0.0003625  loss_mask_dn_2: 0.3284  loss_dice_dn_2: 1.284  loss_bbox_dn_2: 0.3239  loss_giou_dn_2: 0.5265  loss_ce_3: 0.6098  loss_mask_3: 0.3073  loss_dice_3: 1.48  loss_bbox_3: 0.4129  loss_giou_3: 0.6562  loss_ce_dn_3: 0.000355  loss_mask_dn_3: 0.3187  loss_dice_dn_3: 1.285  loss_bbox_dn_3: 0.2984  loss_giou_dn_3: 0.5131  loss_ce_4: 0.6602  loss_mask_4: 0.306  loss_dice_4: 1.379  loss_bbox_4: 0.3469  loss_giou_4: 0.6362  loss_ce_dn_4: 0.0002869  loss_mask_dn_4: 0.3176  loss_dice_dn_4: 1.26  loss_bbox_dn_4: 0.2713  loss_giou_dn_4: 0.4755  loss_ce_5: 0.5761  loss_mask_5: 0.3528  loss_dice_5: 1.279  loss_bbox_5: 0.3653  loss_giou_5: 0.6813  loss_ce_dn_5: 0.0004445  loss_mask_dn_5: 0.3066  loss_dice_dn_5: 1.233  loss_bbox_dn_5: 0.2696  loss_giou_dn_5: 0.4834  loss_ce_6: 0.6768  loss_mask_6: 0.323  loss_dice_6: 1.37  loss_bbox_6: 0.3549  loss_giou_6: 0.6322  loss_ce_dn_6: 0.0004723  loss_mask_dn_6: 0.3078  loss_dice_dn_6: 1.245  loss_bbox_dn_6: 0.259  loss_giou_dn_6: 0.4791  loss_ce_7: 0.5953  loss_mask_7: 0.3448  loss_dice_7: 1.418  loss_bbox_7: 0.3441  loss_giou_7: 0.6456  loss_ce_dn_7: 0.0002743  loss_mask_dn_7: 0.3064  loss_dice_dn_7: 1.23  loss_bbox_dn_7: 0.2583  loss_giou_dn_7: 0.475  loss_ce_8: 0.5826  loss_mask_8: 0.3229  loss_dice_8: 1.362  loss_bbox_8: 0.3516  loss_giou_8: 0.6396  loss_ce_dn_8: 0.0002202  loss_mask_dn_8: 0.3076  loss_dice_dn_8: 1.223  loss_bbox_dn_8: 0.258  loss_giou_dn_8: 0.4737  loss_ce_interm: 0.687  loss_mask_interm: 0.3019  loss_dice_interm: 1.409  loss_bbox_interm: 0.3958  loss_giou_interm: 0.7976    time: 0.4921  last_time: 0.4810  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:33:34 d2.utils.events]:  eta: 0:05:06  iter: 3379  total_loss: 59.56  loss_ce: 0.204  loss_mask: 0.3196  loss_dice: 1.102  loss_bbox: 0.3134  loss_giou: 0.4587  loss_ce_dn: 9.776e-05  loss_mask_dn: 0.3028  loss_dice_dn: 1.113  loss_bbox_dn: 0.219  loss_giou_dn: 0.383  loss_ce_0: 0.6766  loss_mask_0: 0.3268  loss_dice_0: 1.14  loss_bbox_0: 0.668  loss_giou_0: 0.9131  loss_ce_dn_0: 0.05817  loss_mask_dn_0: 0.6424  loss_dice_dn_0: 2.581  loss_bbox_dn_0: 0.8206  loss_giou_dn_0: 0.8449  loss_ce_1: 0.6507  loss_mask_1: 0.3108  loss_dice_1: 1.119  loss_bbox_1: 0.3837  loss_giou_1: 0.5031  loss_ce_dn_1: 0.0007645  loss_mask_dn_1: 0.3104  loss_dice_dn_1: 1.15  loss_bbox_dn_1: 0.3034  loss_giou_dn_1: 0.453  loss_ce_2: 0.4454  loss_mask_2: 0.3334  loss_dice_2: 1.112  loss_bbox_2: 0.3436  loss_giou_2: 0.5054  loss_ce_dn_2: 0.0004601  loss_mask_dn_2: 0.3097  loss_dice_dn_2: 1.121  loss_bbox_dn_2: 0.2621  loss_giou_dn_2: 0.4017  loss_ce_3: 0.3175  loss_mask_3: 0.3322  loss_dice_3: 1.08  loss_bbox_3: 0.3265  loss_giou_3: 0.4859  loss_ce_dn_3: 0.0003918  loss_mask_dn_3: 0.3146  loss_dice_dn_3: 1.138  loss_bbox_dn_3: 0.2453  loss_giou_dn_3: 0.3927  loss_ce_4: 0.2485  loss_mask_4: 0.3273  loss_dice_4: 1.115  loss_bbox_4: 0.3184  loss_giou_4: 0.4694  loss_ce_dn_4: 0.0001879  loss_mask_dn_4: 0.3045  loss_dice_dn_4: 1.12  loss_bbox_dn_4: 0.2294  loss_giou_dn_4: 0.3903  loss_ce_5: 0.2274  loss_mask_5: 0.3285  loss_dice_5: 1.084  loss_bbox_5: 0.3077  loss_giou_5: 0.4409  loss_ce_dn_5: 0.0003284  loss_mask_dn_5: 0.3036  loss_dice_dn_5: 1.121  loss_bbox_dn_5: 0.2287  loss_giou_dn_5: 0.3783  loss_ce_6: 0.2069  loss_mask_6: 0.3563  loss_dice_6: 1.081  loss_bbox_6: 0.3276  loss_giou_6: 0.4356  loss_ce_dn_6: 0.0002695  loss_mask_dn_6: 0.307  loss_dice_dn_6: 1.118  loss_bbox_dn_6: 0.2224  loss_giou_dn_6: 0.3846  loss_ce_7: 0.1864  loss_mask_7: 0.31  loss_dice_7: 1.089  loss_bbox_7: 0.3009  loss_giou_7: 0.4484  loss_ce_dn_7: 0.0001411  loss_mask_dn_7: 0.2989  loss_dice_dn_7: 1.111  loss_bbox_dn_7: 0.2161  loss_giou_dn_7: 0.3791  loss_ce_8: 0.198  loss_mask_8: 0.3337  loss_dice_8: 1.154  loss_bbox_8: 0.3216  loss_giou_8: 0.4511  loss_ce_dn_8: 0.0001033  loss_mask_dn_8: 0.3021  loss_dice_dn_8: 1.115  loss_bbox_dn_8: 0.2215  loss_giou_dn_8: 0.3835  loss_ce_interm: 0.7252  loss_mask_interm: 0.3335  loss_dice_interm: 1.145  loss_bbox_interm: 0.3766  loss_giou_interm: 0.5475    time: 0.4920  last_time: 0.4580  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:33:44 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:33:44 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:33:44 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:33:44 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:33:45 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0835 s/iter. Eval: 0.0090 s/iter. Total: 0.0933 s/iter. ETA=0:00:05\n",
      "[03/14 17:33:50 d2.evaluation.evaluator]: Total inference time: 0:00:05.421759 (0.087448 s / iter per device, on 1 devices)\n",
      "[03/14 17:33:50 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075853 s / iter per device, on 1 devices)\n",
      "[03/14 17:33:50 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:33:50 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:33:50 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:33:50 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:33:50 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:33:50 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:33:50 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.828\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.530\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.497\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.630\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.580\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.597\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:33:50 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 50.534 | 82.780 | 53.015 | 49.715 | 62.956 |  nan  |\n",
      "[03/14 17:33:50 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:33:51 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:33:51 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:33:51 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:33:51 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.756\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.240\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.359\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.226\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.341\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:33:51 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.608 | 75.575 | 4.721  | 23.986 | 35.947 |  nan  |\n",
      "[03/14 17:33:51 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:33:51 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: 50.5341,82.7802,53.0148,49.7146,62.9563,nan\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:33:51 d2.evaluation.testing]: copypaste: 26.6080,75.5747,4.7208,23.9864,35.9474,nan\n",
      "[03/14 17:33:51 d2.utils.events]:  eta: 0:04:56  iter: 3399  total_loss: 57.27  loss_ce: 0.2838  loss_mask: 0.3513  loss_dice: 1.149  loss_bbox: 0.2672  loss_giou: 0.4736  loss_ce_dn: 9.997e-05  loss_mask_dn: 0.3637  loss_dice_dn: 1.187  loss_bbox_dn: 0.2356  loss_giou_dn: 0.4054  loss_ce_0: 0.6559  loss_mask_0: 0.3383  loss_dice_0: 1.204  loss_bbox_0: 0.7567  loss_giou_0: 0.9389  loss_ce_dn_0: 0.01809  loss_mask_dn_0: 0.4562  loss_dice_dn_0: 1.905  loss_bbox_dn_0: 0.8161  loss_giou_dn_0: 0.8518  loss_ce_1: 0.7219  loss_mask_1: 0.3369  loss_dice_1: 1.144  loss_bbox_1: 0.294  loss_giou_1: 0.4901  loss_ce_dn_1: 0.0003771  loss_mask_dn_1: 0.3793  loss_dice_dn_1: 1.241  loss_bbox_dn_1: 0.2928  loss_giou_dn_1: 0.4549  loss_ce_2: 0.3475  loss_mask_2: 0.4066  loss_dice_2: 1.212  loss_bbox_2: 0.29  loss_giou_2: 0.5581  loss_ce_dn_2: 0.0003348  loss_mask_dn_2: 0.3497  loss_dice_dn_2: 1.189  loss_bbox_dn_2: 0.2588  loss_giou_dn_2: 0.417  loss_ce_3: 0.2141  loss_mask_3: 0.3641  loss_dice_3: 1.199  loss_bbox_3: 0.2806  loss_giou_3: 0.5042  loss_ce_dn_3: 0.0003003  loss_mask_dn_3: 0.3452  loss_dice_dn_3: 1.189  loss_bbox_dn_3: 0.2487  loss_giou_dn_3: 0.4077  loss_ce_4: 0.166  loss_mask_4: 0.3752  loss_dice_4: 1.176  loss_bbox_4: 0.2745  loss_giou_4: 0.554  loss_ce_dn_4: 0.0001599  loss_mask_dn_4: 0.3568  loss_dice_dn_4: 1.173  loss_bbox_dn_4: 0.2331  loss_giou_dn_4: 0.3887  loss_ce_5: 0.1719  loss_mask_5: 0.3858  loss_dice_5: 1.247  loss_bbox_5: 0.2794  loss_giou_5: 0.5437  loss_ce_dn_5: 0.0002742  loss_mask_dn_5: 0.3558  loss_dice_dn_5: 1.204  loss_bbox_dn_5: 0.2346  loss_giou_dn_5: 0.4082  loss_ce_6: 0.06953  loss_mask_6: 0.4037  loss_dice_6: 1.258  loss_bbox_6: 0.2773  loss_giou_6: 0.5417  loss_ce_dn_6: 0.0002534  loss_mask_dn_6: 0.3645  loss_dice_dn_6: 1.205  loss_bbox_dn_6: 0.2327  loss_giou_dn_6: 0.4057  loss_ce_7: 0.2767  loss_mask_7: 0.3827  loss_dice_7: 1.192  loss_bbox_7: 0.2707  loss_giou_7: 0.5121  loss_ce_dn_7: 0.0001642  loss_mask_dn_7: 0.3578  loss_dice_dn_7: 1.203  loss_bbox_dn_7: 0.237  loss_giou_dn_7: 0.4062  loss_ce_8: 0.2763  loss_mask_8: 0.35  loss_dice_8: 1.142  loss_bbox_8: 0.2716  loss_giou_8: 0.4695  loss_ce_dn_8: 0.0001157  loss_mask_dn_8: 0.3672  loss_dice_dn_8: 1.194  loss_bbox_dn_8: 0.2369  loss_giou_dn_8: 0.403  loss_ce_interm: 0.5553  loss_mask_interm: 0.3551  loss_dice_interm: 1.171  loss_bbox_interm: 0.3455  loss_giou_interm: 0.6119    time: 0.4920  last_time: 0.4916  data_time: 0.0035  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:00 d2.utils.events]:  eta: 0:04:46  iter: 3419  total_loss: 57.89  loss_ce: 0.2852  loss_mask: 0.2709  loss_dice: 1.123  loss_bbox: 0.3257  loss_giou: 0.5061  loss_ce_dn: 0.0001168  loss_mask_dn: 0.2527  loss_dice_dn: 1.077  loss_bbox_dn: 0.2402  loss_giou_dn: 0.4136  loss_ce_0: 0.6266  loss_mask_0: 0.2714  loss_dice_0: 1.173  loss_bbox_0: 0.7696  loss_giou_0: 1.055  loss_ce_dn_0: 0.01813  loss_mask_dn_0: 0.4674  loss_dice_dn_0: 2.596  loss_bbox_dn_0: 0.7534  loss_giou_dn_0: 0.855  loss_ce_1: 0.6125  loss_mask_1: 0.2848  loss_dice_1: 1.198  loss_bbox_1: 0.3682  loss_giou_1: 0.5313  loss_ce_dn_1: 0.0004587  loss_mask_dn_1: 0.2671  loss_dice_dn_1: 1.123  loss_bbox_dn_1: 0.2767  loss_giou_dn_1: 0.468  loss_ce_2: 0.4642  loss_mask_2: 0.3038  loss_dice_2: 1.139  loss_bbox_2: 0.352  loss_giou_2: 0.5314  loss_ce_dn_2: 0.0004831  loss_mask_dn_2: 0.2593  loss_dice_dn_2: 1.116  loss_bbox_dn_2: 0.2672  loss_giou_dn_2: 0.4447  loss_ce_3: 0.5127  loss_mask_3: 0.2962  loss_dice_3: 1.091  loss_bbox_3: 0.3429  loss_giou_3: 0.5132  loss_ce_dn_3: 0.0003907  loss_mask_dn_3: 0.2556  loss_dice_dn_3: 1.114  loss_bbox_dn_3: 0.2458  loss_giou_dn_3: 0.4436  loss_ce_4: 0.3958  loss_mask_4: 0.267  loss_dice_4: 1.108  loss_bbox_4: 0.3327  loss_giou_4: 0.5283  loss_ce_dn_4: 0.0001978  loss_mask_dn_4: 0.2505  loss_dice_dn_4: 1.108  loss_bbox_dn_4: 0.2481  loss_giou_dn_4: 0.4369  loss_ce_5: 0.2512  loss_mask_5: 0.2761  loss_dice_5: 1.079  loss_bbox_5: 0.3318  loss_giou_5: 0.499  loss_ce_dn_5: 0.0002403  loss_mask_dn_5: 0.2557  loss_dice_dn_5: 1.097  loss_bbox_dn_5: 0.2455  loss_giou_dn_5: 0.4109  loss_ce_6: 0.2494  loss_mask_6: 0.289  loss_dice_6: 1.084  loss_bbox_6: 0.336  loss_giou_6: 0.5152  loss_ce_dn_6: 0.0002756  loss_mask_dn_6: 0.2475  loss_dice_dn_6: 1.076  loss_bbox_dn_6: 0.2457  loss_giou_dn_6: 0.4089  loss_ce_7: 0.2744  loss_mask_7: 0.2892  loss_dice_7: 1.151  loss_bbox_7: 0.345  loss_giou_7: 0.5078  loss_ce_dn_7: 0.0001348  loss_mask_dn_7: 0.2532  loss_dice_dn_7: 1.094  loss_bbox_dn_7: 0.244  loss_giou_dn_7: 0.4293  loss_ce_8: 0.289  loss_mask_8: 0.2776  loss_dice_8: 1.15  loss_bbox_8: 0.3345  loss_giou_8: 0.5091  loss_ce_dn_8: 0.0001225  loss_mask_dn_8: 0.2506  loss_dice_dn_8: 1.089  loss_bbox_dn_8: 0.2367  loss_giou_dn_8: 0.4183  loss_ce_interm: 0.6423  loss_mask_interm: 0.2792  loss_dice_interm: 1.151  loss_bbox_interm: 0.3822  loss_giou_interm: 0.6103    time: 0.4918  last_time: 0.4546  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:10 d2.utils.events]:  eta: 0:04:36  iter: 3439  total_loss: 55.64  loss_ce: 0.1996  loss_mask: 0.3122  loss_dice: 1.141  loss_bbox: 0.2662  loss_giou: 0.4532  loss_ce_dn: 0.0001598  loss_mask_dn: 0.2763  loss_dice_dn: 1.118  loss_bbox_dn: 0.1985  loss_giou_dn: 0.3537  loss_ce_0: 0.6587  loss_mask_0: 0.2991  loss_dice_0: 1.215  loss_bbox_0: 0.6296  loss_giou_0: 0.93  loss_ce_dn_0: 0.05776  loss_mask_dn_0: 0.4717  loss_dice_dn_0: 2.521  loss_bbox_dn_0: 0.727  loss_giou_dn_0: 0.8492  loss_ce_1: 0.6762  loss_mask_1: 0.3167  loss_dice_1: 1.186  loss_bbox_1: 0.307  loss_giou_1: 0.5789  loss_ce_dn_1: 0.000573  loss_mask_dn_1: 0.2774  loss_dice_dn_1: 1.163  loss_bbox_dn_1: 0.2768  loss_giou_dn_1: 0.4445  loss_ce_2: 0.3831  loss_mask_2: 0.3138  loss_dice_2: 1.216  loss_bbox_2: 0.314  loss_giou_2: 0.544  loss_ce_dn_2: 0.0004658  loss_mask_dn_2: 0.2754  loss_dice_dn_2: 1.135  loss_bbox_dn_2: 0.2294  loss_giou_dn_2: 0.3836  loss_ce_3: 0.1931  loss_mask_3: 0.3059  loss_dice_3: 1.201  loss_bbox_3: 0.2861  loss_giou_3: 0.5401  loss_ce_dn_3: 0.000449  loss_mask_dn_3: 0.2686  loss_dice_dn_3: 1.116  loss_bbox_dn_3: 0.2183  loss_giou_dn_3: 0.3562  loss_ce_4: 0.1848  loss_mask_4: 0.3114  loss_dice_4: 1.204  loss_bbox_4: 0.2527  loss_giou_4: 0.5019  loss_ce_dn_4: 0.000263  loss_mask_dn_4: 0.2771  loss_dice_dn_4: 1.112  loss_bbox_dn_4: 0.1989  loss_giou_dn_4: 0.3432  loss_ce_5: 0.1876  loss_mask_5: 0.3187  loss_dice_5: 1.161  loss_bbox_5: 0.2552  loss_giou_5: 0.5108  loss_ce_dn_5: 0.0003535  loss_mask_dn_5: 0.2777  loss_dice_dn_5: 1.13  loss_bbox_dn_5: 0.2002  loss_giou_dn_5: 0.3548  loss_ce_6: 0.2062  loss_mask_6: 0.3197  loss_dice_6: 1.167  loss_bbox_6: 0.2433  loss_giou_6: 0.4538  loss_ce_dn_6: 0.0003631  loss_mask_dn_6: 0.2776  loss_dice_dn_6: 1.118  loss_bbox_dn_6: 0.1946  loss_giou_dn_6: 0.3472  loss_ce_7: 0.2003  loss_mask_7: 0.3173  loss_dice_7: 1.188  loss_bbox_7: 0.2422  loss_giou_7: 0.4567  loss_ce_dn_7: 0.0001892  loss_mask_dn_7: 0.2765  loss_dice_dn_7: 1.121  loss_bbox_dn_7: 0.1941  loss_giou_dn_7: 0.3479  loss_ce_8: 0.1984  loss_mask_8: 0.2993  loss_dice_8: 1.195  loss_bbox_8: 0.2677  loss_giou_8: 0.4611  loss_ce_dn_8: 0.0001435  loss_mask_dn_8: 0.2755  loss_dice_dn_8: 1.12  loss_bbox_dn_8: 0.1965  loss_giou_dn_8: 0.3516  loss_ce_interm: 0.7307  loss_mask_interm: 0.3177  loss_dice_interm: 1.185  loss_bbox_interm: 0.3066  loss_giou_interm: 0.5511    time: 0.4917  last_time: 0.5022  data_time: 0.0035  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:19 d2.utils.events]:  eta: 0:04:25  iter: 3459  total_loss: 62.02  loss_ce: 0.1779  loss_mask: 0.5478  loss_dice: 1.262  loss_bbox: 0.3817  loss_giou: 0.5046  loss_ce_dn: 0.0001192  loss_mask_dn: 0.512  loss_dice_dn: 1.244  loss_bbox_dn: 0.2736  loss_giou_dn: 0.4206  loss_ce_0: 0.5654  loss_mask_0: 0.5536  loss_dice_0: 1.335  loss_bbox_0: 0.7898  loss_giou_0: 0.8648  loss_ce_dn_0: 0.05776  loss_mask_dn_0: 0.8551  loss_dice_dn_0: 2.205  loss_bbox_dn_0: 0.856  loss_giou_dn_0: 0.8442  loss_ce_1: 0.5055  loss_mask_1: 0.5894  loss_dice_1: 1.331  loss_bbox_1: 0.4218  loss_giou_1: 0.5449  loss_ce_dn_1: 0.0008465  loss_mask_dn_1: 0.5206  loss_dice_dn_1: 1.242  loss_bbox_dn_1: 0.3899  loss_giou_dn_1: 0.4934  loss_ce_2: 0.4733  loss_mask_2: 0.5527  loss_dice_2: 1.311  loss_bbox_2: 0.415  loss_giou_2: 0.5291  loss_ce_dn_2: 0.0004935  loss_mask_dn_2: 0.5127  loss_dice_dn_2: 1.259  loss_bbox_dn_2: 0.3172  loss_giou_dn_2: 0.458  loss_ce_3: 0.3353  loss_mask_3: 0.5746  loss_dice_3: 1.258  loss_bbox_3: 0.3814  loss_giou_3: 0.4903  loss_ce_dn_3: 0.0004263  loss_mask_dn_3: 0.4966  loss_dice_dn_3: 1.235  loss_bbox_dn_3: 0.3084  loss_giou_dn_3: 0.4522  loss_ce_4: 0.3456  loss_mask_4: 0.547  loss_dice_4: 1.305  loss_bbox_4: 0.3956  loss_giou_4: 0.497  loss_ce_dn_4: 0.0002105  loss_mask_dn_4: 0.5108  loss_dice_dn_4: 1.254  loss_bbox_dn_4: 0.2965  loss_giou_dn_4: 0.4296  loss_ce_5: 0.2585  loss_mask_5: 0.5606  loss_dice_5: 1.305  loss_bbox_5: 0.386  loss_giou_5: 0.5001  loss_ce_dn_5: 0.0002842  loss_mask_dn_5: 0.5069  loss_dice_dn_5: 1.244  loss_bbox_dn_5: 0.2946  loss_giou_dn_5: 0.436  loss_ce_6: 0.2703  loss_mask_6: 0.5326  loss_dice_6: 1.277  loss_bbox_6: 0.3796  loss_giou_6: 0.4821  loss_ce_dn_6: 0.0003352  loss_mask_dn_6: 0.5108  loss_dice_dn_6: 1.255  loss_bbox_dn_6: 0.2898  loss_giou_dn_6: 0.4312  loss_ce_7: 0.2656  loss_mask_7: 0.5449  loss_dice_7: 1.28  loss_bbox_7: 0.374  loss_giou_7: 0.4771  loss_ce_dn_7: 0.0001691  loss_mask_dn_7: 0.5115  loss_dice_dn_7: 1.252  loss_bbox_dn_7: 0.2757  loss_giou_dn_7: 0.4241  loss_ce_8: 0.1744  loss_mask_8: 0.5464  loss_dice_8: 1.23  loss_bbox_8: 0.3813  loss_giou_8: 0.497  loss_ce_dn_8: 0.0001472  loss_mask_dn_8: 0.5091  loss_dice_dn_8: 1.245  loss_bbox_dn_8: 0.2757  loss_giou_dn_8: 0.4186  loss_ce_interm: 0.5879  loss_mask_interm: 0.5494  loss_dice_interm: 1.31  loss_bbox_interm: 0.4274  loss_giou_interm: 0.6325    time: 0.4916  last_time: 0.4751  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:28 d2.utils.events]:  eta: 0:04:15  iter: 3479  total_loss: 60.5  loss_ce: 0.2439  loss_mask: 0.3604  loss_dice: 1.248  loss_bbox: 0.249  loss_giou: 0.4884  loss_ce_dn: 0.0001345  loss_mask_dn: 0.3661  loss_dice_dn: 1.242  loss_bbox_dn: 0.2179  loss_giou_dn: 0.4063  loss_ce_0: 0.6801  loss_mask_0: 0.3934  loss_dice_0: 1.343  loss_bbox_0: 0.6808  loss_giou_0: 0.9498  loss_ce_dn_0: 0.018  loss_mask_dn_0: 0.6033  loss_dice_dn_0: 2.745  loss_bbox_dn_0: 0.8304  loss_giou_dn_0: 0.8508  loss_ce_1: 0.7416  loss_mask_1: 0.3942  loss_dice_1: 1.29  loss_bbox_1: 0.3384  loss_giou_1: 0.5426  loss_ce_dn_1: 0.0005558  loss_mask_dn_1: 0.3509  loss_dice_dn_1: 1.269  loss_bbox_dn_1: 0.3189  loss_giou_dn_1: 0.482  loss_ce_2: 0.5128  loss_mask_2: 0.3389  loss_dice_2: 1.293  loss_bbox_2: 0.3075  loss_giou_2: 0.5444  loss_ce_dn_2: 0.0004577  loss_mask_dn_2: 0.362  loss_dice_dn_2: 1.259  loss_bbox_dn_2: 0.26  loss_giou_dn_2: 0.4168  loss_ce_3: 0.3919  loss_mask_3: 0.399  loss_dice_3: 1.292  loss_bbox_3: 0.275  loss_giou_3: 0.5269  loss_ce_dn_3: 0.0003994  loss_mask_dn_3: 0.3583  loss_dice_dn_3: 1.26  loss_bbox_dn_3: 0.2319  loss_giou_dn_3: 0.4093  loss_ce_4: 0.25  loss_mask_4: 0.3716  loss_dice_4: 1.239  loss_bbox_4: 0.273  loss_giou_4: 0.5153  loss_ce_dn_4: 0.0002157  loss_mask_dn_4: 0.3749  loss_dice_dn_4: 1.259  loss_bbox_dn_4: 0.2206  loss_giou_dn_4: 0.3991  loss_ce_5: 0.2361  loss_mask_5: 0.3749  loss_dice_5: 1.252  loss_bbox_5: 0.2665  loss_giou_5: 0.5085  loss_ce_dn_5: 0.0003766  loss_mask_dn_5: 0.3684  loss_dice_dn_5: 1.248  loss_bbox_dn_5: 0.2238  loss_giou_dn_5: 0.4035  loss_ce_6: 0.2374  loss_mask_6: 0.3895  loss_dice_6: 1.292  loss_bbox_6: 0.2569  loss_giou_6: 0.5284  loss_ce_dn_6: 0.000354  loss_mask_dn_6: 0.3649  loss_dice_dn_6: 1.253  loss_bbox_dn_6: 0.2191  loss_giou_dn_6: 0.4007  loss_ce_7: 0.2481  loss_mask_7: 0.3876  loss_dice_7: 1.272  loss_bbox_7: 0.2525  loss_giou_7: 0.512  loss_ce_dn_7: 0.000196  loss_mask_dn_7: 0.3613  loss_dice_dn_7: 1.248  loss_bbox_dn_7: 0.2242  loss_giou_dn_7: 0.4046  loss_ce_8: 0.2392  loss_mask_8: 0.3753  loss_dice_8: 1.308  loss_bbox_8: 0.2464  loss_giou_8: 0.4852  loss_ce_dn_8: 0.0001323  loss_mask_dn_8: 0.367  loss_dice_dn_8: 1.247  loss_bbox_dn_8: 0.2202  loss_giou_dn_8: 0.4044  loss_ce_interm: 0.6762  loss_mask_interm: 0.3926  loss_dice_interm: 1.258  loss_bbox_interm: 0.3663  loss_giou_interm: 0.5838    time: 0.4915  last_time: 0.4882  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:39 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:34:39 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:34:39 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:34:39 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:34:40 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0813 s/iter. Eval: 0.0101 s/iter. Total: 0.0923 s/iter. ETA=0:00:05\n",
      "[03/14 17:34:45 d2.evaluation.evaluator]: Total inference time: 0:00:05.538795 (0.089335 s / iter per device, on 1 devices)\n",
      "[03/14 17:34:45 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077751 s / iter per device, on 1 devices)\n",
      "[03/14 17:34:45 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:34:45 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:34:46 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.835\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.496\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.498\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.601\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.748\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:34:46 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 50.608 | 83.520 | 49.563 | 49.784 | 62.122 |  nan  |\n",
      "[03/14 17:34:46 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:34:46 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.741\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.239\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.364\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:34:46 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 26.513 | 74.140 | 6.100  | 23.932 | 36.423 |  nan  |\n",
      "[03/14 17:34:46 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:34:46 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: 50.6085,83.5203,49.5628,49.7841,62.1218,nan\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:34:46 d2.evaluation.testing]: copypaste: 26.5131,74.1398,6.1003,23.9322,36.4226,nan\n",
      "[03/14 17:34:46 d2.utils.events]:  eta: 0:04:05  iter: 3499  total_loss: 59.27  loss_ce: 0.2154  loss_mask: 0.3165  loss_dice: 1.315  loss_bbox: 0.3531  loss_giou: 0.5656  loss_ce_dn: 9.722e-05  loss_mask_dn: 0.306  loss_dice_dn: 1.17  loss_bbox_dn: 0.2656  loss_giou_dn: 0.4539  loss_ce_0: 0.7134  loss_mask_0: 0.2997  loss_dice_0: 1.268  loss_bbox_0: 0.6776  loss_giou_0: 0.9278  loss_ce_dn_0: 0.05778  loss_mask_dn_0: 0.5126  loss_dice_dn_0: 2.323  loss_bbox_dn_0: 0.8248  loss_giou_dn_0: 0.8492  loss_ce_1: 0.685  loss_mask_1: 0.3039  loss_dice_1: 1.235  loss_bbox_1: 0.3766  loss_giou_1: 0.5365  loss_ce_dn_1: 0.0005586  loss_mask_dn_1: 0.3034  loss_dice_dn_1: 1.185  loss_bbox_dn_1: 0.2961  loss_giou_dn_1: 0.4835  loss_ce_2: 0.4251  loss_mask_2: 0.2972  loss_dice_2: 1.216  loss_bbox_2: 0.3514  loss_giou_2: 0.5504  loss_ce_dn_2: 0.0003538  loss_mask_dn_2: 0.315  loss_dice_dn_2: 1.174  loss_bbox_dn_2: 0.2511  loss_giou_dn_2: 0.4483  loss_ce_3: 0.2992  loss_mask_3: 0.3116  loss_dice_3: 1.306  loss_bbox_3: 0.3744  loss_giou_3: 0.5551  loss_ce_dn_3: 0.0003134  loss_mask_dn_3: 0.3102  loss_dice_dn_3: 1.164  loss_bbox_dn_3: 0.2596  loss_giou_dn_3: 0.4564  loss_ce_4: 0.2101  loss_mask_4: 0.3061  loss_dice_4: 1.311  loss_bbox_4: 0.3657  loss_giou_4: 0.572  loss_ce_dn_4: 0.0001776  loss_mask_dn_4: 0.3074  loss_dice_dn_4: 1.188  loss_bbox_dn_4: 0.2546  loss_giou_dn_4: 0.4518  loss_ce_5: 0.194  loss_mask_5: 0.3238  loss_dice_5: 1.307  loss_bbox_5: 0.3658  loss_giou_5: 0.5736  loss_ce_dn_5: 0.0002516  loss_mask_dn_5: 0.3061  loss_dice_dn_5: 1.198  loss_bbox_dn_5: 0.2606  loss_giou_dn_5: 0.4573  loss_ce_6: 0.2092  loss_mask_6: 0.3106  loss_dice_6: 1.233  loss_bbox_6: 0.3602  loss_giou_6: 0.5617  loss_ce_dn_6: 0.0002957  loss_mask_dn_6: 0.3017  loss_dice_dn_6: 1.173  loss_bbox_dn_6: 0.2639  loss_giou_dn_6: 0.4529  loss_ce_7: 0.2027  loss_mask_7: 0.3236  loss_dice_7: 1.341  loss_bbox_7: 0.3605  loss_giou_7: 0.5779  loss_ce_dn_7: 0.0001522  loss_mask_dn_7: 0.3014  loss_dice_dn_7: 1.168  loss_bbox_dn_7: 0.261  loss_giou_dn_7: 0.4573  loss_ce_8: 0.2012  loss_mask_8: 0.3007  loss_dice_8: 1.298  loss_bbox_8: 0.35  loss_giou_8: 0.5687  loss_ce_dn_8: 9.842e-05  loss_mask_dn_8: 0.3064  loss_dice_dn_8: 1.166  loss_bbox_dn_8: 0.2636  loss_giou_dn_8: 0.4565  loss_ce_interm: 0.7792  loss_mask_interm: 0.2966  loss_dice_interm: 1.212  loss_bbox_interm: 0.3919  loss_giou_interm: 0.6074    time: 0.4914  last_time: 0.5079  data_time: 0.0035  last_data_time: 0.0029   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:34:55 d2.utils.events]:  eta: 0:03:55  iter: 3519  total_loss: 61.84  loss_ce: 0.1256  loss_mask: 0.4123  loss_dice: 1.244  loss_bbox: 0.3675  loss_giou: 0.5067  loss_ce_dn: 0.0001251  loss_mask_dn: 0.3967  loss_dice_dn: 1.144  loss_bbox_dn: 0.3015  loss_giou_dn: 0.3883  loss_ce_0: 0.6941  loss_mask_0: 0.3769  loss_dice_0: 1.25  loss_bbox_0: 0.6756  loss_giou_0: 0.8355  loss_ce_dn_0: 0.05776  loss_mask_dn_0: 0.8481  loss_dice_dn_0: 2.267  loss_bbox_dn_0: 0.8293  loss_giou_dn_0: 0.8508  loss_ce_1: 0.669  loss_mask_1: 0.4381  loss_dice_1: 1.251  loss_bbox_1: 0.3832  loss_giou_1: 0.5594  loss_ce_dn_1: 0.0008679  loss_mask_dn_1: 0.376  loss_dice_dn_1: 1.166  loss_bbox_dn_1: 0.3433  loss_giou_dn_1: 0.4705  loss_ce_2: 0.4478  loss_mask_2: 0.398  loss_dice_2: 1.279  loss_bbox_2: 0.3567  loss_giou_2: 0.5303  loss_ce_dn_2: 0.0003535  loss_mask_dn_2: 0.3866  loss_dice_dn_2: 1.228  loss_bbox_dn_2: 0.3126  loss_giou_dn_2: 0.4264  loss_ce_3: 0.2591  loss_mask_3: 0.4424  loss_dice_3: 1.264  loss_bbox_3: 0.3818  loss_giou_3: 0.4904  loss_ce_dn_3: 0.0003323  loss_mask_dn_3: 0.4056  loss_dice_dn_3: 1.152  loss_bbox_dn_3: 0.3095  loss_giou_dn_3: 0.3828  loss_ce_4: 0.203  loss_mask_4: 0.4161  loss_dice_4: 1.245  loss_bbox_4: 0.3567  loss_giou_4: 0.4855  loss_ce_dn_4: 0.0001656  loss_mask_dn_4: 0.3973  loss_dice_dn_4: 1.122  loss_bbox_dn_4: 0.3017  loss_giou_dn_4: 0.3874  loss_ce_5: 0.2072  loss_mask_5: 0.4123  loss_dice_5: 1.249  loss_bbox_5: 0.3662  loss_giou_5: 0.4568  loss_ce_dn_5: 0.0002282  loss_mask_dn_5: 0.4027  loss_dice_dn_5: 1.146  loss_bbox_dn_5: 0.2953  loss_giou_dn_5: 0.3788  loss_ce_6: 0.1936  loss_mask_6: 0.4105  loss_dice_6: 1.251  loss_bbox_6: 0.3651  loss_giou_6: 0.4601  loss_ce_dn_6: 0.000223  loss_mask_dn_6: 0.3974  loss_dice_dn_6: 1.19  loss_bbox_dn_6: 0.2959  loss_giou_dn_6: 0.3879  loss_ce_7: 0.1512  loss_mask_7: 0.4102  loss_dice_7: 1.251  loss_bbox_7: 0.3758  loss_giou_7: 0.4773  loss_ce_dn_7: 0.000151  loss_mask_dn_7: 0.397  loss_dice_dn_7: 1.146  loss_bbox_dn_7: 0.2984  loss_giou_dn_7: 0.3912  loss_ce_8: 0.1124  loss_mask_8: 0.4052  loss_dice_8: 1.262  loss_bbox_8: 0.3703  loss_giou_8: 0.4885  loss_ce_dn_8: 0.0001414  loss_mask_dn_8: 0.3978  loss_dice_dn_8: 1.147  loss_bbox_dn_8: 0.2998  loss_giou_dn_8: 0.3875  loss_ce_interm: 0.7372  loss_mask_interm: 0.3715  loss_dice_interm: 1.231  loss_bbox_interm: 0.4218  loss_giou_interm: 0.546    time: 0.4913  last_time: 0.4853  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:35:05 d2.utils.events]:  eta: 0:03:45  iter: 3539  total_loss: 61.07  loss_ce: 0.1825  loss_mask: 0.3911  loss_dice: 1.172  loss_bbox: 0.2623  loss_giou: 0.4138  loss_ce_dn: 0.0001232  loss_mask_dn: 0.4041  loss_dice_dn: 1.154  loss_bbox_dn: 0.2136  loss_giou_dn: 0.372  loss_ce_0: 0.613  loss_mask_0: 0.3914  loss_dice_0: 1.204  loss_bbox_0: 0.7408  loss_giou_0: 0.9281  loss_ce_dn_0: 0.0298  loss_mask_dn_0: 0.7233  loss_dice_dn_0: 2.51  loss_bbox_dn_0: 0.8578  loss_giou_dn_0: 0.8542  loss_ce_1: 0.6345  loss_mask_1: 0.3897  loss_dice_1: 1.181  loss_bbox_1: 0.3148  loss_giou_1: 0.451  loss_ce_dn_1: 0.0004052  loss_mask_dn_1: 0.4155  loss_dice_dn_1: 1.188  loss_bbox_dn_1: 0.3198  loss_giou_dn_1: 0.4281  loss_ce_2: 0.4961  loss_mask_2: 0.3984  loss_dice_2: 1.225  loss_bbox_2: 0.2526  loss_giou_2: 0.4656  loss_ce_dn_2: 0.0003292  loss_mask_dn_2: 0.4148  loss_dice_dn_2: 1.177  loss_bbox_dn_2: 0.2509  loss_giou_dn_2: 0.394  loss_ce_3: 0.336  loss_mask_3: 0.4061  loss_dice_3: 1.166  loss_bbox_3: 0.2781  loss_giou_3: 0.4459  loss_ce_dn_3: 0.0003042  loss_mask_dn_3: 0.3997  loss_dice_dn_3: 1.174  loss_bbox_dn_3: 0.2163  loss_giou_dn_3: 0.3787  loss_ce_4: 0.2131  loss_mask_4: 0.387  loss_dice_4: 1.246  loss_bbox_4: 0.2516  loss_giou_4: 0.4271  loss_ce_dn_4: 0.00015  loss_mask_dn_4: 0.3982  loss_dice_dn_4: 1.154  loss_bbox_dn_4: 0.2043  loss_giou_dn_4: 0.3761  loss_ce_5: 0.1966  loss_mask_5: 0.3746  loss_dice_5: 1.204  loss_bbox_5: 0.2606  loss_giou_5: 0.4208  loss_ce_dn_5: 0.0002555  loss_mask_dn_5: 0.4038  loss_dice_dn_5: 1.149  loss_bbox_dn_5: 0.2082  loss_giou_dn_5: 0.3614  loss_ce_6: 0.1822  loss_mask_6: 0.3914  loss_dice_6: 1.225  loss_bbox_6: 0.2634  loss_giou_6: 0.407  loss_ce_dn_6: 0.0002381  loss_mask_dn_6: 0.4044  loss_dice_dn_6: 1.148  loss_bbox_dn_6: 0.2053  loss_giou_dn_6: 0.3628  loss_ce_7: 0.176  loss_mask_7: 0.3955  loss_dice_7: 1.237  loss_bbox_7: 0.2606  loss_giou_7: 0.4146  loss_ce_dn_7: 0.0001472  loss_mask_dn_7: 0.4081  loss_dice_dn_7: 1.161  loss_bbox_dn_7: 0.2118  loss_giou_dn_7: 0.3668  loss_ce_8: 0.1824  loss_mask_8: 0.4013  loss_dice_8: 1.215  loss_bbox_8: 0.2602  loss_giou_8: 0.4148  loss_ce_dn_8: 0.0001327  loss_mask_dn_8: 0.4075  loss_dice_dn_8: 1.145  loss_bbox_dn_8: 0.2149  loss_giou_dn_8: 0.371  loss_ce_interm: 0.7761  loss_mask_interm: 0.3773  loss_dice_interm: 1.191  loss_bbox_interm: 0.3266  loss_giou_interm: 0.5407    time: 0.4913  last_time: 0.5219  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:35:15 d2.utils.events]:  eta: 0:03:35  iter: 3559  total_loss: 50  loss_ce: 0.2013  loss_mask: 0.2609  loss_dice: 1.11  loss_bbox: 0.2777  loss_giou: 0.4557  loss_ce_dn: 7.718e-05  loss_mask_dn: 0.2684  loss_dice_dn: 1.096  loss_bbox_dn: 0.2137  loss_giou_dn: 0.3804  loss_ce_0: 0.4845  loss_mask_0: 0.2661  loss_dice_0: 1.101  loss_bbox_0: 0.7029  loss_giou_0: 0.842  loss_ce_dn_0: 0.01785  loss_mask_dn_0: 0.4672  loss_dice_dn_0: 2.349  loss_bbox_dn_0: 0.8455  loss_giou_dn_0: 0.8417  loss_ce_1: 0.5632  loss_mask_1: 0.2696  loss_dice_1: 1.07  loss_bbox_1: 0.3068  loss_giou_1: 0.4711  loss_ce_dn_1: 0.0004075  loss_mask_dn_1: 0.2787  loss_dice_dn_1: 1.092  loss_bbox_dn_1: 0.3152  loss_giou_dn_1: 0.4453  loss_ce_2: 0.3006  loss_mask_2: 0.2774  loss_dice_2: 1.129  loss_bbox_2: 0.2972  loss_giou_2: 0.4567  loss_ce_dn_2: 0.0003036  loss_mask_dn_2: 0.2763  loss_dice_dn_2: 1.073  loss_bbox_dn_2: 0.2273  loss_giou_dn_2: 0.4096  loss_ce_3: 0.2341  loss_mask_3: 0.255  loss_dice_3: 1.147  loss_bbox_3: 0.2962  loss_giou_3: 0.4636  loss_ce_dn_3: 0.0002757  loss_mask_dn_3: 0.2744  loss_dice_dn_3: 1.093  loss_bbox_dn_3: 0.2248  loss_giou_dn_3: 0.3964  loss_ce_4: 0.2159  loss_mask_4: 0.2544  loss_dice_4: 1.135  loss_bbox_4: 0.2919  loss_giou_4: 0.461  loss_ce_dn_4: 0.0001603  loss_mask_dn_4: 0.2721  loss_dice_dn_4: 1.084  loss_bbox_dn_4: 0.2133  loss_giou_dn_4: 0.3876  loss_ce_5: 0.1824  loss_mask_5: 0.2715  loss_dice_5: 1.143  loss_bbox_5: 0.2883  loss_giou_5: 0.4667  loss_ce_dn_5: 0.0002338  loss_mask_dn_5: 0.2689  loss_dice_dn_5: 1.084  loss_bbox_dn_5: 0.2091  loss_giou_dn_5: 0.3777  loss_ce_6: 0.179  loss_mask_6: 0.2578  loss_dice_6: 1.097  loss_bbox_6: 0.2796  loss_giou_6: 0.4662  loss_ce_dn_6: 0.0001911  loss_mask_dn_6: 0.2686  loss_dice_dn_6: 1.089  loss_bbox_dn_6: 0.2156  loss_giou_dn_6: 0.3759  loss_ce_7: 0.1793  loss_mask_7: 0.2606  loss_dice_7: 1.142  loss_bbox_7: 0.2749  loss_giou_7: 0.4553  loss_ce_dn_7: 0.0001049  loss_mask_dn_7: 0.272  loss_dice_dn_7: 1.085  loss_bbox_dn_7: 0.2119  loss_giou_dn_7: 0.3785  loss_ce_8: 0.1888  loss_mask_8: 0.2612  loss_dice_8: 1.118  loss_bbox_8: 0.2787  loss_giou_8: 0.4586  loss_ce_dn_8: 8.194e-05  loss_mask_dn_8: 0.2688  loss_dice_dn_8: 1.089  loss_bbox_dn_8: 0.2119  loss_giou_dn_8: 0.3775  loss_ce_interm: 0.4871  loss_mask_interm: 0.285  loss_dice_interm: 1.08  loss_bbox_interm: 0.3403  loss_giou_interm: 0.5505    time: 0.4912  last_time: 0.4798  data_time: 0.0035  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:35:25 d2.utils.events]:  eta: 0:03:25  iter: 3579  total_loss: 63.44  loss_ce: 0.2316  loss_mask: 0.2898  loss_dice: 1.109  loss_bbox: 0.2935  loss_giou: 0.5702  loss_ce_dn: 0.0001229  loss_mask_dn: 0.2621  loss_dice_dn: 1.242  loss_bbox_dn: 0.2083  loss_giou_dn: 0.4724  loss_ce_0: 0.6441  loss_mask_0: 0.3114  loss_dice_0: 1.204  loss_bbox_0: 0.7211  loss_giou_0: 1.04  loss_ce_dn_0: 0.01786  loss_mask_dn_0: 0.4648  loss_dice_dn_0: 2.682  loss_bbox_dn_0: 0.6806  loss_giou_dn_0: 0.8499  loss_ce_1: 0.7057  loss_mask_1: 0.3054  loss_dice_1: 1.145  loss_bbox_1: 0.3545  loss_giou_1: 0.6251  loss_ce_dn_1: 0.0003274  loss_mask_dn_1: 0.2623  loss_dice_dn_1: 1.215  loss_bbox_dn_1: 0.2677  loss_giou_dn_1: 0.4978  loss_ce_2: 0.4513  loss_mask_2: 0.2828  loss_dice_2: 1.212  loss_bbox_2: 0.2981  loss_giou_2: 0.522  loss_ce_dn_2: 0.0003301  loss_mask_dn_2: 0.2699  loss_dice_dn_2: 1.219  loss_bbox_dn_2: 0.2405  loss_giou_dn_2: 0.4732  loss_ce_3: 0.4392  loss_mask_3: 0.2831  loss_dice_3: 1.289  loss_bbox_3: 0.2822  loss_giou_3: 0.5394  loss_ce_dn_3: 0.0004092  loss_mask_dn_3: 0.266  loss_dice_dn_3: 1.222  loss_bbox_dn_3: 0.2392  loss_giou_dn_3: 0.4728  loss_ce_4: 0.3794  loss_mask_4: 0.3149  loss_dice_4: 1.276  loss_bbox_4: 0.2989  loss_giou_4: 0.581  loss_ce_dn_4: 0.0001826  loss_mask_dn_4: 0.2666  loss_dice_dn_4: 1.229  loss_bbox_dn_4: 0.2262  loss_giou_dn_4: 0.4696  loss_ce_5: 0.2679  loss_mask_5: 0.2873  loss_dice_5: 1.311  loss_bbox_5: 0.303  loss_giou_5: 0.5664  loss_ce_dn_5: 0.0003424  loss_mask_dn_5: 0.2638  loss_dice_dn_5: 1.231  loss_bbox_dn_5: 0.2189  loss_giou_dn_5: 0.476  loss_ce_6: 0.2515  loss_mask_6: 0.302  loss_dice_6: 1.17  loss_bbox_6: 0.3225  loss_giou_6: 0.5606  loss_ce_dn_6: 0.0003541  loss_mask_dn_6: 0.2617  loss_dice_dn_6: 1.25  loss_bbox_dn_6: 0.2131  loss_giou_dn_6: 0.4768  loss_ce_7: 0.2109  loss_mask_7: 0.2814  loss_dice_7: 1.202  loss_bbox_7: 0.3154  loss_giou_7: 0.5587  loss_ce_dn_7: 0.0001622  loss_mask_dn_7: 0.2661  loss_dice_dn_7: 1.238  loss_bbox_dn_7: 0.2154  loss_giou_dn_7: 0.4748  loss_ce_8: 0.2195  loss_mask_8: 0.2977  loss_dice_8: 1.134  loss_bbox_8: 0.2986  loss_giou_8: 0.5638  loss_ce_dn_8: 0.0001239  loss_mask_dn_8: 0.263  loss_dice_dn_8: 1.24  loss_bbox_dn_8: 0.2076  loss_giou_dn_8: 0.4723  loss_ce_interm: 0.6274  loss_mask_interm: 0.3053  loss_dice_interm: 1.242  loss_bbox_interm: 0.3474  loss_giou_interm: 0.6675    time: 0.4912  last_time: 0.4833  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:35:35 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:35:35 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:35:35 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:35:35 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:35:36 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0803 s/iter. Eval: 0.0098 s/iter. Total: 0.0910 s/iter. ETA=0:00:05\n",
      "[03/14 17:35:41 d2.evaluation.evaluator]: Total inference time: 0:00:05.567106 (0.089792 s / iter per device, on 1 devices)\n",
      "[03/14 17:35:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.078068 s / iter per device, on 1 devices)\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.513\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.848\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.559\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.519\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.595\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.392\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.578\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.644\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.616\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.748\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 51.326 | 84.797 | 55.875 | 51.878 | 59.512 |  nan  |\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:35:41 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.274\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.779\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.254\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.358\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.230\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.341\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.381\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 27.360 | 77.938 | 4.110  | 25.370 | 35.799 |  nan  |\n",
      "[03/14 17:35:41 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:35:41 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: 51.3258,84.7968,55.8749,51.8777,59.5124,nan\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:35:41 d2.evaluation.testing]: copypaste: 27.3597,77.9375,4.1102,25.3703,35.7991,nan\n",
      "[03/14 17:35:41 d2.utils.events]:  eta: 0:03:15  iter: 3599  total_loss: 64.44  loss_ce: 0.3163  loss_mask: 0.4027  loss_dice: 1.358  loss_bbox: 0.3368  loss_giou: 0.5  loss_ce_dn: 0.0001135  loss_mask_dn: 0.3871  loss_dice_dn: 1.357  loss_bbox_dn: 0.2576  loss_giou_dn: 0.4156  loss_ce_0: 0.5989  loss_mask_0: 0.4004  loss_dice_0: 1.432  loss_bbox_0: 0.7685  loss_giou_0: 0.9362  loss_ce_dn_0: 0.03767  loss_mask_dn_0: 0.6535  loss_dice_dn_0: 2.657  loss_bbox_dn_0: 0.7828  loss_giou_dn_0: 0.8579  loss_ce_1: 0.7556  loss_mask_1: 0.3967  loss_dice_1: 1.381  loss_bbox_1: 0.3877  loss_giou_1: 0.5223  loss_ce_dn_1: 0.0004999  loss_mask_dn_1: 0.3896  loss_dice_dn_1: 1.403  loss_bbox_dn_1: 0.3127  loss_giou_dn_1: 0.49  loss_ce_2: 0.4721  loss_mask_2: 0.4211  loss_dice_2: 1.386  loss_bbox_2: 0.3854  loss_giou_2: 0.5179  loss_ce_dn_2: 0.0003535  loss_mask_dn_2: 0.4024  loss_dice_dn_2: 1.389  loss_bbox_dn_2: 0.2804  loss_giou_dn_2: 0.4458  loss_ce_3: 0.3963  loss_mask_3: 0.413  loss_dice_3: 1.355  loss_bbox_3: 0.3465  loss_giou_3: 0.5009  loss_ce_dn_3: 0.0003616  loss_mask_dn_3: 0.3896  loss_dice_dn_3: 1.357  loss_bbox_dn_3: 0.2615  loss_giou_dn_3: 0.433  loss_ce_4: 0.3231  loss_mask_4: 0.4085  loss_dice_4: 1.419  loss_bbox_4: 0.3452  loss_giou_4: 0.4986  loss_ce_dn_4: 0.0002587  loss_mask_dn_4: 0.3933  loss_dice_dn_4: 1.366  loss_bbox_dn_4: 0.2602  loss_giou_dn_4: 0.4228  loss_ce_5: 0.2725  loss_mask_5: 0.418  loss_dice_5: 1.388  loss_bbox_5: 0.3309  loss_giou_5: 0.484  loss_ce_dn_5: 0.0004416  loss_mask_dn_5: 0.3941  loss_dice_dn_5: 1.369  loss_bbox_dn_5: 0.2602  loss_giou_dn_5: 0.4208  loss_ce_6: 0.2548  loss_mask_6: 0.4  loss_dice_6: 1.407  loss_bbox_6: 0.3221  loss_giou_6: 0.494  loss_ce_dn_6: 0.0003938  loss_mask_dn_6: 0.395  loss_dice_dn_6: 1.35  loss_bbox_dn_6: 0.2597  loss_giou_dn_6: 0.4198  loss_ce_7: 0.2594  loss_mask_7: 0.4075  loss_dice_7: 1.418  loss_bbox_7: 0.3435  loss_giou_7: 0.4923  loss_ce_dn_7: 0.0002833  loss_mask_dn_7: 0.3913  loss_dice_dn_7: 1.354  loss_bbox_dn_7: 0.258  loss_giou_dn_7: 0.4134  loss_ce_8: 0.2642  loss_mask_8: 0.4025  loss_dice_8: 1.475  loss_bbox_8: 0.3342  loss_giou_8: 0.5044  loss_ce_dn_8: 0.0001824  loss_mask_dn_8: 0.3871  loss_dice_dn_8: 1.353  loss_bbox_dn_8: 0.259  loss_giou_dn_8: 0.4173  loss_ce_interm: 0.5925  loss_mask_interm: 0.3984  loss_dice_interm: 1.433  loss_bbox_interm: 0.4432  loss_giou_interm: 0.5792    time: 0.4911  last_time: 0.4543  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:35:51 d2.utils.events]:  eta: 0:03:06  iter: 3619  total_loss: 48.72  loss_ce: 0.1403  loss_mask: 0.3066  loss_dice: 1.095  loss_bbox: 0.2593  loss_giou: 0.443  loss_ce_dn: 0.0001255  loss_mask_dn: 0.315  loss_dice_dn: 1.059  loss_bbox_dn: 0.221  loss_giou_dn: 0.3765  loss_ce_0: 0.405  loss_mask_0: 0.3187  loss_dice_0: 1.171  loss_bbox_0: 0.8115  loss_giou_0: 0.8905  loss_ce_dn_0: 0.03764  loss_mask_dn_0: 0.4525  loss_dice_dn_0: 2.405  loss_bbox_dn_0: 0.8449  loss_giou_dn_0: 0.858  loss_ce_1: 0.559  loss_mask_1: 0.3145  loss_dice_1: 1.101  loss_bbox_1: 0.2556  loss_giou_1: 0.4797  loss_ce_dn_1: 0.0005512  loss_mask_dn_1: 0.3217  loss_dice_dn_1: 1.077  loss_bbox_dn_1: 0.2881  loss_giou_dn_1: 0.4336  loss_ce_2: 0.3724  loss_mask_2: 0.3114  loss_dice_2: 1.108  loss_bbox_2: 0.2748  loss_giou_2: 0.4805  loss_ce_dn_2: 0.0002891  loss_mask_dn_2: 0.3251  loss_dice_dn_2: 1.075  loss_bbox_dn_2: 0.2287  loss_giou_dn_2: 0.3933  loss_ce_3: 0.1552  loss_mask_3: 0.3228  loss_dice_3: 1.1  loss_bbox_3: 0.2588  loss_giou_3: 0.4493  loss_ce_dn_3: 0.0002427  loss_mask_dn_3: 0.3142  loss_dice_dn_3: 1.066  loss_bbox_dn_3: 0.2373  loss_giou_dn_3: 0.3802  loss_ce_4: 0.1356  loss_mask_4: 0.3224  loss_dice_4: 1.138  loss_bbox_4: 0.2717  loss_giou_4: 0.464  loss_ce_dn_4: 0.0001207  loss_mask_dn_4: 0.3126  loss_dice_dn_4: 1.068  loss_bbox_dn_4: 0.2183  loss_giou_dn_4: 0.3796  loss_ce_5: 0.1364  loss_mask_5: 0.3123  loss_dice_5: 1.105  loss_bbox_5: 0.274  loss_giou_5: 0.4506  loss_ce_dn_5: 0.0002102  loss_mask_dn_5: 0.3157  loss_dice_dn_5: 1.057  loss_bbox_dn_5: 0.2243  loss_giou_dn_5: 0.3703  loss_ce_6: 0.1266  loss_mask_6: 0.2995  loss_dice_6: 1.101  loss_bbox_6: 0.2686  loss_giou_6: 0.4548  loss_ce_dn_6: 0.0002381  loss_mask_dn_6: 0.3129  loss_dice_dn_6: 1.064  loss_bbox_dn_6: 0.2233  loss_giou_dn_6: 0.3724  loss_ce_7: 0.1197  loss_mask_7: 0.3193  loss_dice_7: 1.097  loss_bbox_7: 0.2554  loss_giou_7: 0.4404  loss_ce_dn_7: 0.0001308  loss_mask_dn_7: 0.3184  loss_dice_dn_7: 1.056  loss_bbox_dn_7: 0.2196  loss_giou_dn_7: 0.3723  loss_ce_8: 0.1417  loss_mask_8: 0.3119  loss_dice_8: 1.137  loss_bbox_8: 0.2497  loss_giou_8: 0.4428  loss_ce_dn_8: 0.0001317  loss_mask_dn_8: 0.3123  loss_dice_dn_8: 1.064  loss_bbox_dn_8: 0.2209  loss_giou_dn_8: 0.3733  loss_ce_interm: 0.4387  loss_mask_interm: 0.3154  loss_dice_interm: 1.132  loss_bbox_interm: 0.3292  loss_giou_interm: 0.5236    time: 0.4911  last_time: 0.4989  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:36:05 d2.utils.events]:  eta: 0:02:56  iter: 3639  total_loss: 53.01  loss_ce: 0.223  loss_mask: 0.262  loss_dice: 0.9378  loss_bbox: 0.3  loss_giou: 0.4742  loss_ce_dn: 8.697e-05  loss_mask_dn: 0.2671  loss_dice_dn: 0.9548  loss_bbox_dn: 0.2398  loss_giou_dn: 0.4204  loss_ce_0: 0.5589  loss_mask_0: 0.3064  loss_dice_0: 0.966  loss_bbox_0: 0.6134  loss_giou_0: 0.846  loss_ce_dn_0: 0.01765  loss_mask_dn_0: 0.4943  loss_dice_dn_0: 2.202  loss_bbox_dn_0: 0.8041  loss_giou_dn_0: 0.8542  loss_ce_1: 0.552  loss_mask_1: 0.2913  loss_dice_1: 1.051  loss_bbox_1: 0.3686  loss_giou_1: 0.4514  loss_ce_dn_1: 0.0003653  loss_mask_dn_1: 0.2779  loss_dice_dn_1: 0.9692  loss_bbox_dn_1: 0.2964  loss_giou_dn_1: 0.4282  loss_ce_2: 0.4735  loss_mask_2: 0.3097  loss_dice_2: 1.056  loss_bbox_2: 0.3331  loss_giou_2: 0.4764  loss_ce_dn_2: 0.0002858  loss_mask_dn_2: 0.2805  loss_dice_dn_2: 0.9587  loss_bbox_dn_2: 0.2613  loss_giou_dn_2: 0.4244  loss_ce_3: 0.3502  loss_mask_3: 0.2954  loss_dice_3: 0.9505  loss_bbox_3: 0.3231  loss_giou_3: 0.4856  loss_ce_dn_3: 0.0002885  loss_mask_dn_3: 0.2663  loss_dice_dn_3: 0.9523  loss_bbox_dn_3: 0.2563  loss_giou_dn_3: 0.4301  loss_ce_4: 0.3281  loss_mask_4: 0.2617  loss_dice_4: 0.9148  loss_bbox_4: 0.3177  loss_giou_4: 0.4864  loss_ce_dn_4: 0.0002266  loss_mask_dn_4: 0.2638  loss_dice_dn_4: 0.9434  loss_bbox_dn_4: 0.2397  loss_giou_dn_4: 0.4331  loss_ce_5: 0.3515  loss_mask_5: 0.2678  loss_dice_5: 0.9838  loss_bbox_5: 0.3016  loss_giou_5: 0.4698  loss_ce_dn_5: 0.0002776  loss_mask_dn_5: 0.2705  loss_dice_dn_5: 0.9561  loss_bbox_dn_5: 0.2454  loss_giou_dn_5: 0.4217  loss_ce_6: 0.2586  loss_mask_6: 0.2702  loss_dice_6: 0.9389  loss_bbox_6: 0.2998  loss_giou_6: 0.4803  loss_ce_dn_6: 0.0003402  loss_mask_dn_6: 0.266  loss_dice_dn_6: 0.9509  loss_bbox_dn_6: 0.2417  loss_giou_dn_6: 0.4256  loss_ce_7: 0.2034  loss_mask_7: 0.2681  loss_dice_7: 0.9958  loss_bbox_7: 0.2946  loss_giou_7: 0.4769  loss_ce_dn_7: 0.0001556  loss_mask_dn_7: 0.27  loss_dice_dn_7: 0.9435  loss_bbox_dn_7: 0.2492  loss_giou_dn_7: 0.4208  loss_ce_8: 0.204  loss_mask_8: 0.2872  loss_dice_8: 0.9548  loss_bbox_8: 0.2993  loss_giou_8: 0.4772  loss_ce_dn_8: 0.0001133  loss_mask_dn_8: 0.2656  loss_dice_dn_8: 0.9512  loss_bbox_dn_8: 0.2377  loss_giou_dn_8: 0.4202  loss_ce_interm: 0.6137  loss_mask_interm: 0.3008  loss_dice_interm: 0.9996  loss_bbox_interm: 0.387  loss_giou_interm: 0.5931    time: 0.4923  last_time: 0.4756  data_time: 0.0098  last_data_time: 0.0034   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:36:15 d2.utils.events]:  eta: 0:02:46  iter: 3659  total_loss: 61.27  loss_ce: 0.3018  loss_mask: 0.3626  loss_dice: 1.344  loss_bbox: 0.2995  loss_giou: 0.5956  loss_ce_dn: 0.0001226  loss_mask_dn: 0.3339  loss_dice_dn: 1.3  loss_bbox_dn: 0.2036  loss_giou_dn: 0.4821  loss_ce_0: 0.628  loss_mask_0: 0.3594  loss_dice_0: 1.502  loss_bbox_0: 0.6827  loss_giou_0: 1.009  loss_ce_dn_0: 0.03745  loss_mask_dn_0: 0.505  loss_dice_dn_0: 2.475  loss_bbox_dn_0: 0.7748  loss_giou_dn_0: 0.856  loss_ce_1: 0.5728  loss_mask_1: 0.3371  loss_dice_1: 1.367  loss_bbox_1: 0.3436  loss_giou_1: 0.6536  loss_ce_dn_1: 0.0004114  loss_mask_dn_1: 0.3372  loss_dice_dn_1: 1.316  loss_bbox_dn_1: 0.2846  loss_giou_dn_1: 0.5074  loss_ce_2: 0.4495  loss_mask_2: 0.4064  loss_dice_2: 1.41  loss_bbox_2: 0.3203  loss_giou_2: 0.6477  loss_ce_dn_2: 0.0002842  loss_mask_dn_2: 0.3346  loss_dice_dn_2: 1.327  loss_bbox_dn_2: 0.249  loss_giou_dn_2: 0.4843  loss_ce_3: 0.3685  loss_mask_3: 0.3773  loss_dice_3: 1.363  loss_bbox_3: 0.3146  loss_giou_3: 0.6146  loss_ce_dn_3: 0.0002418  loss_mask_dn_3: 0.3207  loss_dice_dn_3: 1.285  loss_bbox_dn_3: 0.253  loss_giou_dn_3: 0.4659  loss_ce_4: 0.3144  loss_mask_4: 0.3745  loss_dice_4: 1.42  loss_bbox_4: 0.3333  loss_giou_4: 0.6214  loss_ce_dn_4: 0.000152  loss_mask_dn_4: 0.3233  loss_dice_dn_4: 1.305  loss_bbox_dn_4: 0.2198  loss_giou_dn_4: 0.4736  loss_ce_5: 0.2901  loss_mask_5: 0.3734  loss_dice_5: 1.406  loss_bbox_5: 0.3418  loss_giou_5: 0.666  loss_ce_dn_5: 0.0002272  loss_mask_dn_5: 0.3243  loss_dice_dn_5: 1.314  loss_bbox_dn_5: 0.2167  loss_giou_dn_5: 0.4786  loss_ce_6: 0.2945  loss_mask_6: 0.3766  loss_dice_6: 1.438  loss_bbox_6: 0.3418  loss_giou_6: 0.6585  loss_ce_dn_6: 0.0002667  loss_mask_dn_6: 0.3242  loss_dice_dn_6: 1.315  loss_bbox_dn_6: 0.2119  loss_giou_dn_6: 0.4802  loss_ce_7: 0.2763  loss_mask_7: 0.3575  loss_dice_7: 1.428  loss_bbox_7: 0.3278  loss_giou_7: 0.6558  loss_ce_dn_7: 0.00015  loss_mask_dn_7: 0.3292  loss_dice_dn_7: 1.293  loss_bbox_dn_7: 0.2146  loss_giou_dn_7: 0.4828  loss_ce_8: 0.2884  loss_mask_8: 0.3984  loss_dice_8: 1.413  loss_bbox_8: 0.3196  loss_giou_8: 0.6341  loss_ce_dn_8: 0.0001356  loss_mask_dn_8: 0.3271  loss_dice_dn_8: 1.299  loss_bbox_dn_8: 0.2047  loss_giou_dn_8: 0.4823  loss_ce_interm: 0.6598  loss_mask_interm: 0.343  loss_dice_interm: 1.471  loss_bbox_interm: 0.3851  loss_giou_interm: 0.6675    time: 0.4923  last_time: 0.4559  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:36:25 d2.utils.events]:  eta: 0:02:36  iter: 3679  total_loss: 56.54  loss_ce: 0.215  loss_mask: 0.375  loss_dice: 1.277  loss_bbox: 0.2553  loss_giou: 0.5577  loss_ce_dn: 8.289e-05  loss_mask_dn: 0.36  loss_dice_dn: 1.142  loss_bbox_dn: 0.2107  loss_giou_dn: 0.4408  loss_ce_0: 0.4648  loss_mask_0: 0.3794  loss_dice_0: 1.335  loss_bbox_0: 0.7349  loss_giou_0: 1.014  loss_ce_dn_0: 0.04927  loss_mask_dn_0: 0.7438  loss_dice_dn_0: 2.558  loss_bbox_dn_0: 0.9264  loss_giou_dn_0: 0.8535  loss_ce_1: 0.6282  loss_mask_1: 0.4568  loss_dice_1: 1.183  loss_bbox_1: 0.3431  loss_giou_1: 0.5791  loss_ce_dn_1: 0.000565  loss_mask_dn_1: 0.3752  loss_dice_dn_1: 1.144  loss_bbox_dn_1: 0.3285  loss_giou_dn_1: 0.4784  loss_ce_2: 0.3289  loss_mask_2: 0.3952  loss_dice_2: 1.231  loss_bbox_2: 0.3108  loss_giou_2: 0.6569  loss_ce_dn_2: 0.0003968  loss_mask_dn_2: 0.3733  loss_dice_dn_2: 1.136  loss_bbox_dn_2: 0.2484  loss_giou_dn_2: 0.4499  loss_ce_3: 0.221  loss_mask_3: 0.3806  loss_dice_3: 1.247  loss_bbox_3: 0.2705  loss_giou_3: 0.6062  loss_ce_dn_3: 0.0003677  loss_mask_dn_3: 0.3611  loss_dice_dn_3: 1.144  loss_bbox_dn_3: 0.2243  loss_giou_dn_3: 0.4433  loss_ce_4: 0.1776  loss_mask_4: 0.3789  loss_dice_4: 1.261  loss_bbox_4: 0.2642  loss_giou_4: 0.6229  loss_ce_dn_4: 0.0001813  loss_mask_dn_4: 0.3641  loss_dice_dn_4: 1.121  loss_bbox_dn_4: 0.2133  loss_giou_dn_4: 0.431  loss_ce_5: 0.1827  loss_mask_5: 0.3727  loss_dice_5: 1.176  loss_bbox_5: 0.2732  loss_giou_5: 0.5906  loss_ce_dn_5: 0.0002915  loss_mask_dn_5: 0.3591  loss_dice_dn_5: 1.117  loss_bbox_dn_5: 0.209  loss_giou_dn_5: 0.4259  loss_ce_6: 0.2201  loss_mask_6: 0.3734  loss_dice_6: 1.245  loss_bbox_6: 0.248  loss_giou_6: 0.5782  loss_ce_dn_6: 0.0002549  loss_mask_dn_6: 0.3597  loss_dice_dn_6: 1.143  loss_bbox_dn_6: 0.2135  loss_giou_dn_6: 0.4394  loss_ce_7: 0.2181  loss_mask_7: 0.3669  loss_dice_7: 1.272  loss_bbox_7: 0.2555  loss_giou_7: 0.5608  loss_ce_dn_7: 0.0001486  loss_mask_dn_7: 0.3579  loss_dice_dn_7: 1.142  loss_bbox_dn_7: 0.2061  loss_giou_dn_7: 0.4423  loss_ce_8: 0.2213  loss_mask_8: 0.3717  loss_dice_8: 1.211  loss_bbox_8: 0.2734  loss_giou_8: 0.5598  loss_ce_dn_8: 8.915e-05  loss_mask_dn_8: 0.3548  loss_dice_dn_8: 1.136  loss_bbox_dn_8: 0.2093  loss_giou_dn_8: 0.4387  loss_ce_interm: 0.6575  loss_mask_interm: 0.3608  loss_dice_interm: 1.132  loss_bbox_interm: 0.3873  loss_giou_interm: 0.6101    time: 0.4922  last_time: 0.4696  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:36:35 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:36:35 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:36:35 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:36:35 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:36:36 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0773 s/iter. Eval: 0.0092 s/iter. Total: 0.0875 s/iter. ETA=0:00:04\n",
      "[03/14 17:36:41 d2.evaluation.evaluator]: Total inference time: 0:00:05.428841 (0.087562 s / iter per device, on 1 devices)\n",
      "[03/14 17:36:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075483 s / iter per device, on 1 devices)\n",
      "[03/14 17:36:41 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:36:41 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:36:41 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:36:41 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:36:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:36:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:36:41 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.519\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.868\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.562\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.603\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.379\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.619\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.757\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:36:41 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 51.881 | 86.769 | 56.249 | 51.996 | 60.334 |  nan  |\n",
      "[03/14 17:36:41 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:36:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:36:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "[03/14 17:36:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:36:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.274\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.794\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.357\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.226\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.340\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:36:42 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 27.447 | 79.426 | 5.892  | 25.576 | 35.654 |  nan  |\n",
      "[03/14 17:36:42 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:36:42 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: 51.8812,86.7691,56.2487,51.9955,60.3340,nan\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:36:42 d2.evaluation.testing]: copypaste: 27.4465,79.4255,5.8915,25.5763,35.6545,nan\n",
      "[03/14 17:36:42 d2.utils.events]:  eta: 0:02:26  iter: 3699  total_loss: 61.9  loss_ce: 0.4611  loss_mask: 0.322  loss_dice: 1.193  loss_bbox: 0.282  loss_giou: 0.5688  loss_ce_dn: 0.0001352  loss_mask_dn: 0.3078  loss_dice_dn: 1.199  loss_bbox_dn: 0.2447  loss_giou_dn: 0.4062  loss_ce_0: 0.6134  loss_mask_0: 0.3597  loss_dice_0: 1.376  loss_bbox_0: 0.7155  loss_giou_0: 0.9501  loss_ce_dn_0: 0.05708  loss_mask_dn_0: 0.4991  loss_dice_dn_0: 2.579  loss_bbox_dn_0: 0.8263  loss_giou_dn_0: 0.8538  loss_ce_1: 0.7742  loss_mask_1: 0.3135  loss_dice_1: 1.382  loss_bbox_1: 0.3367  loss_giou_1: 0.6235  loss_ce_dn_1: 0.0004518  loss_mask_dn_1: 0.3195  loss_dice_dn_1: 1.254  loss_bbox_dn_1: 0.2764  loss_giou_dn_1: 0.4443  loss_ce_2: 0.6165  loss_mask_2: 0.3569  loss_dice_2: 1.361  loss_bbox_2: 0.323  loss_giou_2: 0.6045  loss_ce_dn_2: 0.000302  loss_mask_dn_2: 0.3297  loss_dice_dn_2: 1.276  loss_bbox_dn_2: 0.2536  loss_giou_dn_2: 0.4222  loss_ce_3: 0.5227  loss_mask_3: 0.3463  loss_dice_3: 1.207  loss_bbox_3: 0.2966  loss_giou_3: 0.5611  loss_ce_dn_3: 0.0003063  loss_mask_dn_3: 0.3314  loss_dice_dn_3: 1.243  loss_bbox_dn_3: 0.2591  loss_giou_dn_3: 0.4138  loss_ce_4: 0.5192  loss_mask_4: 0.3253  loss_dice_4: 1.242  loss_bbox_4: 0.2931  loss_giou_4: 0.5433  loss_ce_dn_4: 0.000241  loss_mask_dn_4: 0.3192  loss_dice_dn_4: 1.247  loss_bbox_dn_4: 0.2604  loss_giou_dn_4: 0.4062  loss_ce_5: 0.4582  loss_mask_5: 0.2888  loss_dice_5: 1.25  loss_bbox_5: 0.2962  loss_giou_5: 0.5604  loss_ce_dn_5: 0.0002772  loss_mask_dn_5: 0.3133  loss_dice_dn_5: 1.237  loss_bbox_dn_5: 0.2518  loss_giou_dn_5: 0.4038  loss_ce_6: 0.458  loss_mask_6: 0.3262  loss_dice_6: 1.246  loss_bbox_6: 0.2954  loss_giou_6: 0.5362  loss_ce_dn_6: 0.0002527  loss_mask_dn_6: 0.3097  loss_dice_dn_6: 1.218  loss_bbox_dn_6: 0.2553  loss_giou_dn_6: 0.4084  loss_ce_7: 0.4411  loss_mask_7: 0.3117  loss_dice_7: 1.306  loss_bbox_7: 0.2863  loss_giou_7: 0.4993  loss_ce_dn_7: 0.0001379  loss_mask_dn_7: 0.3022  loss_dice_dn_7: 1.225  loss_bbox_dn_7: 0.2534  loss_giou_dn_7: 0.4064  loss_ce_8: 0.461  loss_mask_8: 0.3162  loss_dice_8: 1.282  loss_bbox_8: 0.2815  loss_giou_8: 0.5499  loss_ce_dn_8: 0.0001239  loss_mask_dn_8: 0.3047  loss_dice_dn_8: 1.22  loss_bbox_dn_8: 0.2503  loss_giou_dn_8: 0.4023  loss_ce_interm: 0.6248  loss_mask_interm: 0.3382  loss_dice_interm: 1.329  loss_bbox_interm: 0.3865  loss_giou_interm: 0.7415    time: 0.4922  last_time: 0.4729  data_time: 0.0034  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:36:52 d2.utils.events]:  eta: 0:02:17  iter: 3719  total_loss: 58.47  loss_ce: 0.1717  loss_mask: 0.2928  loss_dice: 1.291  loss_bbox: 0.2706  loss_giou: 0.524  loss_ce_dn: 0.0001419  loss_mask_dn: 0.2884  loss_dice_dn: 1.246  loss_bbox_dn: 0.213  loss_giou_dn: 0.4104  loss_ce_0: 0.6372  loss_mask_0: 0.3092  loss_dice_0: 1.351  loss_bbox_0: 0.6729  loss_giou_0: 1.037  loss_ce_dn_0: 0.0177  loss_mask_dn_0: 0.4779  loss_dice_dn_0: 2.444  loss_bbox_dn_0: 0.763  loss_giou_dn_0: 0.8557  loss_ce_1: 0.6571  loss_mask_1: 0.3104  loss_dice_1: 1.306  loss_bbox_1: 0.3097  loss_giou_1: 0.5433  loss_ce_dn_1: 0.0004141  loss_mask_dn_1: 0.2823  loss_dice_dn_1: 1.252  loss_bbox_dn_1: 0.288  loss_giou_dn_1: 0.4844  loss_ce_2: 0.4921  loss_mask_2: 0.3061  loss_dice_2: 1.21  loss_bbox_2: 0.2901  loss_giou_2: 0.5111  loss_ce_dn_2: 0.000411  loss_mask_dn_2: 0.2903  loss_dice_dn_2: 1.287  loss_bbox_dn_2: 0.2394  loss_giou_dn_2: 0.4201  loss_ce_3: 0.3059  loss_mask_3: 0.2919  loss_dice_3: 1.244  loss_bbox_3: 0.2754  loss_giou_3: 0.4941  loss_ce_dn_3: 0.0003411  loss_mask_dn_3: 0.2966  loss_dice_dn_3: 1.26  loss_bbox_dn_3: 0.2211  loss_giou_dn_3: 0.4182  loss_ce_4: 0.2315  loss_mask_4: 0.3067  loss_dice_4: 1.248  loss_bbox_4: 0.263  loss_giou_4: 0.4885  loss_ce_dn_4: 0.000265  loss_mask_dn_4: 0.2947  loss_dice_dn_4: 1.256  loss_bbox_dn_4: 0.2226  loss_giou_dn_4: 0.4023  loss_ce_5: 0.1779  loss_mask_5: 0.2898  loss_dice_5: 1.25  loss_bbox_5: 0.2683  loss_giou_5: 0.506  loss_ce_dn_5: 0.0003874  loss_mask_dn_5: 0.2906  loss_dice_dn_5: 1.246  loss_bbox_dn_5: 0.2175  loss_giou_dn_5: 0.411  loss_ce_6: 0.1769  loss_mask_6: 0.2993  loss_dice_6: 1.281  loss_bbox_6: 0.2677  loss_giou_6: 0.5101  loss_ce_dn_6: 0.0003445  loss_mask_dn_6: 0.2909  loss_dice_dn_6: 1.26  loss_bbox_dn_6: 0.2174  loss_giou_dn_6: 0.4026  loss_ce_7: 0.1658  loss_mask_7: 0.2928  loss_dice_7: 1.298  loss_bbox_7: 0.2676  loss_giou_7: 0.5137  loss_ce_dn_7: 0.0002005  loss_mask_dn_7: 0.2901  loss_dice_dn_7: 1.26  loss_bbox_dn_7: 0.2137  loss_giou_dn_7: 0.404  loss_ce_8: 0.1674  loss_mask_8: 0.2978  loss_dice_8: 1.243  loss_bbox_8: 0.2752  loss_giou_8: 0.5178  loss_ce_dn_8: 0.0001746  loss_mask_dn_8: 0.2891  loss_dice_dn_8: 1.252  loss_bbox_dn_8: 0.2132  loss_giou_dn_8: 0.407  loss_ce_interm: 0.6933  loss_mask_interm: 0.2765  loss_dice_interm: 1.318  loss_bbox_interm: 0.362  loss_giou_interm: 0.6631    time: 0.4922  last_time: 0.5100  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:37:02 d2.utils.events]:  eta: 0:02:07  iter: 3739  total_loss: 62.88  loss_ce: 0.3102  loss_mask: 0.3677  loss_dice: 1.185  loss_bbox: 0.359  loss_giou: 0.5908  loss_ce_dn: 0.0002015  loss_mask_dn: 0.362  loss_dice_dn: 1.116  loss_bbox_dn: 0.2465  loss_giou_dn: 0.4015  loss_ce_0: 0.5684  loss_mask_0: 0.3703  loss_dice_0: 1.24  loss_bbox_0: 0.7315  loss_giou_0: 0.8355  loss_ce_dn_0: 0.04912  loss_mask_dn_0: 0.9229  loss_dice_dn_0: 2.481  loss_bbox_dn_0: 0.896  loss_giou_dn_0: 0.8483  loss_ce_1: 0.6514  loss_mask_1: 0.3829  loss_dice_1: 1.178  loss_bbox_1: 0.3868  loss_giou_1: 0.5698  loss_ce_dn_1: 0.0005486  loss_mask_dn_1: 0.3772  loss_dice_dn_1: 1.139  loss_bbox_dn_1: 0.3587  loss_giou_dn_1: 0.4732  loss_ce_2: 0.4065  loss_mask_2: 0.3633  loss_dice_2: 1.179  loss_bbox_2: 0.3754  loss_giou_2: 0.572  loss_ce_dn_2: 0.0003408  loss_mask_dn_2: 0.3741  loss_dice_dn_2: 1.099  loss_bbox_dn_2: 0.2776  loss_giou_dn_2: 0.4188  loss_ce_3: 0.3131  loss_mask_3: 0.3803  loss_dice_3: 1.175  loss_bbox_3: 0.3828  loss_giou_3: 0.5709  loss_ce_dn_3: 0.0003124  loss_mask_dn_3: 0.3694  loss_dice_dn_3: 1.101  loss_bbox_dn_3: 0.2645  loss_giou_dn_3: 0.4127  loss_ce_4: 0.328  loss_mask_4: 0.3829  loss_dice_4: 1.168  loss_bbox_4: 0.3406  loss_giou_4: 0.5592  loss_ce_dn_4: 0.0002042  loss_mask_dn_4: 0.3695  loss_dice_dn_4: 1.101  loss_bbox_dn_4: 0.2678  loss_giou_dn_4: 0.4045  loss_ce_5: 0.3066  loss_mask_5: 0.3647  loss_dice_5: 1.165  loss_bbox_5: 0.3919  loss_giou_5: 0.5782  loss_ce_dn_5: 0.0003284  loss_mask_dn_5: 0.3707  loss_dice_dn_5: 1.101  loss_bbox_dn_5: 0.2583  loss_giou_dn_5: 0.3981  loss_ce_6: 0.3011  loss_mask_6: 0.3719  loss_dice_6: 1.215  loss_bbox_6: 0.3642  loss_giou_6: 0.5686  loss_ce_dn_6: 0.0002929  loss_mask_dn_6: 0.3681  loss_dice_dn_6: 1.097  loss_bbox_dn_6: 0.2573  loss_giou_dn_6: 0.4025  loss_ce_7: 0.2972  loss_mask_7: 0.3672  loss_dice_7: 1.132  loss_bbox_7: 0.3597  loss_giou_7: 0.5788  loss_ce_dn_7: 0.00027  loss_mask_dn_7: 0.3591  loss_dice_dn_7: 1.103  loss_bbox_dn_7: 0.25  loss_giou_dn_7: 0.4018  loss_ce_8: 0.3157  loss_mask_8: 0.3581  loss_dice_8: 1.127  loss_bbox_8: 0.3574  loss_giou_8: 0.591  loss_ce_dn_8: 0.0002419  loss_mask_dn_8: 0.3608  loss_dice_dn_8: 1.121  loss_bbox_dn_8: 0.2514  loss_giou_dn_8: 0.4007  loss_ce_interm: 0.5821  loss_mask_interm: 0.3747  loss_dice_interm: 1.203  loss_bbox_interm: 0.4305  loss_giou_interm: 0.6267    time: 0.4924  last_time: 0.5426  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:37:16 d2.utils.events]:  eta: 0:01:58  iter: 3759  total_loss: 61.49  loss_ce: 0.2017  loss_mask: 0.3607  loss_dice: 1.203  loss_bbox: 0.2816  loss_giou: 0.538  loss_ce_dn: 0.0001373  loss_mask_dn: 0.3635  loss_dice_dn: 1.219  loss_bbox_dn: 0.2384  loss_giou_dn: 0.4052  loss_ce_0: 0.6528  loss_mask_0: 0.3669  loss_dice_0: 1.337  loss_bbox_0: 0.6996  loss_giou_0: 0.9028  loss_ce_dn_0: 0.01763  loss_mask_dn_0: 0.5539  loss_dice_dn_0: 2.533  loss_bbox_dn_0: 0.8193  loss_giou_dn_0: 0.8594  loss_ce_1: 0.6386  loss_mask_1: 0.3625  loss_dice_1: 1.234  loss_bbox_1: 0.2895  loss_giou_1: 0.5431  loss_ce_dn_1: 0.0003797  loss_mask_dn_1: 0.369  loss_dice_dn_1: 1.296  loss_bbox_dn_1: 0.294  loss_giou_dn_1: 0.456  loss_ce_2: 0.4972  loss_mask_2: 0.3482  loss_dice_2: 1.235  loss_bbox_2: 0.2987  loss_giou_2: 0.5373  loss_ce_dn_2: 0.0004234  loss_mask_dn_2: 0.3735  loss_dice_dn_2: 1.242  loss_bbox_dn_2: 0.2778  loss_giou_dn_2: 0.4038  loss_ce_3: 0.3825  loss_mask_3: 0.3654  loss_dice_3: 1.228  loss_bbox_3: 0.2894  loss_giou_3: 0.5318  loss_ce_dn_3: 0.0003514  loss_mask_dn_3: 0.3597  loss_dice_dn_3: 1.232  loss_bbox_dn_3: 0.2493  loss_giou_dn_3: 0.4077  loss_ce_4: 0.2766  loss_mask_4: 0.358  loss_dice_4: 1.231  loss_bbox_4: 0.276  loss_giou_4: 0.545  loss_ce_dn_4: 0.0002299  loss_mask_dn_4: 0.3534  loss_dice_dn_4: 1.231  loss_bbox_dn_4: 0.2354  loss_giou_dn_4: 0.4125  loss_ce_5: 0.2109  loss_mask_5: 0.3534  loss_dice_5: 1.262  loss_bbox_5: 0.2735  loss_giou_5: 0.5236  loss_ce_dn_5: 0.0003821  loss_mask_dn_5: 0.3557  loss_dice_dn_5: 1.194  loss_bbox_dn_5: 0.2296  loss_giou_dn_5: 0.3964  loss_ce_6: 0.2209  loss_mask_6: 0.3406  loss_dice_6: 1.234  loss_bbox_6: 0.2709  loss_giou_6: 0.5263  loss_ce_dn_6: 0.0003332  loss_mask_dn_6: 0.3582  loss_dice_dn_6: 1.208  loss_bbox_dn_6: 0.2332  loss_giou_dn_6: 0.3952  loss_ce_7: 0.2219  loss_mask_7: 0.3538  loss_dice_7: 1.248  loss_bbox_7: 0.2783  loss_giou_7: 0.5242  loss_ce_dn_7: 0.0002067  loss_mask_dn_7: 0.3608  loss_dice_dn_7: 1.215  loss_bbox_dn_7: 0.2363  loss_giou_dn_7: 0.3999  loss_ce_8: 0.2116  loss_mask_8: 0.3372  loss_dice_8: 1.229  loss_bbox_8: 0.2734  loss_giou_8: 0.5406  loss_ce_dn_8: 0.0001453  loss_mask_dn_8: 0.3599  loss_dice_dn_8: 1.213  loss_bbox_dn_8: 0.238  loss_giou_dn_8: 0.4054  loss_ce_interm: 0.8186  loss_mask_interm: 0.3842  loss_dice_interm: 1.196  loss_bbox_interm: 0.3958  loss_giou_interm: 0.6539    time: 0.4934  last_time: 0.7379  data_time: 0.0059  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:37:27 d2.utils.events]:  eta: 0:01:48  iter: 3779  total_loss: 63.94  loss_ce: 0.4383  loss_mask: 0.3412  loss_dice: 1.09  loss_bbox: 0.2496  loss_giou: 0.4684  loss_ce_dn: 0.0001098  loss_mask_dn: 0.3377  loss_dice_dn: 1.151  loss_bbox_dn: 0.2361  loss_giou_dn: 0.3824  loss_ce_0: 0.6566  loss_mask_0: 0.3743  loss_dice_0: 1.166  loss_bbox_0: 0.6793  loss_giou_0: 0.9779  loss_ce_dn_0: 0.01758  loss_mask_dn_0: 0.63  loss_dice_dn_0: 2.453  loss_bbox_dn_0: 0.8276  loss_giou_dn_0: 0.8583  loss_ce_1: 0.8731  loss_mask_1: 0.3504  loss_dice_1: 1.144  loss_bbox_1: 0.3546  loss_giou_1: 0.5057  loss_ce_dn_1: 0.0004397  loss_mask_dn_1: 0.3485  loss_dice_dn_1: 1.189  loss_bbox_dn_1: 0.286  loss_giou_dn_1: 0.451  loss_ce_2: 0.5137  loss_mask_2: 0.3401  loss_dice_2: 1.123  loss_bbox_2: 0.2947  loss_giou_2: 0.4641  loss_ce_dn_2: 0.000406  loss_mask_dn_2: 0.3543  loss_dice_dn_2: 1.146  loss_bbox_dn_2: 0.2454  loss_giou_dn_2: 0.4372  loss_ce_3: 0.5683  loss_mask_3: 0.3369  loss_dice_3: 1.159  loss_bbox_3: 0.319  loss_giou_3: 0.4466  loss_ce_dn_3: 0.0003923  loss_mask_dn_3: 0.3415  loss_dice_dn_3: 1.157  loss_bbox_dn_3: 0.244  loss_giou_dn_3: 0.3972  loss_ce_4: 0.4466  loss_mask_4: 0.3381  loss_dice_4: 1.194  loss_bbox_4: 0.2972  loss_giou_4: 0.4492  loss_ce_dn_4: 0.0002351  loss_mask_dn_4: 0.3422  loss_dice_dn_4: 1.171  loss_bbox_dn_4: 0.2412  loss_giou_dn_4: 0.3884  loss_ce_5: 0.3908  loss_mask_5: 0.3527  loss_dice_5: 1.155  loss_bbox_5: 0.2728  loss_giou_5: 0.4656  loss_ce_dn_5: 0.0003147  loss_mask_dn_5: 0.3434  loss_dice_dn_5: 1.154  loss_bbox_dn_5: 0.2381  loss_giou_dn_5: 0.3878  loss_ce_6: 0.4136  loss_mask_6: 0.3446  loss_dice_6: 1.126  loss_bbox_6: 0.2538  loss_giou_6: 0.4595  loss_ce_dn_6: 0.0002644  loss_mask_dn_6: 0.3413  loss_dice_dn_6: 1.16  loss_bbox_dn_6: 0.2344  loss_giou_dn_6: 0.3858  loss_ce_7: 0.436  loss_mask_7: 0.3462  loss_dice_7: 1.124  loss_bbox_7: 0.2429  loss_giou_7: 0.457  loss_ce_dn_7: 0.0001512  loss_mask_dn_7: 0.3526  loss_dice_dn_7: 1.141  loss_bbox_dn_7: 0.233  loss_giou_dn_7: 0.3807  loss_ce_8: 0.4497  loss_mask_8: 0.3435  loss_dice_8: 1.135  loss_bbox_8: 0.2438  loss_giou_8: 0.462  loss_ce_dn_8: 0.0001302  loss_mask_dn_8: 0.339  loss_dice_dn_8: 1.153  loss_bbox_dn_8: 0.2348  loss_giou_dn_8: 0.3818  loss_ce_interm: 0.6832  loss_mask_interm: 0.3565  loss_dice_interm: 1.183  loss_bbox_interm: 0.3578  loss_giou_interm: 0.6234    time: 0.4938  last_time: 0.5569  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:37:37 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:37:37 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:37:37 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:37:37 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:37:39 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0812 s/iter. Eval: 0.0108 s/iter. Total: 0.0929 s/iter. ETA=0:00:05\n",
      "[03/14 17:37:44 d2.evaluation.evaluator]: Total inference time: 0:00:05.552293 (0.089553 s / iter per device, on 1 devices)\n",
      "[03/14 17:37:44 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.077183 s / iter per device, on 1 devices)\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.587\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.590\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 50.887 | 83.170 | 57.424 | 50.618 | 60.899 |  nan  |\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:37:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.278\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.809\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.234\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.384\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.371\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 27.826 | 80.910 | 5.974  | 26.600 | 34.622 |  nan  |\n",
      "[03/14 17:37:44 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:37:44 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: 50.8872,83.1700,57.4237,50.6184,60.8995,nan\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:37:44 d2.evaluation.testing]: copypaste: 27.8256,80.9103,5.9741,26.6005,34.6222,nan\n",
      "[03/14 17:37:44 d2.utils.events]:  eta: 0:01:38  iter: 3799  total_loss: 57.85  loss_ce: 0.3925  loss_mask: 0.2934  loss_dice: 1.213  loss_bbox: 0.2912  loss_giou: 0.5223  loss_ce_dn: 0.0001225  loss_mask_dn: 0.301  loss_dice_dn: 1.177  loss_bbox_dn: 0.2514  loss_giou_dn: 0.4184  loss_ce_0: 0.5318  loss_mask_0: 0.3137  loss_dice_0: 1.222  loss_bbox_0: 0.7077  loss_giou_0: 0.8591  loss_ce_dn_0: 0.04896  loss_mask_dn_0: 0.4716  loss_dice_dn_0: 2.68  loss_bbox_dn_0: 0.7056  loss_giou_dn_0: 0.8501  loss_ce_1: 0.5951  loss_mask_1: 0.3002  loss_dice_1: 1.232  loss_bbox_1: 0.3197  loss_giou_1: 0.5741  loss_ce_dn_1: 0.0004649  loss_mask_dn_1: 0.3047  loss_dice_dn_1: 1.199  loss_bbox_dn_1: 0.2971  loss_giou_dn_1: 0.4616  loss_ce_2: 0.4126  loss_mask_2: 0.2999  loss_dice_2: 1.254  loss_bbox_2: 0.2994  loss_giou_2: 0.5258  loss_ce_dn_2: 0.0003772  loss_mask_dn_2: 0.3104  loss_dice_dn_2: 1.214  loss_bbox_dn_2: 0.269  loss_giou_dn_2: 0.4395  loss_ce_3: 0.4403  loss_mask_3: 0.296  loss_dice_3: 1.284  loss_bbox_3: 0.2598  loss_giou_3: 0.5142  loss_ce_dn_3: 0.0003375  loss_mask_dn_3: 0.3061  loss_dice_dn_3: 1.2  loss_bbox_dn_3: 0.2551  loss_giou_dn_3: 0.4212  loss_ce_4: 0.3806  loss_mask_4: 0.3001  loss_dice_4: 1.286  loss_bbox_4: 0.2882  loss_giou_4: 0.5086  loss_ce_dn_4: 0.0002473  loss_mask_dn_4: 0.3045  loss_dice_dn_4: 1.204  loss_bbox_dn_4: 0.2548  loss_giou_dn_4: 0.4228  loss_ce_5: 0.3552  loss_mask_5: 0.2945  loss_dice_5: 1.237  loss_bbox_5: 0.2831  loss_giou_5: 0.5316  loss_ce_dn_5: 0.0003564  loss_mask_dn_5: 0.3009  loss_dice_dn_5: 1.196  loss_bbox_dn_5: 0.2503  loss_giou_dn_5: 0.4281  loss_ce_6: 0.3627  loss_mask_6: 0.2913  loss_dice_6: 1.276  loss_bbox_6: 0.2852  loss_giou_6: 0.5225  loss_ce_dn_6: 0.0002902  loss_mask_dn_6: 0.3043  loss_dice_dn_6: 1.197  loss_bbox_dn_6: 0.2493  loss_giou_dn_6: 0.426  loss_ce_7: 0.3633  loss_mask_7: 0.2908  loss_dice_7: 1.306  loss_bbox_7: 0.3104  loss_giou_7: 0.5245  loss_ce_dn_7: 0.0001483  loss_mask_dn_7: 0.304  loss_dice_dn_7: 1.17  loss_bbox_dn_7: 0.2515  loss_giou_dn_7: 0.4207  loss_ce_8: 0.4644  loss_mask_8: 0.282  loss_dice_8: 1.23  loss_bbox_8: 0.2892  loss_giou_8: 0.5143  loss_ce_dn_8: 0.0001212  loss_mask_dn_8: 0.3003  loss_dice_dn_8: 1.183  loss_bbox_dn_8: 0.2548  loss_giou_dn_8: 0.4147  loss_ce_interm: 0.5955  loss_mask_interm: 0.3162  loss_dice_interm: 1.244  loss_bbox_interm: 0.3502  loss_giou_interm: 0.6106    time: 0.4937  last_time: 0.5296  data_time: 0.0035  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:37:54 d2.utils.events]:  eta: 0:01:28  iter: 3819  total_loss: 55.77  loss_ce: 0.3222  loss_mask: 0.3044  loss_dice: 1.177  loss_bbox: 0.2667  loss_giou: 0.4711  loss_ce_dn: 6.957e-05  loss_mask_dn: 0.3176  loss_dice_dn: 1.166  loss_bbox_dn: 0.2056  loss_giou_dn: 0.4256  loss_ce_0: 0.6041  loss_mask_0: 0.3176  loss_dice_0: 1.414  loss_bbox_0: 0.6748  loss_giou_0: 1.007  loss_ce_dn_0: 0.01746  loss_mask_dn_0: 0.4209  loss_dice_dn_0: 2.304  loss_bbox_dn_0: 0.7993  loss_giou_dn_0: 0.8474  loss_ce_1: 0.6192  loss_mask_1: 0.3089  loss_dice_1: 1.238  loss_bbox_1: 0.3287  loss_giou_1: 0.5368  loss_ce_dn_1: 0.0003583  loss_mask_dn_1: 0.318  loss_dice_dn_1: 1.234  loss_bbox_dn_1: 0.2487  loss_giou_dn_1: 0.4526  loss_ce_2: 0.5675  loss_mask_2: 0.3079  loss_dice_2: 1.21  loss_bbox_2: 0.2799  loss_giou_2: 0.5183  loss_ce_dn_2: 0.0003205  loss_mask_dn_2: 0.3135  loss_dice_dn_2: 1.211  loss_bbox_dn_2: 0.2333  loss_giou_dn_2: 0.4458  loss_ce_3: 0.4277  loss_mask_3: 0.3152  loss_dice_3: 1.258  loss_bbox_3: 0.2833  loss_giou_3: 0.5312  loss_ce_dn_3: 0.0002913  loss_mask_dn_3: 0.3161  loss_dice_dn_3: 1.198  loss_bbox_dn_3: 0.216  loss_giou_dn_3: 0.4429  loss_ce_4: 0.3058  loss_mask_4: 0.3105  loss_dice_4: 1.226  loss_bbox_4: 0.2931  loss_giou_4: 0.5119  loss_ce_dn_4: 0.0001654  loss_mask_dn_4: 0.3178  loss_dice_dn_4: 1.187  loss_bbox_dn_4: 0.2038  loss_giou_dn_4: 0.4265  loss_ce_5: 0.3721  loss_mask_5: 0.3188  loss_dice_5: 1.196  loss_bbox_5: 0.2749  loss_giou_5: 0.4815  loss_ce_dn_5: 0.0001689  loss_mask_dn_5: 0.3155  loss_dice_dn_5: 1.184  loss_bbox_dn_5: 0.2095  loss_giou_dn_5: 0.4313  loss_ce_6: 0.3123  loss_mask_6: 0.3016  loss_dice_6: 1.237  loss_bbox_6: 0.2649  loss_giou_6: 0.4566  loss_ce_dn_6: 0.0001614  loss_mask_dn_6: 0.3195  loss_dice_dn_6: 1.184  loss_bbox_dn_6: 0.2061  loss_giou_dn_6: 0.4339  loss_ce_7: 0.311  loss_mask_7: 0.2929  loss_dice_7: 1.146  loss_bbox_7: 0.2634  loss_giou_7: 0.4729  loss_ce_dn_7: 8.153e-05  loss_mask_dn_7: 0.3247  loss_dice_dn_7: 1.177  loss_bbox_dn_7: 0.2102  loss_giou_dn_7: 0.431  loss_ce_8: 0.3167  loss_mask_8: 0.3154  loss_dice_8: 1.215  loss_bbox_8: 0.264  loss_giou_8: 0.476  loss_ce_dn_8: 7.656e-05  loss_mask_dn_8: 0.3209  loss_dice_dn_8: 1.164  loss_bbox_dn_8: 0.2061  loss_giou_dn_8: 0.4252  loss_ce_interm: 0.6929  loss_mask_interm: 0.3075  loss_dice_interm: 1.209  loss_bbox_interm: 0.3253  loss_giou_interm: 0.5831    time: 0.4937  last_time: 0.4857  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:38:04 d2.utils.events]:  eta: 0:01:18  iter: 3839  total_loss: 66.64  loss_ce: 0.4379  loss_mask: 0.291  loss_dice: 1.484  loss_bbox: 0.3252  loss_giou: 0.6159  loss_ce_dn: 0.0001827  loss_mask_dn: 0.2751  loss_dice_dn: 1.385  loss_bbox_dn: 0.2051  loss_giou_dn: 0.4883  loss_ce_0: 0.6499  loss_mask_0: 0.2779  loss_dice_0: 1.488  loss_bbox_0: 0.625  loss_giou_0: 1.124  loss_ce_dn_0: 0.05686  loss_mask_dn_0: 0.3495  loss_dice_dn_0: 3.097  loss_bbox_dn_0: 0.598  loss_giou_dn_0: 0.8582  loss_ce_1: 0.666  loss_mask_1: 0.2926  loss_dice_1: 1.591  loss_bbox_1: 0.3716  loss_giou_1: 0.7894  loss_ce_dn_1: 0.0003003  loss_mask_dn_1: 0.2723  loss_dice_dn_1: 1.479  loss_bbox_dn_1: 0.2554  loss_giou_dn_1: 0.5666  loss_ce_2: 0.4729  loss_mask_2: 0.2785  loss_dice_2: 1.565  loss_bbox_2: 0.3354  loss_giou_2: 0.6904  loss_ce_dn_2: 0.0002675  loss_mask_dn_2: 0.262  loss_dice_dn_2: 1.449  loss_bbox_dn_2: 0.2293  loss_giou_dn_2: 0.5502  loss_ce_3: 0.5593  loss_mask_3: 0.2839  loss_dice_3: 1.559  loss_bbox_3: 0.3541  loss_giou_3: 0.6881  loss_ce_dn_3: 0.000319  loss_mask_dn_3: 0.2664  loss_dice_dn_3: 1.42  loss_bbox_dn_3: 0.2092  loss_giou_dn_3: 0.5194  loss_ce_4: 0.4385  loss_mask_4: 0.2791  loss_dice_4: 1.653  loss_bbox_4: 0.3667  loss_giou_4: 0.6684  loss_ce_dn_4: 0.0002366  loss_mask_dn_4: 0.2676  loss_dice_dn_4: 1.395  loss_bbox_dn_4: 0.2131  loss_giou_dn_4: 0.4986  loss_ce_5: 0.4373  loss_mask_5: 0.2792  loss_dice_5: 1.462  loss_bbox_5: 0.3352  loss_giou_5: 0.6739  loss_ce_dn_5: 0.0002969  loss_mask_dn_5: 0.2762  loss_dice_dn_5: 1.426  loss_bbox_dn_5: 0.2082  loss_giou_dn_5: 0.4977  loss_ce_6: 0.4314  loss_mask_6: 0.2843  loss_dice_6: 1.514  loss_bbox_6: 0.3372  loss_giou_6: 0.6533  loss_ce_dn_6: 0.0002966  loss_mask_dn_6: 0.2808  loss_dice_dn_6: 1.432  loss_bbox_dn_6: 0.2057  loss_giou_dn_6: 0.4903  loss_ce_7: 0.4351  loss_mask_7: 0.2876  loss_dice_7: 1.538  loss_bbox_7: 0.3394  loss_giou_7: 0.685  loss_ce_dn_7: 0.0001783  loss_mask_dn_7: 0.2766  loss_dice_dn_7: 1.393  loss_bbox_dn_7: 0.2035  loss_giou_dn_7: 0.4979  loss_ce_8: 0.4343  loss_mask_8: 0.2911  loss_dice_8: 1.497  loss_bbox_8: 0.3398  loss_giou_8: 0.6091  loss_ce_dn_8: 0.0001719  loss_mask_dn_8: 0.2759  loss_dice_dn_8: 1.377  loss_bbox_dn_8: 0.2082  loss_giou_dn_8: 0.487  loss_ce_interm: 0.6762  loss_mask_interm: 0.2683  loss_dice_interm: 1.587  loss_bbox_interm: 0.3243  loss_giou_interm: 0.7734    time: 0.4938  last_time: 0.5005  data_time: 0.0043  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:38:14 d2.utils.events]:  eta: 0:01:08  iter: 3859  total_loss: 56.77  loss_ce: 0.1811  loss_mask: 0.3805  loss_dice: 1.156  loss_bbox: 0.2857  loss_giou: 0.5478  loss_ce_dn: 7.074e-05  loss_mask_dn: 0.373  loss_dice_dn: 1.187  loss_bbox_dn: 0.2405  loss_giou_dn: 0.4257  loss_ce_0: 0.5574  loss_mask_0: 0.3897  loss_dice_0: 1.169  loss_bbox_0: 0.6818  loss_giou_0: 0.9505  loss_ce_dn_0: 0.01734  loss_mask_dn_0: 0.5624  loss_dice_dn_0: 2.453  loss_bbox_dn_0: 0.8299  loss_giou_dn_0: 0.847  loss_ce_1: 0.639  loss_mask_1: 0.384  loss_dice_1: 1.204  loss_bbox_1: 0.3143  loss_giou_1: 0.567  loss_ce_dn_1: 0.0002914  loss_mask_dn_1: 0.3645  loss_dice_dn_1: 1.204  loss_bbox_dn_1: 0.3578  loss_giou_dn_1: 0.5188  loss_ce_2: 0.3464  loss_mask_2: 0.3728  loss_dice_2: 1.171  loss_bbox_2: 0.2992  loss_giou_2: 0.5943  loss_ce_dn_2: 0.0003128  loss_mask_dn_2: 0.358  loss_dice_dn_2: 1.176  loss_bbox_dn_2: 0.3327  loss_giou_dn_2: 0.4505  loss_ce_3: 0.2222  loss_mask_3: 0.3752  loss_dice_3: 1.209  loss_bbox_3: 0.3256  loss_giou_3: 0.5702  loss_ce_dn_3: 0.0002957  loss_mask_dn_3: 0.363  loss_dice_dn_3: 1.17  loss_bbox_dn_3: 0.2653  loss_giou_dn_3: 0.4222  loss_ce_4: 0.1536  loss_mask_4: 0.3748  loss_dice_4: 1.21  loss_bbox_4: 0.2944  loss_giou_4: 0.5566  loss_ce_dn_4: 0.0001632  loss_mask_dn_4: 0.3662  loss_dice_dn_4: 1.176  loss_bbox_dn_4: 0.2451  loss_giou_dn_4: 0.4164  loss_ce_5: 0.1529  loss_mask_5: 0.3757  loss_dice_5: 1.218  loss_bbox_5: 0.3018  loss_giou_5: 0.5361  loss_ce_dn_5: 0.0002469  loss_mask_dn_5: 0.3641  loss_dice_dn_5: 1.182  loss_bbox_dn_5: 0.2475  loss_giou_dn_5: 0.428  loss_ce_6: 0.168  loss_mask_6: 0.3791  loss_dice_6: 1.204  loss_bbox_6: 0.2877  loss_giou_6: 0.5396  loss_ce_dn_6: 0.0002362  loss_mask_dn_6: 0.3632  loss_dice_dn_6: 1.18  loss_bbox_dn_6: 0.231  loss_giou_dn_6: 0.4282  loss_ce_7: 0.1751  loss_mask_7: 0.3718  loss_dice_7: 1.201  loss_bbox_7: 0.2888  loss_giou_7: 0.542  loss_ce_dn_7: 0.0001211  loss_mask_dn_7: 0.3645  loss_dice_dn_7: 1.185  loss_bbox_dn_7: 0.2338  loss_giou_dn_7: 0.4246  loss_ce_8: 0.1842  loss_mask_8: 0.3791  loss_dice_8: 1.24  loss_bbox_8: 0.2853  loss_giou_8: 0.5434  loss_ce_dn_8: 8.205e-05  loss_mask_dn_8: 0.3693  loss_dice_dn_8: 1.196  loss_bbox_dn_8: 0.2411  loss_giou_dn_8: 0.4239  loss_ce_interm: 0.6071  loss_mask_interm: 0.3855  loss_dice_interm: 1.249  loss_bbox_interm: 0.4105  loss_giou_interm: 0.6766    time: 0.4938  last_time: 0.4994  data_time: 0.0036  last_data_time: 0.0040   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:38:25 d2.utils.events]:  eta: 0:00:59  iter: 3879  total_loss: 58.03  loss_ce: 0.3536  loss_mask: 0.3101  loss_dice: 1.276  loss_bbox: 0.238  loss_giou: 0.4512  loss_ce_dn: 0.0001091  loss_mask_dn: 0.3159  loss_dice_dn: 1.26  loss_bbox_dn: 0.2027  loss_giou_dn: 0.3897  loss_ce_0: 0.542  loss_mask_0: 0.3083  loss_dice_0: 1.323  loss_bbox_0: 0.7049  loss_giou_0: 1.019  loss_ce_dn_0: 0.05675  loss_mask_dn_0: 0.6255  loss_dice_dn_0: 2.706  loss_bbox_dn_0: 0.7437  loss_giou_dn_0: 0.856  loss_ce_1: 0.6206  loss_mask_1: 0.2987  loss_dice_1: 1.321  loss_bbox_1: 0.3014  loss_giou_1: 0.5248  loss_ce_dn_1: 0.000624  loss_mask_dn_1: 0.3134  loss_dice_dn_1: 1.297  loss_bbox_dn_1: 0.253  loss_giou_dn_1: 0.4433  loss_ce_2: 0.4759  loss_mask_2: 0.2998  loss_dice_2: 1.233  loss_bbox_2: 0.2703  loss_giou_2: 0.4831  loss_ce_dn_2: 0.0004286  loss_mask_dn_2: 0.305  loss_dice_dn_2: 1.274  loss_bbox_dn_2: 0.2142  loss_giou_dn_2: 0.4144  loss_ce_3: 0.302  loss_mask_3: 0.2911  loss_dice_3: 1.287  loss_bbox_3: 0.238  loss_giou_3: 0.4534  loss_ce_dn_3: 0.0003402  loss_mask_dn_3: 0.3027  loss_dice_dn_3: 1.275  loss_bbox_dn_3: 0.2082  loss_giou_dn_3: 0.4093  loss_ce_4: 0.3146  loss_mask_4: 0.2882  loss_dice_4: 1.289  loss_bbox_4: 0.2643  loss_giou_4: 0.461  loss_ce_dn_4: 0.0002117  loss_mask_dn_4: 0.2991  loss_dice_dn_4: 1.285  loss_bbox_dn_4: 0.2016  loss_giou_dn_4: 0.3919  loss_ce_5: 0.2896  loss_mask_5: 0.2895  loss_dice_5: 1.311  loss_bbox_5: 0.2311  loss_giou_5: 0.4541  loss_ce_dn_5: 0.0002145  loss_mask_dn_5: 0.3086  loss_dice_dn_5: 1.277  loss_bbox_dn_5: 0.2001  loss_giou_dn_5: 0.3914  loss_ce_6: 0.3276  loss_mask_6: 0.3166  loss_dice_6: 1.291  loss_bbox_6: 0.235  loss_giou_6: 0.4541  loss_ce_dn_6: 0.0002578  loss_mask_dn_6: 0.3151  loss_dice_dn_6: 1.268  loss_bbox_dn_6: 0.1994  loss_giou_dn_6: 0.3886  loss_ce_7: 0.2971  loss_mask_7: 0.3141  loss_dice_7: 1.253  loss_bbox_7: 0.2317  loss_giou_7: 0.4831  loss_ce_dn_7: 0.0001453  loss_mask_dn_7: 0.3179  loss_dice_dn_7: 1.263  loss_bbox_dn_7: 0.2012  loss_giou_dn_7: 0.3902  loss_ce_8: 0.3507  loss_mask_8: 0.3037  loss_dice_8: 1.295  loss_bbox_8: 0.2385  loss_giou_8: 0.4506  loss_ce_dn_8: 0.000118  loss_mask_dn_8: 0.3124  loss_dice_dn_8: 1.272  loss_bbox_dn_8: 0.2062  loss_giou_dn_8: 0.3904  loss_ce_interm: 0.5057  loss_mask_interm: 0.3094  loss_dice_interm: 1.318  loss_bbox_interm: 0.418  loss_giou_interm: 0.5733    time: 0.4941  last_time: 0.5879  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:38:38 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:38:38 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:38:38 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:38:38 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:38:39 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0009 s/iter. Inference: 0.0715 s/iter. Eval: 0.0112 s/iter. Total: 0.0836 s/iter. ETA=0:00:04\n",
      "[03/14 17:38:44 d2.evaluation.evaluator]: Total inference time: 0:00:05.421322 (0.087441 s / iter per device, on 1 devices)\n",
      "[03/14 17:38:44 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.075291 s / iter per device, on 1 devices)\n",
      "[03/14 17:38:44 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:38:44 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:38:44 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:38:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:38:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:38:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:38:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.514\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.852\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.520\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.513\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.615\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.634\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.597\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:38:44 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 51.418 | 85.159 | 51.988 | 51.286 | 61.457 |  nan  |\n",
      "[03/14 17:38:44 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:38:45 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:38:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[03/14 17:38:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:38:45 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.822\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.263\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.364\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.350\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:38:45 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 28.360 | 82.225 | 5.983  | 26.305 | 36.412 |  nan  |\n",
      "[03/14 17:38:45 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:38:45 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: 51.4177,85.1592,51.9880,51.2856,61.4575,nan\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:38:45 d2.evaluation.testing]: copypaste: 28.3603,82.2252,5.9835,26.3048,36.4124,nan\n",
      "[03/14 17:38:45 d2.utils.events]:  eta: 0:00:49  iter: 3899  total_loss: 55.75  loss_ce: 0.07478  loss_mask: 0.3495  loss_dice: 1.104  loss_bbox: 0.3058  loss_giou: 0.4143  loss_ce_dn: 6.523e-05  loss_mask_dn: 0.3694  loss_dice_dn: 1.082  loss_bbox_dn: 0.2594  loss_giou_dn: 0.3464  loss_ce_0: 0.5904  loss_mask_0: 0.3616  loss_dice_0: 1.122  loss_bbox_0: 0.8046  loss_giou_0: 0.8747  loss_ce_dn_0: 0.0173  loss_mask_dn_0: 0.5314  loss_dice_dn_0: 2.018  loss_bbox_dn_0: 0.9198  loss_giou_dn_0: 0.8542  loss_ce_1: 0.6143  loss_mask_1: 0.3489  loss_dice_1: 1.126  loss_bbox_1: 0.3068  loss_giou_1: 0.467  loss_ce_dn_1: 0.0004061  loss_mask_dn_1: 0.3625  loss_dice_dn_1: 1.154  loss_bbox_dn_1: 0.2981  loss_giou_dn_1: 0.4347  loss_ce_2: 0.282  loss_mask_2: 0.3635  loss_dice_2: 1.088  loss_bbox_2: 0.2913  loss_giou_2: 0.4448  loss_ce_dn_2: 0.0003276  loss_mask_dn_2: 0.365  loss_dice_dn_2: 1.111  loss_bbox_dn_2: 0.2725  loss_giou_dn_2: 0.3865  loss_ce_3: 0.1456  loss_mask_3: 0.3658  loss_dice_3: 1.107  loss_bbox_3: 0.29  loss_giou_3: 0.4087  loss_ce_dn_3: 0.0002907  loss_mask_dn_3: 0.3684  loss_dice_dn_3: 1.091  loss_bbox_dn_3: 0.2542  loss_giou_dn_3: 0.3504  loss_ce_4: 0.09816  loss_mask_4: 0.3345  loss_dice_4: 1.076  loss_bbox_4: 0.2923  loss_giou_4: 0.4316  loss_ce_dn_4: 0.0001441  loss_mask_dn_4: 0.3617  loss_dice_dn_4: 1.092  loss_bbox_dn_4: 0.258  loss_giou_dn_4: 0.3446  loss_ce_5: 0.09118  loss_mask_5: 0.3536  loss_dice_5: 1.089  loss_bbox_5: 0.3154  loss_giou_5: 0.4072  loss_ce_dn_5: 0.0001869  loss_mask_dn_5: 0.368  loss_dice_dn_5: 1.086  loss_bbox_dn_5: 0.2591  loss_giou_dn_5: 0.3413  loss_ce_6: 0.08305  loss_mask_6: 0.3491  loss_dice_6: 1.148  loss_bbox_6: 0.3146  loss_giou_6: 0.4188  loss_ce_dn_6: 0.0001749  loss_mask_dn_6: 0.3659  loss_dice_dn_6: 1.081  loss_bbox_dn_6: 0.2627  loss_giou_dn_6: 0.3386  loss_ce_7: 0.06451  loss_mask_7: 0.3434  loss_dice_7: 1.05  loss_bbox_7: 0.3097  loss_giou_7: 0.4145  loss_ce_dn_7: 8.85e-05  loss_mask_dn_7: 0.3684  loss_dice_dn_7: 1.086  loss_bbox_dn_7: 0.2622  loss_giou_dn_7: 0.3478  loss_ce_8: 0.07565  loss_mask_8: 0.3404  loss_dice_8: 1.138  loss_bbox_8: 0.3045  loss_giou_8: 0.4108  loss_ce_dn_8: 6.461e-05  loss_mask_dn_8: 0.3692  loss_dice_dn_8: 1.086  loss_bbox_dn_8: 0.2594  loss_giou_dn_8: 0.3436  loss_ce_interm: 0.5845  loss_mask_interm: 0.3579  loss_dice_interm: 1.082  loss_bbox_interm: 0.3369  loss_giou_interm: 0.518    time: 0.4948  last_time: 0.5278  data_time: 0.0037  last_data_time: 0.0033   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:38:54 d2.utils.events]:  eta: 0:00:39  iter: 3919  total_loss: 69.64  loss_ce: 0.3681  loss_mask: 0.3403  loss_dice: 1.457  loss_bbox: 0.3179  loss_giou: 0.5551  loss_ce_dn: 0.0002578  loss_mask_dn: 0.3453  loss_dice_dn: 1.31  loss_bbox_dn: 0.2564  loss_giou_dn: 0.4705  loss_ce_0: 0.7799  loss_mask_0: 0.3784  loss_dice_0: 1.412  loss_bbox_0: 0.7048  loss_giou_0: 1.103  loss_ce_dn_0: 0.01733  loss_mask_dn_0: 0.4622  loss_dice_dn_0: 2.781  loss_bbox_dn_0: 0.8795  loss_giou_dn_0: 0.8524  loss_ce_1: 0.7573  loss_mask_1: 0.3635  loss_dice_1: 1.445  loss_bbox_1: 0.2927  loss_giou_1: 0.6462  loss_ce_dn_1: 0.0002831  loss_mask_dn_1: 0.3523  loss_dice_dn_1: 1.398  loss_bbox_dn_1: 0.3196  loss_giou_dn_1: 0.5208  loss_ce_2: 0.524  loss_mask_2: 0.3608  loss_dice_2: 1.391  loss_bbox_2: 0.3406  loss_giou_2: 0.5538  loss_ce_dn_2: 0.0002823  loss_mask_dn_2: 0.3431  loss_dice_dn_2: 1.329  loss_bbox_dn_2: 0.2729  loss_giou_dn_2: 0.4915  loss_ce_3: 0.4053  loss_mask_3: 0.348  loss_dice_3: 1.389  loss_bbox_3: 0.3063  loss_giou_3: 0.6055  loss_ce_dn_3: 0.0003221  loss_mask_dn_3: 0.3381  loss_dice_dn_3: 1.321  loss_bbox_dn_3: 0.2567  loss_giou_dn_3: 0.4569  loss_ce_4: 0.3033  loss_mask_4: 0.344  loss_dice_4: 1.376  loss_bbox_4: 0.3126  loss_giou_4: 0.6227  loss_ce_dn_4: 0.0002228  loss_mask_dn_4: 0.3438  loss_dice_dn_4: 1.325  loss_bbox_dn_4: 0.259  loss_giou_dn_4: 0.4447  loss_ce_5: 0.2995  loss_mask_5: 0.3379  loss_dice_5: 1.427  loss_bbox_5: 0.2914  loss_giou_5: 0.6207  loss_ce_dn_5: 0.0003739  loss_mask_dn_5: 0.3388  loss_dice_dn_5: 1.307  loss_bbox_dn_5: 0.2589  loss_giou_dn_5: 0.459  loss_ce_6: 0.3542  loss_mask_6: 0.361  loss_dice_6: 1.401  loss_bbox_6: 0.3275  loss_giou_6: 0.6316  loss_ce_dn_6: 0.0003085  loss_mask_dn_6: 0.3481  loss_dice_dn_6: 1.331  loss_bbox_dn_6: 0.2571  loss_giou_dn_6: 0.4646  loss_ce_7: 0.34  loss_mask_7: 0.363  loss_dice_7: 1.477  loss_bbox_7: 0.305  loss_giou_7: 0.5449  loss_ce_dn_7: 0.0002068  loss_mask_dn_7: 0.3459  loss_dice_dn_7: 1.306  loss_bbox_dn_7: 0.2618  loss_giou_dn_7: 0.4747  loss_ce_8: 0.3611  loss_mask_8: 0.3566  loss_dice_8: 1.408  loss_bbox_8: 0.3196  loss_giou_8: 0.5578  loss_ce_dn_8: 0.0002326  loss_mask_dn_8: 0.3452  loss_dice_dn_8: 1.308  loss_bbox_dn_8: 0.2592  loss_giou_dn_8: 0.4691  loss_ce_interm: 0.7799  loss_mask_interm: 0.3693  loss_dice_interm: 1.456  loss_bbox_interm: 0.3961  loss_giou_interm: 0.687    time: 0.4947  last_time: 0.4633  data_time: 0.0034  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:39:04 d2.utils.events]:  eta: 0:00:29  iter: 3939  total_loss: 65.14  loss_ce: 0.3988  loss_mask: 0.3971  loss_dice: 1.391  loss_bbox: 0.3006  loss_giou: 0.5025  loss_ce_dn: 0.0001709  loss_mask_dn: 0.4028  loss_dice_dn: 1.319  loss_bbox_dn: 0.2595  loss_giou_dn: 0.4509  loss_ce_0: 0.7082  loss_mask_0: 0.4084  loss_dice_0: 1.479  loss_bbox_0: 0.62  loss_giou_0: 0.8447  loss_ce_dn_0: 0.0291  loss_mask_dn_0: 0.6276  loss_dice_dn_0: 2.764  loss_bbox_dn_0: 0.7837  loss_giou_dn_0: 0.8383  loss_ce_1: 0.7145  loss_mask_1: 0.4151  loss_dice_1: 1.407  loss_bbox_1: 0.3758  loss_giou_1: 0.5376  loss_ce_dn_1: 0.000428  loss_mask_dn_1: 0.4474  loss_dice_dn_1: 1.401  loss_bbox_dn_1: 0.3568  loss_giou_dn_1: 0.498  loss_ce_2: 0.4749  loss_mask_2: 0.4585  loss_dice_2: 1.455  loss_bbox_2: 0.3325  loss_giou_2: 0.532  loss_ce_dn_2: 0.0003648  loss_mask_dn_2: 0.3991  loss_dice_dn_2: 1.369  loss_bbox_dn_2: 0.3223  loss_giou_dn_2: 0.4717  loss_ce_3: 0.3707  loss_mask_3: 0.4278  loss_dice_3: 1.463  loss_bbox_3: 0.3081  loss_giou_3: 0.5299  loss_ce_dn_3: 0.0003211  loss_mask_dn_3: 0.3999  loss_dice_dn_3: 1.32  loss_bbox_dn_3: 0.2639  loss_giou_dn_3: 0.4517  loss_ce_4: 0.3361  loss_mask_4: 0.4446  loss_dice_4: 1.396  loss_bbox_4: 0.3352  loss_giou_4: 0.5244  loss_ce_dn_4: 0.0001871  loss_mask_dn_4: 0.3979  loss_dice_dn_4: 1.329  loss_bbox_dn_4: 0.2628  loss_giou_dn_4: 0.457  loss_ce_5: 0.3942  loss_mask_5: 0.4239  loss_dice_5: 1.362  loss_bbox_5: 0.2901  loss_giou_5: 0.5102  loss_ce_dn_5: 0.0003267  loss_mask_dn_5: 0.3938  loss_dice_dn_5: 1.326  loss_bbox_dn_5: 0.2793  loss_giou_dn_5: 0.4486  loss_ce_6: 0.3732  loss_mask_6: 0.421  loss_dice_6: 1.336  loss_bbox_6: 0.2935  loss_giou_6: 0.5048  loss_ce_dn_6: 0.0002862  loss_mask_dn_6: 0.4099  loss_dice_dn_6: 1.311  loss_bbox_dn_6: 0.2725  loss_giou_dn_6: 0.4508  loss_ce_7: 0.3661  loss_mask_7: 0.3857  loss_dice_7: 1.356  loss_bbox_7: 0.297  loss_giou_7: 0.514  loss_ce_dn_7: 0.0002457  loss_mask_dn_7: 0.4087  loss_dice_dn_7: 1.316  loss_bbox_dn_7: 0.2674  loss_giou_dn_7: 0.4525  loss_ce_8: 0.3533  loss_mask_8: 0.3791  loss_dice_8: 1.358  loss_bbox_8: 0.2988  loss_giou_8: 0.5028  loss_ce_dn_8: 0.0001931  loss_mask_dn_8: 0.407  loss_dice_dn_8: 1.318  loss_bbox_dn_8: 0.2564  loss_giou_dn_8: 0.452  loss_ce_interm: 0.7494  loss_mask_interm: 0.4105  loss_dice_interm: 1.464  loss_bbox_interm: 0.3865  loss_giou_interm: 0.6386    time: 0.4947  last_time: 0.4447  data_time: 0.0034  last_data_time: 0.0031   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:39:14 d2.utils.events]:  eta: 0:00:19  iter: 3959  total_loss: 61.82  loss_ce: 0.2472  loss_mask: 0.26  loss_dice: 1.291  loss_bbox: 0.2829  loss_giou: 0.4685  loss_ce_dn: 0.0001546  loss_mask_dn: 0.2603  loss_dice_dn: 1.264  loss_bbox_dn: 0.1804  loss_giou_dn: 0.4168  loss_ce_0: 0.7053  loss_mask_0: 0.2711  loss_dice_0: 1.294  loss_bbox_0: 0.771  loss_giou_0: 1.013  loss_ce_dn_0: 0.04863  loss_mask_dn_0: 0.4764  loss_dice_dn_0: 2.857  loss_bbox_dn_0: 0.6043  loss_giou_dn_0: 0.8517  loss_ce_1: 0.764  loss_mask_1: 0.2592  loss_dice_1: 1.301  loss_bbox_1: 0.331  loss_giou_1: 0.6745  loss_ce_dn_1: 0.000354  loss_mask_dn_1: 0.267  loss_dice_dn_1: 1.309  loss_bbox_dn_1: 0.2752  loss_giou_dn_1: 0.5105  loss_ce_2: 0.5394  loss_mask_2: 0.2853  loss_dice_2: 1.381  loss_bbox_2: 0.3274  loss_giou_2: 0.5543  loss_ce_dn_2: 0.0002856  loss_mask_dn_2: 0.2623  loss_dice_dn_2: 1.297  loss_bbox_dn_2: 0.2465  loss_giou_dn_2: 0.4621  loss_ce_3: 0.4077  loss_mask_3: 0.2398  loss_dice_3: 1.351  loss_bbox_3: 0.3176  loss_giou_3: 0.5323  loss_ce_dn_3: 0.0002782  loss_mask_dn_3: 0.2718  loss_dice_dn_3: 1.273  loss_bbox_dn_3: 0.2047  loss_giou_dn_3: 0.4342  loss_ce_4: 0.2626  loss_mask_4: 0.2641  loss_dice_4: 1.32  loss_bbox_4: 0.299  loss_giou_4: 0.4973  loss_ce_dn_4: 0.0001906  loss_mask_dn_4: 0.2656  loss_dice_dn_4: 1.256  loss_bbox_dn_4: 0.1953  loss_giou_dn_4: 0.428  loss_ce_5: 0.2431  loss_mask_5: 0.253  loss_dice_5: 1.218  loss_bbox_5: 0.2801  loss_giou_5: 0.4889  loss_ce_dn_5: 0.0002927  loss_mask_dn_5: 0.2487  loss_dice_dn_5: 1.289  loss_bbox_dn_5: 0.1878  loss_giou_dn_5: 0.4208  loss_ce_6: 0.2229  loss_mask_6: 0.2619  loss_dice_6: 1.259  loss_bbox_6: 0.2919  loss_giou_6: 0.488  loss_ce_dn_6: 0.0002951  loss_mask_dn_6: 0.26  loss_dice_dn_6: 1.244  loss_bbox_dn_6: 0.1887  loss_giou_dn_6: 0.4164  loss_ce_7: 0.2122  loss_mask_7: 0.2496  loss_dice_7: 1.258  loss_bbox_7: 0.269  loss_giou_7: 0.4809  loss_ce_dn_7: 0.0002155  loss_mask_dn_7: 0.258  loss_dice_dn_7: 1.256  loss_bbox_dn_7: 0.1885  loss_giou_dn_7: 0.4163  loss_ce_8: 0.2408  loss_mask_8: 0.2585  loss_dice_8: 1.275  loss_bbox_8: 0.2785  loss_giou_8: 0.4611  loss_ce_dn_8: 0.0001628  loss_mask_dn_8: 0.2611  loss_dice_dn_8: 1.25  loss_bbox_dn_8: 0.1839  loss_giou_dn_8: 0.4153  loss_ce_interm: 0.6703  loss_mask_interm: 0.2649  loss_dice_interm: 1.324  loss_bbox_interm: 0.3616  loss_giou_interm: 0.6708    time: 0.4947  last_time: 0.4970  data_time: 0.0034  last_data_time: 0.0032   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:39:24 d2.utils.events]:  eta: 0:00:09  iter: 3979  total_loss: 64.81  loss_ce: 0.6277  loss_mask: 0.2694  loss_dice: 1.241  loss_bbox: 0.2324  loss_giou: 0.5765  loss_ce_dn: 0.0002806  loss_mask_dn: 0.2812  loss_dice_dn: 1.171  loss_bbox_dn: 0.2066  loss_giou_dn: 0.4525  loss_ce_0: 0.601  loss_mask_0: 0.2872  loss_dice_0: 1.354  loss_bbox_0: 0.6283  loss_giou_0: 1.04  loss_ce_dn_0: 0.01735  loss_mask_dn_0: 0.4276  loss_dice_dn_0: 2.847  loss_bbox_dn_0: 0.7513  loss_giou_dn_0: 0.8449  loss_ce_1: 0.6882  loss_mask_1: 0.2611  loss_dice_1: 1.29  loss_bbox_1: 0.2901  loss_giou_1: 0.6435  loss_ce_dn_1: 0.0004262  loss_mask_dn_1: 0.2807  loss_dice_dn_1: 1.187  loss_bbox_dn_1: 0.2765  loss_giou_dn_1: 0.5236  loss_ce_2: 0.6952  loss_mask_2: 0.2716  loss_dice_2: 1.245  loss_bbox_2: 0.2901  loss_giou_2: 0.567  loss_ce_dn_2: 0.0003263  loss_mask_dn_2: 0.2811  loss_dice_dn_2: 1.188  loss_bbox_dn_2: 0.2285  loss_giou_dn_2: 0.463  loss_ce_3: 0.5049  loss_mask_3: 0.2655  loss_dice_3: 1.322  loss_bbox_3: 0.2783  loss_giou_3: 0.5391  loss_ce_dn_3: 0.0003024  loss_mask_dn_3: 0.2726  loss_dice_dn_3: 1.174  loss_bbox_dn_3: 0.2135  loss_giou_dn_3: 0.4411  loss_ce_4: 0.4108  loss_mask_4: 0.2693  loss_dice_4: 1.198  loss_bbox_4: 0.2699  loss_giou_4: 0.5398  loss_ce_dn_4: 0.0002126  loss_mask_dn_4: 0.2755  loss_dice_dn_4: 1.166  loss_bbox_dn_4: 0.1973  loss_giou_dn_4: 0.4333  loss_ce_5: 0.598  loss_mask_5: 0.264  loss_dice_5: 1.248  loss_bbox_5: 0.2624  loss_giou_5: 0.6139  loss_ce_dn_5: 0.0003187  loss_mask_dn_5: 0.2762  loss_dice_dn_5: 1.163  loss_bbox_dn_5: 0.2037  loss_giou_dn_5: 0.45  loss_ce_6: 0.4711  loss_mask_6: 0.2747  loss_dice_6: 1.223  loss_bbox_6: 0.2557  loss_giou_6: 0.563  loss_ce_dn_6: 0.0003473  loss_mask_dn_6: 0.2824  loss_dice_dn_6: 1.187  loss_bbox_dn_6: 0.201  loss_giou_dn_6: 0.4529  loss_ce_7: 0.4401  loss_mask_7: 0.2721  loss_dice_7: 1.198  loss_bbox_7: 0.2456  loss_giou_7: 0.5746  loss_ce_dn_7: 0.0002695  loss_mask_dn_7: 0.2833  loss_dice_dn_7: 1.187  loss_bbox_dn_7: 0.1989  loss_giou_dn_7: 0.4541  loss_ce_8: 0.4699  loss_mask_8: 0.2577  loss_dice_8: 1.282  loss_bbox_8: 0.2527  loss_giou_8: 0.5705  loss_ce_dn_8: 0.0002909  loss_mask_dn_8: 0.2813  loss_dice_dn_8: 1.161  loss_bbox_dn_8: 0.2074  loss_giou_dn_8: 0.4495  loss_ce_interm: 0.5732  loss_mask_interm: 0.2865  loss_dice_interm: 1.232  loss_bbox_interm: 0.3794  loss_giou_interm: 0.7038    time: 0.4946  last_time: 0.4963  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:39:35 d2.utils.events]:  eta: 0:00:00  iter: 3999  total_loss: 50.63  loss_ce: 0.04987  loss_mask: 0.3605  loss_dice: 1.105  loss_bbox: 0.3147  loss_giou: 0.4805  loss_ce_dn: 6.983e-05  loss_mask_dn: 0.3757  loss_dice_dn: 1.066  loss_bbox_dn: 0.2491  loss_giou_dn: 0.3812  loss_ce_0: 0.5773  loss_mask_0: 0.387  loss_dice_0: 1.192  loss_bbox_0: 0.6691  loss_giou_0: 0.9489  loss_ce_dn_0: 0.0173  loss_mask_dn_0: 0.4819  loss_dice_dn_0: 2.164  loss_bbox_dn_0: 0.8396  loss_giou_dn_0: 0.8516  loss_ce_1: 0.6314  loss_mask_1: 0.3947  loss_dice_1: 1.178  loss_bbox_1: 0.3207  loss_giou_1: 0.4955  loss_ce_dn_1: 0.0002859  loss_mask_dn_1: 0.3804  loss_dice_dn_1: 1.099  loss_bbox_dn_1: 0.3018  loss_giou_dn_1: 0.4347  loss_ce_2: 0.321  loss_mask_2: 0.4028  loss_dice_2: 1.174  loss_bbox_2: 0.332  loss_giou_2: 0.4955  loss_ce_dn_2: 0.000263  loss_mask_dn_2: 0.375  loss_dice_dn_2: 1.08  loss_bbox_dn_2: 0.2829  loss_giou_dn_2: 0.4064  loss_ce_3: 0.2006  loss_mask_3: 0.3899  loss_dice_3: 1.126  loss_bbox_3: 0.3325  loss_giou_3: 0.5059  loss_ce_dn_3: 0.000247  loss_mask_dn_3: 0.3784  loss_dice_dn_3: 1.084  loss_bbox_dn_3: 0.2688  loss_giou_dn_3: 0.4042  loss_ce_4: 0.08986  loss_mask_4: 0.4085  loss_dice_4: 1.132  loss_bbox_4: 0.3266  loss_giou_4: 0.494  loss_ce_dn_4: 0.0001255  loss_mask_dn_4: 0.3693  loss_dice_dn_4: 1.073  loss_bbox_dn_4: 0.2383  loss_giou_dn_4: 0.3931  loss_ce_5: 0.06821  loss_mask_5: 0.395  loss_dice_5: 1.11  loss_bbox_5: 0.3307  loss_giou_5: 0.5018  loss_ce_dn_5: 0.0001816  loss_mask_dn_5: 0.374  loss_dice_dn_5: 1.069  loss_bbox_dn_5: 0.2519  loss_giou_dn_5: 0.3916  loss_ce_6: 0.05428  loss_mask_6: 0.3767  loss_dice_6: 1.104  loss_bbox_6: 0.3245  loss_giou_6: 0.4846  loss_ce_dn_6: 0.0002019  loss_mask_dn_6: 0.3619  loss_dice_dn_6: 1.065  loss_bbox_dn_6: 0.2432  loss_giou_dn_6: 0.3829  loss_ce_7: 0.05017  loss_mask_7: 0.3825  loss_dice_7: 1.16  loss_bbox_7: 0.3187  loss_giou_7: 0.4847  loss_ce_dn_7: 0.0001018  loss_mask_dn_7: 0.3719  loss_dice_dn_7: 1.074  loss_bbox_dn_7: 0.2445  loss_giou_dn_7: 0.3817  loss_ce_8: 0.05047  loss_mask_8: 0.3776  loss_dice_8: 1.133  loss_bbox_8: 0.3177  loss_giou_8: 0.4885  loss_ce_dn_8: 7.178e-05  loss_mask_dn_8: 0.3738  loss_dice_dn_8: 1.082  loss_bbox_dn_8: 0.2466  loss_giou_dn_8: 0.3788  loss_ce_interm: 0.541  loss_mask_interm: 0.3717  loss_dice_interm: 1.157  loss_bbox_interm: 0.345  loss_giou_interm: 0.5858    time: 0.4946  last_time: 0.5235  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 2505M\n",
      "[03/14 17:39:35 d2.engine.hooks]: Overall training speed: 3998 iterations in 0:32:57 (0.4947 s / it)\n",
      "[03/14 17:39:35 d2.engine.hooks]: Total training time: 0:37:49 (0:04:51 on hooks)\n",
      "[03/14 17:39:36 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[03/14 17:39:36 d2.data.common]: Serializing 67 elements to byte tensors and concatenating them all ...\n",
      "[03/14 17:39:36 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "coco\n",
      "[03/14 17:39:36 d2.evaluation.evaluator]: Start inference on 67 batches\n",
      "[03/14 17:39:37 d2.evaluation.evaluator]: Inference done 11/67. Dataloading: 0.0008 s/iter. Inference: 0.0710 s/iter. Eval: 0.0113 s/iter. Total: 0.0831 s/iter. ETA=0:00:04\n",
      "[03/14 17:39:42 d2.evaluation.evaluator]: Total inference time: 0:00:05.325214 (0.085891 s / iter per device, on 1 devices)\n",
      "[03/14 17:39:42 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:04 (0.073156 s / iter per device, on 1 devices)\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Saving results to output/1/inference/coco_instances_results.json\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.828\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.542\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.623\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.379\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.586\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.776\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 50.907 | 82.823 | 54.194 | 50.288 | 62.305 |  nan  |\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[03/14 17:39:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.767\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.261\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.229\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.350\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.384\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 27.927 | 76.708 | 6.604  | 26.084 | 36.216 |  nan  |\n",
      "[03/14 17:39:42 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[03/14 17:39:42 d2.engine.defaults]: Evaluation results for fiftyone_person_valid in csv format:\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: Task: bbox\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: 50.9066,82.8230,54.1942,50.2878,62.3053,nan\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: Task: segm\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
      "[03/14 17:39:42 d2.evaluation.testing]: copypaste: 27.9266,76.7080,6.6043,26.0841,36.2157,nan\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "# setup and launch the trainer\n",
    "from helpers.model_trainer import Trainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR)\n",
    "\n",
    "# dump labels for deployment\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"classes.json\"), \"w\") as fp:\n",
    "    json.dump(classes, fp)\n",
    "\n",
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    trainer = Trainer(cfg, freeze=False, MaskDINO_weights=True)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37823b-4365-4157-9c72-6038075f1464",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76839d47-d90e-4075-93b5-0f596763156c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "[03/14 17:40:09 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from output/1/model_final.pth ...\n",
      "TEST DATASET\n",
      "\n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 68/68 [612.8ms elapsed, 0s remaining, 111.0 samples/s]     \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████████| 68/68 [496.9ms elapsed, 0s remaining, 136.9 samples/s]      \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      person       0.70      0.23      0.34        93\n",
      "\n",
      "   micro avg       0.70      0.23      0.34        93\n",
      "   macro avg       0.70      0.23      0.34        93\n",
      "weighted avg       0.70      0.23      0.34        93\n",
      "\n",
      "VALIDATION DATASET\n",
      "\n",
      "Evaluating detections...\n",
      " 100% |███████████████████| 67/67 [457.3ms elapsed, 0s remaining, 146.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████████| 67/67 [455.5ms elapsed, 0s remaining, 147.1 samples/s]      \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      person       0.59      0.13      0.21        99\n",
      "\n",
      "   micro avg       0.59      0.13      0.21        99\n",
      "   macro avg       0.59      0.13      0.21        99\n",
      "weighted avg       0.59      0.13      0.21        99\n",
      "\n",
      "TRAIN DATASET\n",
      "\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 307/307 [2.0s elapsed, 0s remaining, 190.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 307/307 [1.9s elapsed, 0s remaining, 196.8 samples/s]      \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      person       0.60      0.17      0.26       430\n",
      "\n",
      "   micro avg       0.60      0.17      0.26       430\n",
      "   macro avg       0.60      0.17      0.26       430\n",
      "weighted avg       0.60      0.17      0.26       430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers.benchmark import benchmark\n",
    "\n",
    "# if we just want to load the results of a previous model use that models experiment_name here\n",
    "# else leave model_name at None to use the most recently trained model\n",
    "\n",
    "model_name = None\n",
    "#model_name = 'model_0000499.pth'\n",
    "\n",
    "weights_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") if model_name is None else os.path.join(cfg.OUTPUT_DIR, model_name)\n",
    "best_weights_path = os.path.join('include', '1_class_r_50.pth')\n",
    "\n",
    "cfg.MODEL.WEIGHTS =  weights_path\n",
    "#cfg.MODEL.WEIGHTS =  best_weights_path\n",
    "\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\n",
    "# cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.8\n",
    "\n",
    "benchmark(cfg, classes, labels_dict, train_dataset_combined, test_dataset_combined, valid_dataset_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c6618d-0de9-4103-b382-56eb5c2e0a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006/?notebook=True&subscription=25801f9e-42b5-44b6-8c8d-e0d012078ba9\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc26c7cf40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook sessions cannot wait\n"
     ]
    }
   ],
   "source": [
    "# launch a fiftyone session to view the model predictions after training\n",
    "import fiftyone as fo\n",
    "session = fo.launch_app(test_dataset_combined, port = 6006)\n",
    "session.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158bcffb-d357-4a1c-9ec7-fefc025f234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9526eeb-0f17-409a-b273-ee6970c2e94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
